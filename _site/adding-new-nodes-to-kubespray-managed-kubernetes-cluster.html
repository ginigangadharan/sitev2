<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Adding new nodes to Kubespray Managed Kubernetes Cluster | Gineesh Madapparambath</title>

    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Adding new nodes to Kubespray Managed Kubernetes Cluster | i am gini</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Adding new nodes to Kubespray Managed Kubernetes Cluster" />
<meta name="author" content="gini" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Kubespray is a combination of Ansible and Kubernetes and you can use Kubespray for deploying production ready Kubernetes clusters. Learn how to add new nodes in a Kubernetes cluster using Kubespray." />
<meta property="og:description" content="Kubespray is a combination of Ansible and Kubernetes and you can use Kubespray for deploying production ready Kubernetes clusters. Learn how to add new nodes in a Kubernetes cluster using Kubespray." />
<link rel="canonical" href="http://localhost:4000/adding-new-nodes-to-kubespray-managed-kubernetes-cluster" />
<meta property="og:url" content="http://localhost:4000/adding-new-nodes-to-kubespray-managed-kubernetes-cluster" />
<meta property="og:site_name" content="i am gini" />
<meta property="og:image" content="http://localhost:4000/assets/images/2020/pexels-albin-berlin-kubespray-add-node-kubernetes-cluster-new-ship-1536x1024.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-27T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"Adding new nodes to Kubespray Managed Kubernetes Cluster","dateModified":"2020-11-27T00:00:00+08:00","datePublished":"2020-11-27T00:00:00+08:00","image":"http://localhost:4000/assets/images/2020/pexels-albin-berlin-kubespray-add-node-kubernetes-cluster-new-ship-1536x1024.jpg","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo-g-v1.png"},"name":"gini"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/adding-new-nodes-to-kubespray-managed-kubernetes-cluster"},"url":"http://localhost:4000/adding-new-nodes-to-kubespray-managed-kubernetes-cluster","author":{"@type":"Person","name":"gini"},"description":"Kubespray is a combination of Ansible and Kubernetes and you can use Kubespray for deploying production ready Kubernetes clusters. Learn how to add new nodes in a Kubernetes cluster using Kubespray.","@type":"BlogPosting","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/logo-g-v1.png">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;800&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@400;800&display=swap" rel="stylesheet">
    

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <!-- Additional Stylesheet -->
    <link rel="stylesheet" href="/assets/css/customize.css">
    
    <!-- Google adsense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1676785078101005"
     crossorigin="anonymous"></script>

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158250524-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158250524-1');
</script>



</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><img src="/assets/images/logo-g-v1.png" alt="Gineesh Madapparambath" style="height: 32px;">&nbsp;&nbsp;<strong>i am gini</strong></a>

        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
  <a class="nav-link" href="/about">About</a>
  </li>
<!-- 
<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
-->

<!-- 
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Authors</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>
<li class="nav-item">
<a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mundana-wordpress/">WP</a>
</li>
<li class="nav-item">
<a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mundana-ghost/">Ghost</a>
</li>
<li class="nav-item">
<a target="_blank" class="nav-link" href="https://www.wowthemes.net/donate/">Buy me a coffee <i class="fa fa-coffee text-danger"></i></a>
</li>

-->

<li class="nav-item">
  <a class="nav-link" href="/ansible">Ansible</a>
  </li>

  <li class="nav-item">
  <a class="nav-link" href="/openshift">OpenShift</a>
  </li>

  <li class="nav-item">
  <a class="nav-link" href="/kubernetes">Kubernetes</a>
  </li>

  <li class="nav-item">
    <a class="nav-link" href="/awesome-tools">Tools</a>
    </li>
            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/Notes-DevOps-Essentials",
    "title": "DevOps Essentials",
    "body": "What is DevOps:  Is a culture of collabaration with Developers and Operations people.  Is NOT tools but tools are essentail to success in DevOps Is NOT a standard Is NOT a product Is NOT a job title. Read : Waterfall vs Agile method of software development. DevOps Culture: Traditional Model : Dev and Ops are seperated. DevOps Culture : Dev and Ops works together and share the same goals. Goals:  Fast time-to-market (TTM) Minimize production FailuresDevops vs Traditional Silos: Traditional Flow: Devlopers -&gt; QA team -&gt; Operations  No Trust between teams Different Priorities Different Goals Developers focus on features but Operations focus on uptime.  Lengthy process Lacks of automationDevOps Way:  Dev writes code Commit will trigger build and integration tests QA can be automate Automated monitoring Auto-rollback previous versionBuild Automation: Automate the process of preparing code for deployment. Why ?  fast consistent repeatable portable reliableContinuous Integration:    frequent code changes   Using a CI Server Multiple times a day feedback on merge and buildWhy ?  Easy to detect issue with smaller changes small releases instead of big releases continuous testingContinuous Delivery and Continuous Deployment:  code is always ready for deployment   frequently deploying small code changes   Automated deployment if issue, automatic roll backWhy ?  faster time-to-market fewer problems caused by deployment process lower risk Reliable rollbacksInfrastructure As Code:  manage and provision infrastructure via code provisioning and managing will be done via automationWhy ?  consistency re-usability scalability self-documenting simplify complex infrastructureConfiguration Management:  automated infrastructure management maintainable wayWhy ?  Save time Insight maintainability less configuration driftsOrchestration:  orchestrate build and automateWhy ?  Scalability Stability Save Time Self-serviceMonitoring:  Collect data like usage of CPU&lt; memory, disk i/o etcWhy ?  Fast Recovery Better root cause analysis Visibility across teams Automated response - self healingDevOps Practices:  ### DevOps Tools"
    }, {
    "id": 2,
    "url": "http://localhost:4000/openstack",
    "title": "OpenStack Learning References",
    "body": "OpenStack Releases:  OpenStack Releases ReleasesOpenStack Training Labs Training Labs provides an easy way to deploy virtualized OpenStack in a desktop environment. Training Labs supports all modern Linux, Windows and macOS platforms. You need to have VirtualBox or KVM/libvirt installed before running Training Labs.  Training Labs Wiki"
    }, {
    "id": 3,
    "url": "http://localhost:4000/about",
    "title": "About",
    "body": "I am Gineesh Madapparambath, aka Gini Gangadharan. An IT professional born and raised in Kerala/India and currently working in Singapore. I am a fan of travel, food and technology. I share travel related articles via my blog rovervibes. com. I also contribute back to the technical community via techbeatly. com. Contact me at @ginigangadharan / email. Thank you,Gini Gangadharan "
    }, {
    "id": 4,
    "url": "http://localhost:4000/awx-installation",
    "title": "Ansible AWX Installation",
    "body": "Ref:  Ansible AWX Project Introduction to Ansible AWX Project AWX Command Line Interface 3. Install AWX Using Ansible     3. 1. Configure AWX Node for Ansible   3. 2. Install Pre-Req roles   3. 3. Install AWX using Roles    4. Install AWX Manually     4. 1. Install epel repo and then install jq   4. 2. Install docker-ce related packages   4. 3. Enable docker-ce repo and install docker engine.    4. 4. Install latest docker-compose   4. 5. Install AWX dependencies   4. 6. Download Packages   4. 7. Disable dockerhub reference in order to build local images.    4. 8. Create a folder in /opt/ to hold awx psql data   4. 9. Provide psql data path to installer.    4. 10. Enable SSL   4. 11. Adding Logo   4. 12. Add admin user and password   4. 13. Install AWX    5. AWX Backup &amp; Restore 6. Migration     6. 1. Upgrade AWX   3. Install AWX Using AnsibleYou need an ansible machine (just a machine with ansible configured). This is very easy and straight forward as there are roles availalbe for installaing and configuring Ansible AWX. If you want to know the actual steps and items involved in the AWX installation, try manual method. 3. 1. Configure AWX Node for Ansible:  Create a node (VM in public or private cloud) and make sure you this is configured for Ansible to access. (You can manually do this or just use any roles if multiple nodes). I using a very simple role setup_ansible_user which will add a user and configure ssh keys. (You can skip this steps if your new VM already configured with ssh keys and sudo access)Sample content of setup-ansible. yaml - hosts: all become: true vars_files: vars:  remote_user: devops  remote_user_public_key: 'YOUR_PUBLIC_KEY' # use your public key to add to remote node roles:  - { role: setup-ansible-user } Add your AWX node to Inventory[awx]awx-node-01 ansible_host=YOUR_VM_IPRemember to use proper switches to ask password, become sudo, sudo password etc. ansible-playbook setup-ansible. yaml -b -K -k -u root3. 2. Install Pre-Req roles: We can use the awx role by Jeff Geerling. But, make sure you have installed all pre-req roles from Ansible Galaxy; add roles in requirements. yml and install. $ cat requirements. yml# from galaxy- src: geerlingguy. repo-epel- src: geerlingguy. git- src: geerlingguy. ansible- src: geerlingguy. docker- src: geerlingguy. pip- src: geerlingguy. nodejs- src: geerlingguy. awxThen, $ ansible-galaxy install -r requirements. yml# verify roles directory$ ls -l roles/total 32drwxr-xr-x 6 net_gini net_gini 4096 Feb 12 22:09 geerlingguy. ansibledrwxr-xr-x 7 net_gini net_gini 4096 Feb 12 22:09 geerlingguy. awxdrwxr-xr-x 8 net_gini net_gini 4096 Feb 12 22:09 geerlingguy. dockerdrwxr-xr-x 7 net_gini net_gini 4096 Feb 12 22:09 geerlingguy. gitdrwxr-xr-x 8 net_gini net_gini 4096 Feb 12 22:09 geerlingguy. nodejsdrwxr-xr-x 6 net_gini net_gini 4096 Feb 12 22:09 geerlingguy. pipdrwxr-xr-x 6 net_gini net_gini 4096 Feb 12 22:08 geerlingguy. repo-epeldrwxr-xr-x 9 net_gini net_gini 4096 Feb 11 15:15 makarenalabs. wordpress3. 3. Install AWX using Roles: Now, just call your awx install playbook; sample playbook below. *Note : You may customize AWX installation, refer awx for more. - hosts: awx-node-01 become: true vars:  nodejs_version:  6. x   pip_install_packages:   - name: docker-py roles:  - geerlingguy. repo-epel  - geerlingguy. git  - geerlingguy. ansible  - geerlingguy. docker  - geerlingguy. pip  - geerlingguy. nodejs  - geerlingguy. awxInstall Ansible AWX $ ansible-playbook awx-install. yaml After AWX is installed, you can log in with the default username admin and password password. 4. Install AWX Manually4. 1. Install epel repo and then install jq: yum install -y epel-release -y &amp;&amp; yum install jq4. 2. Install docker-ce related packages: yum install -y yum-utils device-mapper-persistent-data lvm24. 3. Enable docker-ce repo and install docker engine. : yum-config-manager --add-repo https://download. docker. com/linux/centos/docker-ce. repoyum -y install docker-cesystemctl enable docker &amp;&amp; systemctl start docker4. 4. Install latest docker-compose: LATEST_VERSION=$(curl -s https://api. github. com/repos/docker/compose/releases/latest | jq -r '. tag_name')curl -L  https://github. com/docker/compose/releases/download/$LATEST_VERSION/docker-compose-$(uname -s)-$(uname -m)  &gt; /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose4. 5. Install AWX dependencies: yum install -y python2-pippip install ansiblepip install docker-compose4. 6. Download Packages: Change dir to the home directory and get ansible-awx release tarball and extract it. cd ~curl \ -o ansible-awx-6. 1. 0. tar. gz https://codeload. github. com/ansible/awx/tar. gz/6. 1. 0 &amp;&amp; \ tar xvfz ansible-awx-6. 1. 0. tar. gz &amp;&amp; \ rm -f ansible-awx-6. 1. 0. tar. gz4. 7. Disable dockerhub reference in order to build local images. : cd awx-6. 1. 0sed -i  s|^dockerhub_base=ansible|#dockerhub_base=ansible|g  installer/inventory4. 8. Create a folder in /opt/ to hold awx psql data: mkdir -p /opt/awx-psql-data4. 9. Provide psql data path to installer. : sed -i  s|^postgres_data_dir. *|postgres_data_dir=/opt/awx-psql-data|g  installer/inventoryNote: If you wish to use an external database, in the inventory file, set the value of pg_hostname, and update pg_username, pg_password, pg_database, and pg_port with the connection information. 4. 10. Enable SSL: # Create awx-ssl folder in /etc. mkdir -p /etc/awx-ssl/# Make a self-signed ssl certificateopenssl req -subj '/CN=labs. local/O=Labs Local/C=TR' \	-new -newkey rsa:2048 \	-sha256 -days 1365 \	-nodes -x509 \	-keyout /etc/awx-ssl/awx. key \	-out /etc/awx-ssl/awx. crt# Merge awx. key and awx. crt filescat /etc/awx-ssl/awx. key /etc/awx-ssl/awx. crt &gt; /etc/awx-ssl/awx-bundled-key. crt# Pass the full path of awx-bundled-key. crt file to ssl_certificate variable in inventory. sed -i -E  s|^#([[:space:]]?)ssl_certificate=|ssl_certificate=/etc/awx-ssl/awx-bundled-key. crt|g  installer/inventory4. 11. Adding Logo: # Change dir to where awx main folder is placed:cd ~# Download and extract awx-logos repository. curl -L -o awx-logos. tar. gz https://github. com/ansible/awx-logos/archive/master. tar. gztar xvfz awx-logos. tar. gz# Rename awx-logos-master folder as awx-logos mv awx-logos-master awx-logos# Remove tarballrm -f *awx*. tar. gzNote: AWX installer which resides at installer/install. yml, searches for awx-logos directory as . . /. . /awx-logos. So, your awx-logos folder should be located at where awx installer’s parent directory is placed. # Change dir to awx and replace awx_official parametercd ~/awx-6. 1. 0sed -i -E  s|^#([[:space:]]?)awx_official=false|awx_official=true|g  installer/inventory4. 12. Add admin user and password: # Define the default admin usernamesed -i  s|^admin_user=. *|admin_user=awx-admin|g  installer/inventory# Set a password for the adminsed -i  s|^admin_password=. *|admin_password=CHANGE_ME|g  installer/inventory4. 13. Install AWX: # Enter the installer directory. cd ~/awx-6. 1. 0/installer# Initiate install. ymlansible-playbook -i inventory install. ymlReference:  Installing AWX Ansible AWX Installatio5. AWX Backup &amp; Restorehttp://yallalabs. com/automation-tool/how-to-backup-restore-awx-ansible-tower-objects-tower-cli-tool/https://www. unixarena. com/2019/03/backup-restore-ansible-awx-tower-cli. html/https://www. insentragroup. com/au/insights/geek-speak/modern-workplace/protecting-the-automation-engine-backup-for-ansible-awx-project/ 6. Migration Migrating Data Between AWX Installations Upgrading from previous versions6. 1. Upgrade AWX: https://stackoverflow. com/questions/59624053/upgrade-ansible-tower-minor-upgrade/59639499#59639499 If you used the docker-compose installation method and pointed postgres_data_dir to a persistent directory on the host, upgrading AWX is straightforward. I deployed AWX 2. 0. 0 in 2018 and have upgraded it to every subsequent release (currently running 9. 1. 0) without issue. Below is my upgrade method which preserves all data including secrets between upgrades and does not rely on using the tower cli / awx cli tool. AWX path assumptions: Existing installation: /opt/awx New release: /tmp/awx AWX inventory file assumptions: use_docker_compose=truepostgres_data_dir=/opt/postgresdocker_compose_dir=/var/lib/awxManual upgrade process: Backup your AWX host before continuing! Consider backing up your postgres database as well. Download the new release of AWX and unpack it to /tmp/awxEnsure that the patch package is installed on the host. Create a patch file containing the differences between the new and existing inventory files:diff -u /tmp/awx/installer/inventory /opt/awx/installer/inventory &gt; /tmp/awx_inv_patch Patch the new inventory file with the differences:patch /tmp/awx/installer/inventory &lt; /tmp/awx_inv_patch Verify that the files now match:diff -s /tmp/awx/installer/inventory /opt/awx/installer/inventory Copy the new release directory over the existing one:cp -Rp /tmp/awx/* /opt/awx/ Edit /var/lib/awx/docker-compose. yml and change the version numbers after image: ansible/awx_web: and image: ansible/awx_task: to match the new version of AWX that you’re upgrading to. Stop the current AWX containers:cd /var/lib/awx docker-compose stop Run the installer:cd /opt/awx/inventory ansible-playbook -i inventory install. yml AWX starts the upgrade process, which usually completes within a couple minutes. I’ll typically monitor the upgrade progress with docker logs -f awx_web until I see RESULT 2 / OKREADY appear. If everything is working as intended, I shut the containers down, pull and then recreate them using docker-compose:cd /var/lib/awx docker-compose stop docker-compose pull &amp;&amp; docker-compose up –force-recreate -d If everything is still working as intended, I delete /tmp/awx and /tmp/awx_inv_patch. "
    }, {
    "id": 5,
    "url": "http://localhost:4000/ansible-awx-operator",
    "title": "Install Ansible AWX using AWX Operator (on Kubernetes)",
    "body": " Install Ansible AWX using AWX Operator (on Kubernetes)     Setup a Kubernete s Cluster   Deploy AWX Operator   Create AWX Deployment   Get Admin Password   Get the Path to Access Ansible AWX         Access Ansible AWX on Remote Minikube or Kubernetes Cluster           ReferencesInstall Ansible AWX using AWX Operator (on Kubernetes)Setup a Kubernete s Cluster: We will install it on a minikube. (Refere minikube installation) $ minikube start --addons=ingress --cni=flannel --install-addons=true \  --kubernetes-version=stable \  --vm-driver=docker --wait=false \  --cpus=4 --memory=6gEnable Addons as needed ## Check addons$ minikube addons list$ minikube addons enable metrics-server$ minikube dashboardDeploy AWX Operator: Replace TAG with the version from Release Page kubectl apply -f https://raw. githubusercontent. com/ansible/awx-operator/&lt;TAG&gt;/deploy/awx-operator. yamlCreate AWX Deployment: Create a file awx-demo. yaml ---apiVersion: awx. ansible. com/v1beta1kind: AWXmetadata: name: awx-demospec: service_type: nodeport ingress_type: none hostname: awx-demo. example. com$ kubectl apply -f awx-demo. ymlawx. awx. ansible. com/awx-demo createdRefer AWX Operator documentation for advanced installation options. Get Admin Password: By default, the admin user is admin and the password is available in the &lt;resourcename&gt;-admin-password secret. $ kubectl get secret awx-demo-admin-password -o jsonpath= {. data. password}  | base64 --decodelxQ8uWlE9Wevkgmy5Kx2AqFdY80v34gxGet the Path to Access Ansible AWX: You will find the NodePort and IP for service awx-demo-service $ kubectl get svcNAME          TYPE    CLUSTER-IP    EXTERNAL-IP  PORT(S)       AGEawx-demo-postgres   ClusterIP  None       &lt;none&gt;    5432/TCP      23mawx-demo-service    NodePort  10. 110. 146. 161  &lt;none&gt;    80:31726/TCP    23mawx-operator-metrics  ClusterIP  10. 104. 113. 88  &lt;none&gt;    8383/TCP,8686/TCP  23mkubernetes       ClusterIP  10. 96. 0. 1    &lt;none&gt;    443/TCP       26mAccess Ansible AWX Portal at IP_ADDRESS:NODE_PORT. Or let minikube open the browser $ minikube service hello-minikubeAccess Ansible AWX on Remote Minikube or Kubernetes Cluster: If your Kubernetes/minikube Cluster is on Remote Machine/VM (eg: Cloud Instance with Public IP) then you can access it using above method (if NodePort is same is remote IP) or you can use LoadBalancer methods. In our case, we have deplyed this in a Google Cloud instance without GUI and we need to enabled port-forwading as below. ## Forward localhost (minkube VM Localhost) port 7080 -&gt; 80$ kubectl port-forward service/awx-demo-service 7080:80So the Ansible AWX service is available at minikube VM localhost:7080 now; but we dont have GUI there to access !! So, we do a port-forwarding from our laptop/workstation via SSH Tunnel. ## On your Workstation/Laptop## ssh -L LOCAL_PORT:localhost:REMOTE_PORT User@REMOTE_IP$ ssh -L 7080:localhost:7080 gini@123. 123. 234. 234Now, open a browser on your laptop/workstation and goto localhost:7080; that’s it. Enjoy Ansible AWX running on top of Kubernetes/minikube. References"
    }, {
    "id": 6,
    "url": "http://localhost:4000/ansible-best-practices",
    "title": "Ansible Best Practices",
    "body": " Content Organization     Directory Layout   Alternative Directory Layout    Use Dynamic Inventory With Clouds How to Differentiate Staging vs Production Group And Host Variables Top Level Playbooks Are Separated By Role Task And Handler Organization For A Role What This Organization Enables (Examples) Deployment vs Configuration Organization Staging vs Production ReferencesNote : This document is somewhat based on original document - Best Practices - and I am keeping a modified copy with more details for my own references. Photo by Pixabay from Pexels Content Organization: The following section shows one of many possible ways to organize playbook content. Your usage of Ansible should fit your needs, however, not ours, so feel free to modify this approach and organize as you see fit. One crucial way to organize your playbook content is Ansible’s “roles” organization feature, which is documented as partof the main playbooks page.  You should take the time to read and understand the roles documentation which is available here: :ref:playbooks_reuse_roles. . . _directory_layout: Directory Layout: The top level of the directory would contain files and directories like so::   production        # inventory file for production servers  staging          # inventory file for staging environment  group_vars/    group1. yml       # here we assign variables to particular groups    group2. yml  host_vars/    hostname1. yml     # here we assign variables to particular systems    hostname2. yml  library/         # if any custom modules, put them here (optional)  module_utils/       # if any custom module_utils to support modules, put them here (optional)  filter_plugins/      # if any custom filter plugins, put them here (optional)  site. yml         # master playbook  webservers. yml      # playbook for webserver tier  dbservers. yml       # playbook for dbserver tier  tasks/          # task files included from playbooks    webservers-extra. yml # &lt;-- avoids confusing playbook with task files  roles/    common/        # this hierarchy represents a  role       tasks/      #        main. yml   # &lt;-- tasks file can include smaller files if warranted      handlers/     #        main. yml   # &lt;-- handlers file      templates/    # &lt;-- files for use with the template resource        ntp. conf. j2  # &lt;------- templates end in . j2      files/      #        bar. txt    # &lt;-- files for use with the copy resource        foo. sh    # &lt;-- script files for use with the script resource      vars/       #        main. yml   # &lt;-- variables associated with this role      defaults/     #        main. yml   # &lt;-- default lower priority variables for this role      meta/       #        main. yml   # &lt;-- role dependencies      library/     # roles can also include custom modules      module_utils/   # roles can also include custom module_utils      lookup_plugins/  # or other types of plugins, like lookup in this case    webtier/       # same kind of structure as  common  was above, done for the webtier role    monitoring/      #       fooapp/        #   . . note: If you find yourself having too many top level playbooks (for instance you have a playbook you wrote for a specific hotfix, etc), it may make sense to have a playbooks/ directory instead.  This can be a good idea as you get larger.  If you do this, configure your roles_path in ansible. cfg to find your roles location. . . _alternative_directory_layout: Alternative Directory Layout: Alternatively you can put each inventory file with its group_vars/host_vars in a separate directory. This is particularly useful if your group_vars/host_vars don’t have that much in common in different environments. The layout could look something like this:: inventories/  production/   hosts        # inventory file for production servers   group_vars/     group1. yml    # here we assign variables to particular groups     group2. yml   host_vars/     hostname1. yml  # here we assign variables to particular systems     hostname2. yml  staging/   hosts        # inventory file for staging environment   group_vars/     group1. yml    # here we assign variables to particular groups     group2. yml   host_vars/     stagehost1. yml  # here we assign variables to particular systems     stagehost2. ymllibrary/module_utils/filter_plugins/site. ymlwebservers. ymldbservers. ymlroles/  common/  webtier/  monitoring/  fooapp/This layout gives you more flexibility for larger environments, as well as a total separation of inventory variables between different environments. The downside is that it is harder to maintain, because there are more files. . . _use_dynamic_inventory_with_clouds: Use Dynamic Inventory With Clouds: If you are using a cloud provider, you should not be managing your inventory in a static file.  See :ref:intro_dynamic_inventory. This does not just apply to clouds – If you have another system maintaining a canonical list of systemsin your infrastructure, usage of dynamic inventory is a great idea in general. . . _staging_vs_prod: How to Differentiate Staging vs Production: If managing static inventory, it is frequently asked how to differentiate different types of environments.  The following exampleshows a good way to do this.  Similar methods of grouping could be adapted to dynamic inventory (for instance, consider applying the AWStag “environment:production”, and you’ll get a group of systems automatically discovered named “ec2_tag_environment_production”. Let’s show a static inventory example though.  Below, the production file contains the inventory of all of your production hosts. It is suggested that you define groups based on purpose of the host (roles) and also geography or datacenter location (if applicable):: # file: production[atlanta_webservers]www-atl-1. example. comwww-atl-2. example. com[boston_webservers]www-bos-1. example. comwww-bos-2. example. com[atlanta_dbservers]db-atl-1. example. comdb-atl-2. example. com[boston_dbservers]db-bos-1. example. com# webservers in all geos[webservers:children]atlanta_webserversboston_webservers# dbservers in all geos[dbservers:children]atlanta_dbserversboston_dbservers# everything in the atlanta geo[atlanta:children]atlanta_webserversatlanta_dbservers# everything in the boston geo[boston:children]boston_webserversboston_dbservers. . _groups_and_hosts: Group And Host Variables: This section extends on the previous example. Groups are nice for organization, but that’s not all groups are good for.  You can also assign variables to them! For instance, atlanta has its own NTP servers, so when setting up ntp. conf, we should use them.  Let’s set those now:: ---# file: group_vars/atlantantp: ntp-atlanta. example. combackup: backup-atlanta. example. comVariables aren’t just for geographic information either! Maybe the webservers have some configuration that doesn’t make sense for the database servers:: ---# file: group_vars/webserversapacheMaxRequestsPerChild: 3000apacheMaxClients: 900If we had any default values, or values that were universally true, we would put them in a file called group_vars/all:: ---# file: group_vars/allntp: ntp-boston. example. combackup: backup-boston. example. comWe can define specific hardware variance in systems in a host_vars file, but avoid doing this unless you need to:: ---# file: host_vars/db-bos-1. example. comfoo_agent_port: 86bar_agent_port: 99Again, if we are using dynamic inventory sources, many dynamic groups are automatically created.  So a tag like “class:webserver” would load invariables from the file “group_vars/ec2_tag_class_webserver” automatically. . . _split_by_role: Top Level Playbooks Are Separated By Role: In site. yml, we import a playbook that defines our entire infrastructure.  This is a very short example, because it’s just importingsome other playbooks:: ---# file: site. yml- import_playbook: webservers. yml- import_playbook: dbservers. ymlIn a file like webservers. yml (also at the top level), we map the configuration of the webservers group to the roles performed by the webservers group:: ---# file: webservers. yml- hosts: webservers roles:  - common  - webtierThe idea here is that we can choose to configure our whole infrastructure by “running” site. yml or we could just choose to run a subset by runningwebservers. yml.  This is analogous to the “–limit” parameter to ansible but a little more explicit:: ansible-playbook site. yml –limit webservers  ansible-playbook webservers. yml . . _role_organization: Task And Handler Organization For A Role: Below is an example tasks file that explains how a role works.  Our common role here just sets up NTP, but it could do more if we wanted:: ---# file: roles/common/tasks/main. yml- name: be sure ntp is installed yum:  name: ntp  state: present tags: ntp- name: be sure ntp is configured template:  src: ntp. conf. j2  dest: /etc/ntp. conf notify:  - restart ntpd tags: ntp- name: be sure ntpd is running and enabled service:  name: ntpd  state: started  enabled: yes tags: ntpHere is an example handlers file.  As a review, handlers are only fired when certain tasks report changes, and are run at the endof each play:: ---# file: roles/common/handlers/main. yml- name: restart ntpd service:  name: ntpd  state: restartedSee :ref:playbooks_reuse_roles for more information. . . _organization_examples: What This Organization Enables (Examples): Above we’ve shared our basic organizational structure. Now what sort of use cases does this layout enable? Lots! If I want to reconfigure my whole infrastructure, it’s just:: ansible-playbook -i production site. ymlTo reconfigure NTP on everything:: ansible-playbook -i production site. yml --tags ntpTo reconfigure just my webservers:: ansible-playbook -i production webservers. ymlFor just my webservers in Boston:: ansible-playbook -i production webservers. yml --limit bostonFor just the first 10, and then the next 10:: ansible-playbook -i production webservers. yml --limit boston[0:9]ansible-playbook -i production webservers. yml --limit boston[10:19]And of course just basic ad-hoc stuff is also possible:: ansible boston -i production -m pingansible boston -i production -m command -a '/sbin/reboot'And there are some useful commands to know:: # confirm what task names would be run if I ran this command and said  just ntp tasks ansible-playbook -i production webservers. yml --tags ntp --list-tasks# confirm what hostnames might be communicated with if I said  limit to boston ansible-playbook -i production webservers. yml --limit boston --list-hosts. . _dep_vs_config: Deployment vs Configuration Organization: The above setup models a typical configuration topology.  When doing multi-tier deployments, there are goingto be some additional playbooks that hop between tiers to roll out an application.  In this case, ‘site. yml’may be augmented by playbooks like ‘deploy_exampledotcom. yml’ but the general concepts can still apply. Consider “playbooks” as a sports metaphor – you don’t have to just have one set of plays to use against your infrastructureall the time – you can have situational plays that you use at different times and for different purposes. Ansible allows you to deploy and configure using the same tool, so you would likely reuse groups and justkeep the OS configuration in separate playbooks from the app deployment. . . _staging_vs_production: Staging vs Production: As also mentioned above, a good way to keep your staging (or testing) and production environments separate is to use a separate inventory file for staging and production.  This way you pick with -i what you are targeting.  Keeping them all in one file can lead to surprises! Testing things in a staging environment before trying in production is always a great idea.  Your environments need not be the samesize and you can use group variables to control the differences between those environments. . . _rolling_update: Rolling Updates+++++++++++++++ Understand the ‘serial’ keyword.  If updating a webserver farm you really want to use it to control how many machines you areupdating at once in the batch. See :ref:playbooks_delegation. . . _mention_the_state: Always Mention The State++++++++++++++++++++++++ The ‘state’ parameter is optional to a lot of modules.  Whether ‘state=present’ or ‘state=absent’, it’s always best to leave thatparameter in your playbooks to make it clear, especially as some modules support additional states. . . _group_by_roles: Group By Roles++++++++++++++ We’re somewhat repeating ourselves with this tip, but it’s worth repeating. A system can be in multiple groups.  See :ref:intro_inventory and :ref:intro_patterns.  Having groups named after things likewebservers and dbservers is repeated in the examples because it’s a very powerful concept. This allows playbooks to target machines based on role, as well as to assign role specific variablesusing the group variable system. See :ref:playbooks_reuse_roles. . . _os_variance: Operating System and Distribution Variance++++++++++++++++++++++++++++++++++++++++++ When dealing with a parameter that is different between two different operating systems, a great way to handle this isby using the group_by module. This makes a dynamic group of hosts matching certain criteria, even if that group is not defined in the inventory file:: - name: talk to all hosts just so we can learn about them hosts: all tasks:  - name: Classify hosts depending on their OS distribution   group_by:    key: os_{{ ansible_facts['distribution'] }}# now just on the CentOS hosts. . . - hosts: os_CentOS gather_facts: False tasks:  - # tasks that only happen on CentOS go hereThis will throw all systems into a dynamic group based on the operating system name. If group-specific settings are needed, this can also be done. For example:: ---# file: group_vars/allasdf: 10---# file: group_vars/os_CentOSasdf: 42In the above example, CentOS machines get the value of ‘42’ for asdf, but other machines get ‘10’. This can be used not only to set variables, but also to apply certain roles to only certain systems. Alternatively, if only variables are needed:: - hosts: all tasks:  - name: Set OS distribution dependent variables   include_vars:  os_{{ ansible_facts['distribution'] }}. yml   - debug:    var: asdfThis will pull in variables based on the OS name. . . _ship_modules_with_playbooks: Bundling Ansible Modules With Playbooks+++++++++++++++++++++++++++++++++++++++ If a playbook has a :file:. /library directory relative to its YAML file, this directory can be used to add ansible modules that willautomatically be in the ansible module path.  This is a great way to keep modules that go with a playbook together.  This is shownin the directory structure example at the start of this section. . . _whitespace: Whitespace and Comments+++++++++++++++++++++++ Generous use of whitespace to break things up, and use of comments (which start with ‘#’), is encouraged. . . _name_tasks: Always Name Tasks+++++++++++++++++ It is possible to leave off the ‘name’ for a given task, though it is recommended to provide a descriptionabout why something is being done instead.  This name is shown when the playbook is run. . . _keep_it_simple: Keep It Simple++++++++++++++ When you can do something simply, do something simply.  Do not reachto use every feature of Ansible together, all at once.  Use what worksfor you.  For example, you will probably not need vars,vars_files, vars_prompt and --extra-vars all at once,while also using an external inventory file. If something feels complicated, it probably is, and may be a good opportunity to simplify things. . . _version_control: Version Control+++++++++++++++ Use version control.  Keep your playbooks and inventory file in git(or another version control system), and commit when you make changesto them.  This way you have an audit trail describing when and why youchanged the rules that are automating your infrastructure. . . _best_practices_for_variables_and_vaults: Variables and Vaults++++++++++++++++++++++++++++++++++++++++ For general maintenance, it is often easier to use grep, or similar tools, to find variables in your Ansible setup. Since vaults obscure these variables, it is best to work with a layer of indirection. When running a playbook, Ansible finds the variables in the unencrypted file and all sensitive variables come from the encrypted file. A best practice approach for this is to start with a group_vars/ subdirectory named after the group. Inside of this subdirectory, create two files named vars and vault. Inside of the vars file, define all of the variables needed, including any sensitive ones. Next, copy all of the sensitive variables over to the vault file and prefix these variables with vault_. You should adjust the variables in the vars file to point to the matching vault_ variables using jinja2 syntax, and ensure that the vault file is vault encrypted. This best practice has no limit on the amount of variable and vault files or their names. . . seealso:: :ref:yaml_syntax    Learn about YAML syntax  :ref:working_with_playbooks    Review the basic playbook features  :ref:all_modules    Learn about available modules  :ref:developing_modules    Learn how to extend Ansible by writing your own modules  :ref:intro_patterns    Learn about how to select hosts  GitHub examples directory &lt;https://github. com/ansible/ansible-examples&gt;_    Complete playbook files from the github project source  Mailing List &lt;https://groups. google. com/group/ansible-project&gt;_    Questions? Help? Ideas? Stop by the list on Google Groups References:  Four Ansible Practices I Would Recommend ANSIBLE BEST PRACTICES: THE ESSENTIALS (PDF Slides)"
    }, {
    "id": 7,
    "url": "http://localhost:4000/ansible-collections",
    "title": "Ansible Collections",
    "body": " Installing collections     Using Collection on Tower    ReferencesInstalling collectionsansible-galaxy collection install my_namespace. my_collectionor $ cat collections/requirements. ymlcollections: - name: junipernetworks. junos  source: https://galaxy. ansible. com  - name: f5networks. f5_modules  source: https://cloud. redhat. com/api/automation-hub/ansible-galaxy collection install -r collections/requirements. yml Using Collection on Tower: Installing and using collections on Ansible Tower References Getting Started With Ansible Content Collections Using collections Hands on with Ansible collections"
    }, {
    "id": 8,
    "url": "http://localhost:4000/ansible-dictionary",
    "title": "Ansible Dictionary Sample",
    "body": "Sample Playbook to Handle Ansible dictionary ---- name: Test play for dictionary hosts: all vars:  interfaces:   ansible-node-2:    int1:     intname: ens192     intip: 10. 154. 158. 146     intmask: 255. 255. 255. 128     intgw: 10. 154. 158. 1 tasks:  - name: Looping   debug:    msg: Hello {{ mydict }}   #with_dict:   # - '{{ ens192 }}'  - name: Looping 2   debug:    msg:  Int1 : {{ interfaces['ansible-node-2']['int1']['intname'] }} "
    }, {
    "id": 9,
    "url": "http://localhost:4000/ansible-for-cyberark",
    "title": "Ansible for CyberArk",
    "body": " ReferencesReferences cyberark/ansible-modules Securing Ansible Automation Environments with CyberArk"
    }, {
    "id": 10,
    "url": "http://localhost:4000/ansible-for-data",
    "title": "Formatting Data in Ansible",
    "body": "Process JSON DataRefer Sample Playbook References Ansible read JSON file – JSON file Parsing JSON Query Filter"
    }, {
    "id": 11,
    "url": "http://localhost:4000/ansible-for-fortinet",
    "title": "Ansible for FortiNet",
    "body": " Setup FortiNet device on GNS3 API References Prerequisites     Install fortiosAPI   Install FortiOS Collection from Ansible Galaxy    Web Rating Overrides     CLI commands   Using Ansible Modules    Add the new member to blacklist group Appendix     ansible-fortios-generic module    Troubleshooting     HTTPS issue    ReferencesSetup FortiNet device on GNS3Refer : Fortinet Device Setup in GNS3 API References Run Your First Playbook GitHub : ansible_fortios_api Use REST API Access FortiGate Fortinet Ansible Modules Documentation$ curl -k -i -X POST http://10. 1. 10. 70/logincheck -d  username=admin&amp;secretkey=password  --dump-header headers. txt -c cookies. txt$ curl -k -i -X GET http://10. 1. 10. 70/api/v2/cmdb/router/static -b headers. txt### logout$ curl -k -i -X POST http://10. 1. 10. 70/logoutPrerequisitesInstall fortiosAPI: pip install fortiosapi#orpip3 install fortiosapi#orpython3. 6 -m pip install &lt;module&gt;Ref: https://pypi. org/project/fortiosapi/ Dependancy  oyaml Install FortiOS Collection from Ansible Galaxy: ansible-galaxy collection install fortinet. fortios https://github. com/fortinet-ansible-dev/ansible-galaxy-fortios-collection/tree/fos_v6. 0. 0/galaxy_1. 0. 13 Web Rating OverridesRef: Overriding FortiGuard website categorization CLI commands: In the CLI, the term is local category. To create a local category: config webfilter ftgd-local-cat edit local_category_1  set id 140 endeg: config webfilter ftgd-local-cat edit custom_category  set id 150 endTo set a rating to a Local Category: config webfilter ftgd-local-rating edit &lt;url_str&gt;  set rating {[&lt;category_int&gt;] [group_str] . . . ]  set status {enable | disable} endeg: config webfilter ftgd-local-rating edit testurl101. com  set rating 150  set status enable endUsing Ansible Modules:  fortios_webfilter_ftgd_local_cat - Configure FortiGuard Web Filter local categories in Fortinet’s FortiOS and FortiGate fortios_webfilter_ftgd_local_rating - Configure local FortiGuard Web Filter local ratings in Fortinet’s FortiOS and FortiGateAdd the new member to blacklist grouphttps://help. fortinet. com/fos60hlp/60/Content/FortiOS/fortigate-firewall/Object%20Configuration/Addresses/Address%20Groups. htm?Highlight=add%20address%20to%20group https://docs. fortinet. com/vm/cisco-aci/fortigate/5. 6/sdn-connector/5. 6. 3/617358/configuring-the-firewall-address-and-address-group config firewall address edit  test-tag   set type dynamic  set sdn aci  set tenant  TENANT-NAME   set epg-name  AP-NAME|EPG-NAME   set sdn-tag  TAG-NAME  nextendconfig firewall addrgrp edit  test-group   set member  test-tag   Adobe Login  nextendFortiGate-VM64-KVM # show firewall addrgrp blklist1config firewall addrgrp  edit  blklist1     set uuid b257da0e-bd59-51ea-2067-4bd93d716b5f    set member  block-10. 6. 10. 0/24     set comment  Added via Ansible     set color 6  nextendAppendixconfig firewall addressedit  eg block ip1     set subnet 192. 168. 82. 82 255. 255. 255. 255  next  edit  eg block ip2     set subnet 192. 168. 81. 81 255. 255. 255. 255  nextendconfig firewall addrgrp	edit  Group of block ip     set member  eg block ip1   eg block ip2 nextendconfig webfilter urlfilter edit 1    set name  webfilter-url     config entries	edit 1        set url  *         set type wildcard        set action block      next	end	nextendconfig firewall policyedit 1    set name  Weburl filter     set srcintf  lan1     set dstintf  wan1     set srcaddr  Group of block ip     set dstaddr  all     set action accept    set schedule  always     set service  ALL     set utm-status enable    set inspection-mode proxy    set logtraffic all    set webfilter-profile  webfilter-url       set profile-protocol-options  protocol     set ssl-ssh-profile  protocols     set nat enable  nextendansible-fortios-generic module: Ref: ansible-fortios-generic TroubleshootingHTTPS issue: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl. c:1076)’))) A - Disable https and use http References GitHub : ansible-galaxy-fortios-collectionhttps://docs. fortinet. com/document/fortigate/6. 2. 3/cookbook/954635/getting-startedhttps://ftnt-ansible-docs. readthedocs. io/en/latest/fortios_modules/fortios_webfilter_urlfilter. htmlhttps://docs. ansible. com/ansible/latest/modules/fortios_webfilter_urlfilter_module. htmlhttps://docs. ansible. com/ansible/latest/modules/fortios_firewall_policy_module. html"
    }, {
    "id": 12,
    "url": "http://localhost:4000/ansible-for-infoblox",
    "title": "Ansible for Infoblox",
    "body": " Configure Ansible for Infoblox     Install Python3 on RHEL   Instal Python3 on Centos   Setup Python VirtualEnv (venv)   DONOT Upgrade pip to Latest !!!   Install Ansible   Install infoblox Client    Infoblox API References     Sample API Calls    Appendix About Infoblox vNIOS Virtual Appliance for Microsoft Hyper-VConfigure Ansible for InfobloxInstall Python3 on RHEL: https://developers. redhat. com/blog/2018/08/13/install-python3-rhel/#why-rhscl $ su -# subscription-manager repos --enable rhel-7-server-optional-rpms \ --enable rhel-server-rhscl-7-rpms# yum -y install @development# yum -y install rh-python36 # yum -y install rh-python36-numpy \ rh-python36-scipy \ rh-python36-python-tools \ rh-python36-python-sixInstal Python3 on Centos: # yum install -y python3Setup Python VirtualEnv (venv): $ scl enable rh-python36 bash$ python3 -VPython 3. 6. 3 $ python -V # python now also points to Python3 Python 3. 6. 3 $ mkdir ~/pydev$ cd ~/pydev $ python3 -m venv py36-venv$ source py36-venv/bin/activate (py36-venv) $ python3 -m pip install . . . some modules. . . DONOT Upgrade pip to Latest !!!: $ python -m pip install pip==20. 2. 3Install Ansible: ## install required packagespip install wheel setuptools rust etcpip install ansibleInstall infoblox Client: pip install infoblox-clientInfoblox API ReferencesFind the WAPI documentation from https://&lt;GRID_IP_ADDRESS&gt;/wapidoc/ url. ## list WAPI versionshttps://10. 10. 10. 10/wapi/v1. 0/?_schema## sampleshttps://10. 6. 1. 216/wapi/v2. 7/networkview?name=defaulthttps://10. 6. 1. 216/wapi/v2. 7/networkview?name=default&amp;_return_type=json## get the rpz url entryhttps://{{ infoblox_wapi_server }}/wapi/{{ infoblox_wapi_version }}/allrpzrecords?zone={{ infoblox_rpz_name }}&amp;_return_fields%2B=rpz_rule&amp;_return_as_object=1&amp;name={{ infoblox_url_name_to_add }} Sample API Calls: $ curl -k -s \ -H 'content-type: application/json' \ -X POST \ --user admin:adminadmin \  https://10. 6. 1. 216/wapi/v2. 7/record:rpz:a?_return_fields%2B=name,rp_zone&amp;_return_as_object=1  \ -d '{ name : serverblock. com. category-01 , ipv4addr : 1. 1. 1. 156 , rp_zone : category-01 }'  https://10. 6. 1. 216/wapi/v2. 11/record:rpz:a?_return_fields%2B=name,rp_zone&amp;_return_as_object=1 Appendix Infoblox Guide Automate Infoblox Infrastructure Using Ansible Ansible and Infoblox: Roles Deep Dive Infoblox and Ansible Integration  About Infoblox vNIOS Virtual Appliance for Microsoft Hyper-V    Installing vNIOS Virtual Appliance   The definitive list of REST examples"
    }, {
    "id": 13,
    "url": "http://localhost:4000/ansible-for-network",
    "title": "Ansible for Network Automation",
    "body": " References Privilege Escalation for Network Devices in Ansible Communication Protocols Network modules Ansible Network Playbooks Ansible Network Roles Task Reference     Using username and password for authentication   Reboot ios device   Backup eos   Change config   Add VLAN nxos   Add ACL (Access Control List)   Add config iso_config    Appendix     Download IOS Images   Read : How Network Automation is Different References Ansible for Network Automation Tutorial Ansible for Network Automation Red Hat : NETWORK AUTOMATION WITH ANSIBLE Automating Network VLAN Deployments with Ansible Networking with Ansible 104   ANSIBLE - VLAN PROVISIONING   Using Ansible to Mitigate Network VulnerabilitiesPrivilege Escalation for Network Devices in AnsibleSample environment variable ansible_connection: network_cliansible_network_os: iosansible_become: yesansible_become_method: enableCommunication Protocols      ansible_connection   Protocol   Requires   Persistent?         network_cli   CLI over SSH   network_os setting   yes       netconf   XML over SSH   network_os setting   yes       httpapi   API over HTTP/HTTPS   network_os setting   yes       local   depends on provider   provider setting   no   Network modulesArista EOS = eos_*Cisco IOS/IOS-XE = ios_*Cisco NX-OS = nxos_*Cisco IOS-XR = iosxr_*F5 BIG-IP = bigip_*F5 BIG-IQ = bigiq_*Juniper Junos = junos_*VyOS = vyos_* and modules as  *_facts *_command *_configAnd more Ansible Network PlaybooksSample Playbookf or ios - name: configure cisco routers hosts: routers connection: network_cli gather_facts: no vars:  dns:  8. 8. 8. 8 8. 8. 4. 4  tasks:  - name: configure hostname   ios_config:    lines: hostname {{ inventory_hostname }}  - name: configure DNS   ios_config:    lines: ip name-server {{dns}}Another one for interface config Ansible Network Roleshttps://galaxy. ansible. com/ansible-network network-engineThis role provides the foundation for building network roles by providing modules and plugins that are common to all Ansible Network roles. Galaxy config_manager This role is designed to provide a network platform agnostic approach to managing the active (running) configuration file on a remote device. This role requires one (or more) platform provider roles to execute properly. Galaxy Install roles ansible-galaxy install ansible-network. cisco_iosansible-galaxy install ansible-network. config_managerUpdate existing role ansible-galaxy install ansible-network. network_engine,v2. 7. 0 --forceTask ReferenceUsing username and password for authentication: - name: User usernname vars:  cli:   username: user1   password: password   transport: cli tasks:  - name: Test Login   ios_config:    provider:  {{ cli }}    .     . Reboot ios device: ---- name: reboot ios device cli_command:  command: reload  prompt:   - Save?   - confirm  answer:   - y   - y # To make sure the current connection to the network device  # is closed so that the socket can be reestablished to the network  # device after the reboot takes place. - name: reset the connection meta: reset_connection- name: Wait for the network device to reload wait_for_connection:  delay: 10Backup eos: Backup configuration ---- name: BACKUP NETWORK CONFIGURATIONS hosts: arista gather_facts: false tasks:  - name: BACKUP CONFIG   eos_config:    backup: yesBackup using cli_command  run arbitrary commands on network devices using cli_command---- name: RUN COMMAND AND PRINT TO TERMINAL WINDOW hosts: arista gather_facts: false tasks:  - name: RUN ARISTA COMMAND   cli_command:    command: show run   register: backup  - name: PRINT TO TERMINAL WINDOW   copy:    content:  {{backup. stdout}}     dest:  {{inventory_hostname}}. backup Change config: # varsshow_interfaces:  show ip interface brief backup:  show running-config save:  write memory ntp_commands: ntp server 192. 168. 1. 1---- name: CHANGE CONFIGURATION hosts: routers gather_facts: false tasks:  - name: LOAD NTP CONFIGURATION   cli_config:    config:  {{ntp_commands}}    notify:    - SAVE CONFIGURATION handlers:  - name: SAVE CONFIGURATION   cli_command:    command:  {{save}} # Show interface  - name: RUN SHOW COMMAND   cli_command:    command:  {{show_interfaces}}    register: command_outputAdd VLAN nxos: ---- name: deploy vlans hosts: cisco gather_facts: no  tasks:  - name: ensure vlans exist   nxos_vlan:    vlan_id: 100    admin_state: up    name: WEBAdd ACL (Access Control List): https://dodgydudes. se/ansible-net104/ Add config iso_config: ---- name: snmp ro/rw string configuration hosts: cisco gather_facts: no tasks:  - name: ensure snmp strings are present   ios_config:    lines:     - snmp-server community ansible-public RO     - snmp-server community ansible-private RWAppendix DEVNET developer. cisco. com -&gt; https://developer. cisco. com/site/sandbox/ -&gt; https://devnetsandbox. cisco. com/eg: IOS XE on CSR Latest Code Always On https://devnetsandbox. cisco. com/RM/Diagram/Index/38ded1f0-16ce-43f2-8df5-43a40ebf752e?diagramType=Topology add variables[routers:vars]ansile_user=developeransible_password=passwordansible_connection=network_cliansible_network_os=isoansible_port=8181 #if diff port Download IOS Images:  Where do I get IOS images? Virl. cisco. com"
    }, {
    "id": 14,
    "url": "http://localhost:4000/ansible-for-networking",
    "title": "Ansible for Network Automation",
    "body": " Ansible for Network Automation     First Lab to test GNS3 setup   Importing image to GNS3   Importing appliance to GNS3   Configure a router   NetworkAutomation Component   Ansible Network Modules and Adhoc commands    Ansible Ad Hoc commands: Ansible CLI Playbook Appendix     Driver is probably stuck stopping/starting   vboxmanage cli   Instal vmware workstation ubuntu   Add IP to VM         To configure a dynamic or Static IP address     Configure VLAN with IP     VLANs     assign to port     assign IP to VLAN           Appendix to take full backup     Configure HP Switch (5130)         Ansible Modules     Configuring password authentication for console login     Configuring SSH login on the device     Install HP Comware 7 Python Library     Install HP Ansible Modules     HOW TO ENABLE NETCONF ON COMWARE7:     Check Comware Ansible Documentation     Assign port with VLAN           Appendix Troubleshooting     SSH Error   discovered_interpreter_python”: “/usr/bin/python”   Ansible with netconf   Enable scp on IOS device   References   Ansible for Network Automation install GNS3 GUI install and setup GNS3 VMFirst Lab to test GNS3 setup:  Create a lab with 2 PC and 1 SW add IP address for PCs  PC1&gt; ip 10. 1. 1. 1 255. 255. 255. 0     test a ping from one PC to another remember to save device configurationImporting image to GNS3: Goto Edit -&gt; Preferences -&gt; Dynamips -&gt; ISO Routers  click New -&gt; Browse the image (bin) Choose Yes when ask for decompress remember to refer the Cisco/GNS3 site for minimum memory requirementImporting appliance to GNS3: https://docs. gns3. com/1_3RdgLWgfk4ylRr99htYZrGMoFlJcmKAAaUAc8x9Ph8/index. html  Download applicance file from websiteConfigure a router: sh ip int br          # show ip interface detailsconf t             # configure terminalcopy running-config startup-config                # save running configuration to startupNetworkAutomation Component:  download NetworkAutomation Component appliance from marketpalce which is a docker image with Ansible pre-installed. Ansible Network Modules and Adhoc commands: root@NetworkAutomation-1:~# ansible S1 -m raw -a  show version  -u david -kRef:  https://docs. ansible. com/ansible/latest/modules. html https://docs. ansible. com/ansible/latest/modules_by_category. html https://docs. ansible. com/ansible/latest/raw_module. htmlAnsible Ad Hoc commands:root@NetworkAutomation-1:~# ansible S1 -m raw -a  show version  -u david -kroot@NetworkAutomation-1:~# ansible S1 -m raw -a  show run  -u david -kroot@NetworkAutomation-1:~# ansible gns3-core -i . /gns3hosts -m raw -a  show version  -u david -kroot@NetworkAutomation-1:~# ansible gns3-core -i . /gns3hosts -m raw -a  show version  -u david -k | grep flash0root@NetworkAutomation-1:~# ansible gns3-core -i . /gns3hosts -m raw -a  show version  -u david -k | grep 'SUCCESS\|Software'root@NetworkAutomation-1:~# ansible gns3-core -i . /gns3hosts -m raw -a  show version  -u david -k | grep 'SUCCESS\|Version'root@NetworkAutomation-1:~# ansible gns3-core -i . /gns3hosts -m raw -a  show run  -u david -k | grep 'username'root@NetworkAutomation-1:~# ansible gns3-core -i . /gns3hosts -m raw -a  show run  -u david -k | grep 'SUCCESS\|username'root@NetworkAutomation-1:~# ansible gns3-core -i . /gns3hosts -m raw -a  show run  -u david -k | grep 'username' &gt; usernames. txtroot@NetworkAutomation-1:~# cat usernames. txtroot@NetworkAutomation-1:~# ansible gns3-core -i . /gns3hosts -m raw -a  show run  -u david -k &gt; shrun. txtroot@NetworkAutomation-1:~# cat shrun. txtroot@NetworkAutomation-1:~# more shrun. txtroot@NetworkAutomation-1:~# ansible gns3-core -i . /gns3hosts -m raw -a  show ver  -u david -k &gt; shver. txtroot@NetworkAutomation-1:~# more shver. txt | grep Versionroot@NetworkAutomation-1:~# more shver. txt | grep 'SUCCESS\|Version'root@NetworkAutomation-1:~# ansible all -i gns3hosts -m raw -a  show arp  -u david -k root@NetworkAutomation-1:~# ansible all -i gns3hosts -m raw -a  show arp  -u david -k | grep 71root@NetworkAutomation-1:~# ansible all -i gns3hosts -m raw -a  show arp  -u david -k | grep 'SUCCESS\|71'root@NetworkAutomation-1:~# ansible all -i gns3hosts -m raw -a  show arp  -u david -k | grep 'SUCCESS\|\. 71'root@NetworkAutomation-1:~# ansible all -i gns3hosts -m raw -a  show mac address-table  -u david -k root@NetworkAutomation-1:~# ansible all -i gns3hosts -m raw -a  show mac address-table  -u david -k | grep 7aroot@NetworkAutomation-1:~# ansible all -i gns3hosts -m raw -a  show mac address-table  -u david -k | grep 'SUCCESS\|fe7a'root@NetworkAutomation-1:~# ansible all -i gns3hosts -m raw -a  show mac address-table  -u david -k | grep 'SUCCESS\|fe7a'root@NetworkAutomation-1:~# cat gns3hostsAnsible CLI PlaybookAppendixHP : https://docs. gns3. com/appliances/hp-vsr1001. html Nested Virtualization - VirtualBoxhttps://www. virtualbox. org/manual/ch09. html#nested-virt Driver is probably stuck stopping/starting: Steps: Navigate to “C:\Program Files\Oracle\VirtualBox\drivers\vboxdrv”Right click on “VBoxDrv. inf” file and select Install optionOpen the Console as a administrator and run the following command vboxmanage cli: https://blog. scottlowe. org/2016/11/10/intro-to-vbox-cli/ vboxmanage startvm k8svboxmanage list runningvms https://www. virtualbox. org/manual/ch08. html#vboxmanage-modifyvm vboxmanage modifyvm Ubuntu –nested-hw-virt on Instal vmware workstation ubuntu: https://phoenixnap. com/kb/install-vmware-workstation-ubuntu Add IP to VM: sudo ip addr add 192. 168. 1. 14/24 dev eth0sudo ip link set dev eth0 upsudo ip route add default via 192. 168. 1. 1 sudo vi /etc/network/interfaces sudo /etc/init. d/networking restart To configure a dynamic or Static IP address: auto eth0iface eth0 inet dhcp## Or configure a static IPauto eth0iface eth0 inet static address 192. 168. 1. 14 gateway 192. 168. 1. 1 netmask 255. 255. 255. 0 network 192. 168. 1. 0 broadcast 192. 168. 1. 255Configure VLAN with IP: conf tint vlan 100ip address IP subnetVLANs: show vlansac-02#vlan database ac-02(vlan)# ac-02(config)#int vlan 100                    # create vlan interfaceac-02(config)#no interface vlan 100              # delete vlan interfaceassign to port: ac-02(config)#interface range fastEthernet 1/0 - 5ac-02(config-if-range)#switchport mode access ac-02(config-if-range)#switchport access vlan 100assign IP to VLAN: ac-02(config)#interface vlan 100ac-02(config-if)#ip address 10. 1. 10. 70 255. 255. 255. 0Appendixhttps://srijit. com/working-cisco-ios-gns3/ to take full backupterminal length 0show run Configure HP Switch (5130): https://support. hpe. com/hpsc/doc/public/display?docId=emr_na-c03182828#N10013http://patg. net/ansible,comware,switches/2014/10/16/ansible-comware/https://porter. io/github. com/HPENetworking/ansible-hpe-cw7 Ansible Modules:  https://hp-ansible. readthedocs. io/en/latest/list_of_All_modules. html https://github. com/HPENetworking/ansible-hpe-cw7Configuring password authentication for console login: &lt;HPE&gt; system-view[HPE] line aux 0[HPE-line-aux0] authentication-mode password# Password is password[HPE-line-aux0] set authentication password simple &lt;PASSWORD&gt;[HPE-line-aux0] user-role network-admin# save config[HPE-line-aux0] save forceSSH on vty [ds-01]line vty 0[ds-01-line-vty0]authentication-mode scheme [ds-01-line-vty0]set authentication password simple password[ds-01-line-vty0]protocol inbound ssh[ds-01-line-vty0]user-role network-adminConfiguring SSH login on the device:  Create a key```system-view System View: return to User View with Ctrl+Z. [ds-01]public-key local create rsa name ansiblekeyThe range of public key modulus is (512 ~ 2048). If the key modulus is greater than 512, it will take a few minutes. Press CTRL+C to abort. Input the modulus length [default = 1024]:Generating Keys. . . . Create the key pair successfully. ```2. Enable SSH```[ds-01]ssh server enable[ds-01]ssh user hp service-type all authentication-type password[ds-01]netconf ssh server enable``````[ds-01]sftp server enablescp server enable```Enabling NETCONF over SSH[ds-01]sftp server enableConfiguring the user lines for SSH loginConfiguring a client's host public key[ds-01]display public-key local rsa publicthen find the key and copy for next command[ds-01]public-key peer ansible-nwEnter public key view. Return to system view with  peer-public-key end  command. [ds-01-pkey-public-key-ansible-nw] [ds-01-pkey-public-key-ansible-nw]peer-public-key end3. Create an SSH user ```#local-user hp password simple hp123 service-type ssh http https authorization-attribute user-role network-admin#line vty 0 15 authentication-mode scheme user-role network-admin```And specify the authentication mode. By default, no SSH user is configured on the device. ```[ds-01]ssh user ansible service-type stelnet authentication-type password```1. Enter VTY line view or class view.  ```[ds-01]line vty 0[ds-01-line-vty0]authentication-mode scheme[ds-01-line-vty0]protocol inbound ssh```### Install HP Comware 7 Python LibraryWhile in a terminal session on your Linux machine, execute one of the following blocks of commands:Latest From Source```$ git clone https://github. com/HPENetworking/pyhpecw7. git$ cd pyhpecw7$ sudo python setup. py install```Latest Stable Release via PIP (not supported yet)$ sudo pip install pyhpecw7### Install HP Ansible ModulesFirst go back to your home directory. $ cdPerform a clone of this project. $ git clone https://github. com/HPENetworking/ansible-hpe-cw7Navigate to the new hp-ansible directory. $ cd hp-ansible### HOW TO ENABLE NETCONF ON COMWARE7:https://networkgeekstuff. com/networking/hp-networking-comware-netconf-interface-quick-tutorial-using-pythons-ncclient-and-pyhpecw7/Simple, here is a configuration snapshot that actually enables both NETCONF over SSH layer and creates a single user “admin” with password “admin” to access it. ```ssh server enablenetconf ssh server enablelocal-user admin class manage password simple admin service-type telnet ssh terminal authorization-attribute user-role network-adminline vty 0 15 authentication-mode scheme user-role network-operator idle-timeout 15 0```### Check Comware Ansible Documentation`$ ansible-doc -M library/ comware_vlan`### Assign port with VLAN```[ds-01]interface ge3/0[ds-01-GigabitEthernet3/0]port link-mode bridge[ds-01-GigabitEthernet3/0]port link-type access [ds-01-GigabitEthernet3/0]port access vlan 100```# Appendix- [Connect GNS3 to the Internet (local server)](https://docs. gns3. com/1vFs-KENh2uUFfb47Q2oeSersmEK4WahzWX-HrMIMd00/index. html)# Troubleshootinghttps://docs. ansible. com/ansible/latest/network/user_guide/network_debug_troubleshooting. html## SSH Error```root@NW-Auto-100:~/ansible# ssh cisco@\10. 1. 10. 51ssh: error while loading shared libraries: libgssapi_krb5. so. 2: cannot open shared object file: No such file or directory## Solutionroot@NW-Auto-100:~/ansible# /sbin/ldconfig -v```## discovered_interpreter_python :  /usr/bin/python ```fatal: [ac-02]: FAILED! =&gt; { ansible_facts : { discovered_interpreter_python :  /usr/bin/python },  changed : false,  msg :  [Errno -2] Name or service not known }```- https://www. toptechskills. com/ansible-tutorials-courses/how-to-fix-usr-bin-python-not-found-error-tutorial/## Ansible with netconf- [Netconf enabled Platform Options](https://docs. ansible. com/ansible/latest/network/user_guide/platform_netconf_enabled. html)## Enable scp on IOS devicescp ~/Downloads/c2960x-universalk9-mz. 152-2. E5. bin sysadmin1@192. 0. 2. 1:flash2:/Password:Administratively disabled. To fix that, get into config mode and run:ip scp server enableRef: https://www. ispcolohost. com/2016/08/16/scping-an-ios-image-to-a-cisco-2960xr-stack/- sometimes it happens due to encryption or integrity mismatch between SCP server and ASA. so you can support all cipher methods by the following 2 command (I face the same issue and TAC fixed bu those command)- ```ssh cipher encryption allssh cipher integrity all```## References- [Getting Started with Ansible for Network Automation](https://docs. ansible. com/ansible/latest/network/getting_started/index. html)- [Ansible for Network Automation](https://docs. ansible. com/ansible/latest/network/index. html)- [Ansible Network Examples](https://docs. ansible. com/ansible/latest/network/user_guide/network_best_practices_2. 5. html)- [Advanced Topics with Ansible for Network Automation](https://docs. ansible. com/ansible/latest/network/user_guide/)- [Platform Options](https://docs. ansible. com/ansible/latest/network/user_guide/platform_index. html)- [Netconf enabled Platform Options](https://docs. ansible. com/ansible/latest/network/user_guide/platform_netconf_enabled. html)- [Ansible for Network Automation Tutorial](https://www. networkcomputing. com/networking/ansible-network-automation-tutorial)"
    }, {
    "id": 15,
    "url": "http://localhost:4000/ansible-ovirt-rhv",
    "title": "Ansible and oVirt/Red Hat Virtualization",
    "body": "Modules:  rhevm - https://docs. ansible. com/ansible/latest/modules/rhevm_module. html     supports oVirt/RHEV version 3.     ovirt_vm - https://docs. ansible. com/ansible/latest/modules/ovirt_vm_module. html     need ovirt-engine-sdk-python &gt;= 4. 3. 0    sudo apt install python-pipapt-get install python-lxmlapt-cache depends python-pycurl# then install all dependanciessudo apt-get install libcurl4-gnutls-devsudo apt install libcurl4-gnutls-dev librtmp-devpip install ovirt-engine-sdk-python        Ref: https://github. com/oVirt/ovirt-engine-sdk/issues/18       https://calgaryrhce. ca/blog/2018/10/23/using-ansible-to-manage-rhv-ovirt/ rhevm: - name: Basic get info from VM rhevm:  server: rhevm01  user: '{{ rhev. admin. name }}'  password: '{{ rhev. admin. pass }}'  name: demo  state: info- name: Basic create example from image rhevm:  server: rhevm01  user: '{{ rhev. admin. name }}'  password: '{{ rhev. admin. pass }}'  name: demo  cluster: centos  image: centos7_x64  state: present- name: Power management rhevm:  server: rhevm01  user: '{{ rhev. admin. name }}'  password: '{{ rhev. admin. pass }}'  cluster: RH  name: uptime_server  image: centos7_x64  state: downovirt_vm:  use ovirt_auth module to reuse authentication- name: Creates a new Virtual Machine from template named 'rhel7_template' ovirt_vm:  state: present  name: myvm  template: rhel7_template  cluster: mycluster- name: Register VM ovirt_vm:  state: registered  storage_domain: mystorage  cluster: mycluster  name: myvmAll modules:  ovirt - oVirt/RHEV platform management ovirt_affinity_group - Module to manage affinity groups in oVirt/RHV ovirt_affinity_label - Module to manage affinity labels in oVirt/RHV ovirt_affinity_label_facts - Retrieve facts about one or more oVirt/- RHV affinity labels ovirt_api_facts - Retrieve facts about the oVirt/RHV API ovirt_auth - Module to manage authentication to oVirt/RHV ovirt_cluster - Module to manage clusters in oVirt/RHV ovirt_cluster_facts - Retrieve facts about one or more oVirt/RHV - clusters ovirt_datacenter - Module to manage data centers in oVirt/RHV ovirt_datacenter_facts - Retrieve facts about one or more oVirt/RHV - datacenters ovirt_disk - Module to manage Virtual Machine and floating disks in - oVirt/RHV ovirt_disk_facts - Retrieve facts about one or more oVirt/RHV disks ovirt_external_provider - Module to manage external providers in oVirt/- RHV ovirt_external_provider_facts - Retrieve facts about one or more oVirt/- RHV external providers ovirt_group - Module to manage groups in oVirt/RHV ovirt_group_facts - Retrieve facts about one or more oVirt/RHV groups ovirt_host_networks - Module to manage host networks in oVirt/RHV ovirt_host_pm - Module to manage power management of hosts in oVirt/RHV ovirt_host_storage_facts - Retrieve facts about one or more oVirt/RHV - HostStorages (applicable only for block storage) ovirt_hosts - Module to manage hosts in oVirt/RHV ovirt_hosts_facts - Retrieve facts about one or more oVirt/RHV hosts ovirt_mac_pools - Module to manage MAC pools in oVirt/RHV ovirt_networks - Module to manage logical networks in oVirt/RHV ovirt_networks_facts - Retrieve facts about one or more oVirt/RHV - networks ovirt_nics - Module to manage network interfaces of Virtual Machines - in oVirt/RHV ovirt_nics_facts - Retrieve facts about one or more oVirt/RHV virtual - machine network interfaces ovirt_permissions - Module to manage permissions of users/groups in - oVirt/RHV ovirt_permissions_facts - Retrieve facts about one or more oVirt/RHV - permissions ovirt_quotas - Module to manage datacenter quotas in oVirt/RHV ovirt_quotas_facts - Retrieve facts about one or more oVirt/RHV quotas ovirt_scheduling_policies_facts - Retrieve facts about one or more - oVirt scheduling policies ovirt_snapshots - Module to manage Virtual Machine Snapshots in oVirt/- RHV ovirt_snapshots_facts - Retrieve facts about one or more oVirt/RHV - virtual machine snapshots ovirt_storage_connections - Module to manage storage connections in - oVirt ovirt_storage_domains - Module to manage storage domains in oVirt/RHV ovirt_storage_domains_facts - Retrieve facts about one or more oVirt/- RHV storage domains ovirt_storage_templates_facts - Retrieve facts about one or more oVirt/- RHV templates relate to a storage domain.  ovirt_storage_vms_facts - Retrieve facts about one or more oVirt/RHV - virtual machines relate to a storage domain.  ovirt_tags - Module to manage tags in oVirt/RHV ovirt_tags_facts - Retrieve facts about one or more oVirt/RHV tags ovirt_templates - Module to manage virtual machine templates in oVirt/- RHV ovirt_templates_facts - Retrieve facts about one or more oVirt/RHV - templates ovirt_users - Module to manage users in oVirt/RHV ovirt_users_facts - Retrieve facts about one or more oVirt/RHV users ovirt_vmpools - Module to manage VM pools in oVirt/RHV ovirt_vmpools_facts - Retrieve facts about one or more oVirt/RHV - vmpools ovirt_vms - Module to manage Virtual Machines in oVirt/RHV ovirt_vms_facts - Retrieve facts about one or more oVirt/RHV virtual machines"
    }, {
    "id": 16,
    "url": "http://localhost:4000/ansible-for-vmware",
    "title": "Ansible for VMware",
    "body": "Ref: VMware Guide for Ansible  1. VMware Prerequisites     1. 1. vmware_guest module         1. 1. 1. Installing vCenter SSL certificates for Ansible     1. 1. 2. Installing ESXi SSL certificates for Ansible           2. Appendix     2. 1. VMWware Templates   2. 2. Enable custom scripts in vmware-tools   2. 3. vCenter Simumlator Container   2. 4. Useful Links    1. VMware Prerequisites      Ansible VMware modules are written on top of pyVmomi which is the Python SDK for the VMware vSphere API that allows user to manage ESX, ESXi, and vCenter infrastructure. curl https://bootstrap. pypa. io/get-pip. py -o get-pip. pypython get-pip. py $ pip install pyvmomi1. 1. vmware_guest module: The vmware_guest module manages various operations related to virtual machines in the given ESXi or vCenter server. 1. 1. 1. Installing vCenter SSL certificates for Ansible: From any web browser, go to the base URL of the vCenter Server without port number like https://vcenter-domain. example. comClick the “Download trusted root CA certificates” link at the bottom of the grey box on the right and download the file. Change the extension of the file to . zip. The file is a ZIP file of all root certificates and all CRLs. Extract the contents of the zip file. The extracted directory contains a . certs directory that contains two types of files. Files with a number as the extension (. 0, . 1, and so on) are root certificates. Install the certificate files are trusted certificates by the process that is appropriate for your operating system. 1. 1. 2. Installing ESXi SSL certificates for Ansible: Enable SSH Service on ESXi either by using Ansible VMware module vmware_host_service_manager or manually using vSphere Web interface. SSH to ESXi server using administrative credentials, and navigate to directory /etc/vmware/sslSecure copy (SCP) rui. crt located in /etc/vmware/ssl directory to Ansible control node. Install the certificate file by the process that is appropriate for your operating system. 2. Appendix2. 1. VMWware Templates: https://www. nakivo. com/blog/vm-templates-a-to-z/ 2. 2. Enable custom scripts in vmware-tools: vmware-toolbox-cmd config set deployPkg enable-custom-scripts true Ref: Create a Customization Specification for Linux 2. 3. vCenter Simumlator Container:  vCenter Simulator Docker Container vCenter and ESi API based simulator ansible / vcenter-test-container vcenter-test-container2. 4. Useful Links:  Enum - VirtualMachineGuestOsIdentifier"
    }, {
    "id": 17,
    "url": "http://localhost:4000/molecule",
    "title": "Molecule for Testing Ansible Roles",
    "body": "Setting up your environmentPython Virtual Environment (Optional): Note: I am using Python Virtual environment for my Ansible Development and testing which is safe and I can test multiple versions of Ansible without breaking exisitng setup. Read my post How to set up and use Python virtual environments for Ansible in Red Hat Sysadmin blog. Configure Podman for Mac (Optional): This is for MacOS as podman is not natively work on Mac but you can use podman with a virtual machine or remote VM. # Install podman$ brew install podman# start the Podman-managed VMpodman machine initpodman machine start# check statuspodman info Using podman-machine on MacOS Podman Installation InstructionsInstalling Molecule$ pip install molecule$ molecule --versionmolecule 3. 4. 0 using python 3. 7   ansible:2. 9. 0  delegated:3. 4. 0 from moleculeMolecule DriversMolecule uses drivers to bring up Ansible ready hosts to operate on. Currently, Molecule supports three drivers: Vagrant, Docker, and OpenStack. The driver is set to vagrant by default if the --docker flag is not passed when molecule init is run. Make sure you have proper plugin or tools installed to use these drivers. For example, to use vagrant as driver, install the appropriate python library. $ pip install python-vagrant$ pip install molecule-vagrant# Install docker library if you are using podman$ pip install  molecule[docker]  # Install podman library if you are using podman$ pip install  molecule[vagrant]  # Install podman library if you are using podman$ pip install  molecule[podman]  $ pip install  molecule[lint] Create a new Role with Molecule# init role with podman drivermolecule init role newrole --driver-name=podman# init role without a driver$ molecule init role ginigangadharan. hello-demo      INFO   Initializing new role ginigangadharan. hello-demo. . . INFO   Initialized role in /Users/gini/workarea/ansible-collection-custom-modules/roles/ginigangadharan. hello-demo successfully. $ ls -l ginigangadharan. hello-demo/molecule/default total 48-rw-r--r-- 1 gini staff  280 10 Sep 15:14 INSTALL. rst-rw-r--r-- 1 gini staff  155 10 Sep 15:14 converge. yml-rw-r--r-- 1 gini staff 1066 10 Sep 15:14 create. yml-rw-r--r-- 1 gini staff  567 10 Sep 15:14 destroy. yml-rw-r--r-- 1 gini staff  142 10 Sep 15:14 molecule. yml-rw-r--r-- 1 gini staff  177 10 Sep 15:14 verify. ymlWhen you execute above command,  ‘ansible-galaxy` command will be used in the backend.  Add a molecule directory inside the role Configure the role to run tests in a vagrant environmentUpdate your molecule. yml: ---dependency: name: galaxydriver: name: vagrantplatforms: - name: instance  box: fedora/32-cloud-base  memory: 512  cpus: 1provisioner: name: ansibleverifier: name: ansibleReferences Testing your Ansible roles with Molecule Continuous Testing with Molecule, Ansible, and GitHub Actions Molecules - Ansible for DevOps by Jeff Geerling Setting up Molecule for testing Ansible roles with Vagrant and Testinfra"
    }, {
    "id": 18,
    "url": "http://localhost:4000/ansible-questions",
    "title": "Ansible - Questions and References",
    "body": "Note: These are questions I have received via chat groups and communities. This is a living document and I will update the page whenever there is a new question or better answer or references.  What is Ansible What is Ansible Tower Why Ansible is different than other automation tools Can I use Ansible for Auto Remediation ? What is Instance Groups and Isolated Nodes in Ansibe Tower ?What is Ansible: Ansible is an open source automation tool with simple automation language (YAML) What is Ansible Tower: Ansible Tower is an enterprise framework for controlling, securing and managing your Ansible Automation with UI and RESTful API Why Ansible is different than other automation tools: There are other automation tools like Puppet, Chef, Saltstack etc but they all need a agent to be run on managed nodes. Ansible is agent-less; as long as Ansible node have access to managed node via SSH, API etc, Ansible can manage that node. Reference Can I use Ansible for Auto Remediation ?: Yes, you can use Ansible for auto-remediation; but keep in mind that someone has to trigger the Ansible job either manually or via some integrated example. Eg:  Step 1: Your monitoring software (nagios, icinga, zabbix, Prometheus etc) detects an issue on system configuration (like unauthorized sudo user, or low disk space).  Step 2: Monitoring tool will create a ticket (ServiceNow, Jira, Slack etc) Step 3: Monitoring tool will trigger a call to Ansible Job (custmized ) to remediate.  Step 4: Ansible will run the job on target node and return result to Monitoring/Ticketing Tool. What is Instance Groups and Isolated Nodes in Ansibe Tower ?: An Ansible Tower Instance group is a set of cluster nodes dedicated for a particular purpose. You can organize your Ansible Tower Cluster into any number of instance groups, and cluster nodes can exist in multiple instance groups. Each instance group has its own job queue, and any node in the group can take jobs off of that queue. An Isolated Node is an Ansible Tower node that contains a small piece of software for running playbooks locally to manage a set of infrastructure. It can be deployed behind a firewall/VPC or in a remote datacenter, with only SSH access available.  Instance Groups and Isolated Nodes - Ansible Blog"
    }, {
    "id": 19,
    "url": "http://localhost:4000/samdoran-pgsql-replication-role-notes",
    "title": "Issues and Fixes for Ansible PostgreSQL Replication Role",
    "body": "Sam Doran contributed this Ansible role for PostgreSQL Repliction, especially for Database Streaming Replication for Ansible Tower Cluster. Some of the issues are already fixed and good to go. Note: I have moved this note to samdoran-pgsql-replication-role-notes "
    }, {
    "id": 20,
    "url": "http://localhost:4000/ansible-tower",
    "title": "Ansible Tower - Load Balancing",
    "body": "1. Ansible Tower Installation 1. Ansible Tower Installation     1. 1. Ansible Tower Installation and Reference Guide v3. 6. 1   1. 2. Ansible Tower Quick Installation Guide v3. 6. 1    2. Appendix     2. 1. Ansible Tower: RabbitMQ Monitoring   1. 1. Ansible Tower Installation and Reference Guide v3. 6. 1: (https://docs. ansible. com/ansible-tower/latest/html/installandreference/index. html)[https://access. redhat. com/articles/3344101] Guidelines, Troubleshooting, and Recommended Configurations for Ansible Tower 1. 2. Ansible Tower Quick Installation Guide v3. 6. 1: https://docs. ansible. com/ansible-tower/latest/html/quickinstall/index. html 2. Appendix2. 1. Ansible Tower: RabbitMQ Monitoring: https://access. redhat. com/articles/3394961  Ansible Tower High Availability and Disaster Recoveryhttps://access. redhat. com/solutions/3110791https://keithtenzer. com/2017/12/12/ansible-tower-cluster-configuration-guide/https://100things. wzzrd. com/2017/03/13/Building-a-highly-available-Ansible-Tower-cluster. html https://www. jeffgeerling. com/blog/2019/run-ansible-tower-or-awx-kubernetes-or-openshift-tower-operator "
    }, {
    "id": 21,
    "url": "http://localhost:4000/ansible-troubleshooting",
    "title": "Troubleshooting Ansible",
    "body": " Error with Self Signed SSL Cert on SCM server Error:module ‘enum’ has no attribute ‘IntFlag’ Error with nosuid when read/write filesError with Self Signed SSL Cert on SCM server: Error : Peer’s certificate issuer has been marked as not trusted by the user {   stderr_lines : [     fatal: unable to access 'https://$encrypted$:$encrypted$@vm-gitnode-01. lab. local/ansible/network-automation. git/': Peer's certificate issuer has been marked as not trusted by the user.    ],   cmd :  /usr/bin/git clone --origin origin 'https://$encrypted$:$encrypted$@vm-gitnode-01. lab. local/ansible/network-automation. git' /var/lib/awx/projects/_8__network_poc ,   _ansible_no_log : false,   stdout :  Cloning into '/var/lib/awx/projects/_8__network_poc'. . . \n ,   changed : false,   invocation : {     module_args : {       force : false,       track_submodules : false,       reference : null,       dest :  /var/lib/awx/projects/_8__network_poc ,       umask : null,       clone : true,       gpg_whitelist : [],       accept_hostkey : false,       update : true,       ssh_opts : null,       repo :  https://$encrypted$:$encrypted$@vm-gitnode-01. lab. local/ansible/network-automation. git ,       bare : false,       archive : null,       refspec : null,       executable : null,       remote :  origin ,       recursive : true,       separate_git_dir : null,       verify_commit : false,       depth : null,       version :  HEAD ,       key_file : null    }  },   stderr :  fatal: unable to access 'https://$encrypted$:$encrypted$@vm-gitnode-01. lab. local/ansible/network-automation. git/': Peer's certificate issuer has been marked as not trusted by the user. \n ,   rc : 128,   _ansible_delegated_vars : {     ansible_host :  localhost   },   stdout_lines : [     Cloning into '/var/lib/awx/projects/_8__network_poc'. . .    ],   msg :  fatal: unable to access 'https://$encrypted$:$encrypted$@vm-gitnode-01. lab. local/ansible/network-automation. git/': Peer's certificate issuer has been marked as not trusted by the user.  }SolutionResolutionSSL certificate validation can be prevented for Git connections originating from Tower by adding the following settings in the Tower UI at Settings » Configure Tower » Jobs in JSON format: {  GIT_SSL_NO_VERIFY :  True }Reference Note : For cloning repo, use below methods git config --global http. sslverify false#orexport GIT_SSL_NO_VERIFY=trueError:module ‘enum’ has no attribute ‘IntFlag’: {   exception :  Traceback (most recent call last):\n File \ /var/lib/awx/. ansible/tmp/ansible-tmp-1599103337. 21-17-81526385316505/AnsiballZ_fortios_facts. py\ , line 102, in &lt;module&gt;\n  _ansiballz_main()\n File \ /var/lib/awx/. ansible/tmp/ansible-tmp-1599103337. 21-17-81526385316505/AnsiballZ_fortios_facts. py\ , line 17, in _ansiballz_main\n  import base64\n File \ /usr/lib64/python3. 6/base64. py\ , line 9, in &lt;module&gt;\n  import re\n File \ /usr/lib64/python3. 6/re. py\ , line 142, in &lt;module&gt;\n  class RegexFlag(enum. IntFlag):\nAttributeError: module 'enum' has no attribute 'IntFlag'\n ,   _ansible_no_log : false,   _ansible_delegated_vars : {     ansible_host :  localhost   },   module_stderr :  Traceback (most recent call last):\n File \ /var/lib/awx/. ansible/tmp/ansible-tmp-1599103337. 21-17-81526385316505/AnsiballZ_fortios_facts. py\ , line 102, in &lt;module&gt;\n  _ansiballz_main()\n File \ /var/lib/awx/. ansible/tmp/ansible-tmp-1599103337. 21-17-81526385316505/AnsiballZ_fortios_facts. py\ , line 17, in _ansiballz_main\n  import base64\n File \ /usr/lib64/python3. 6/base64. py\ , line 9, in &lt;module&gt;\n  import re\n File \ /usr/lib64/python3. 6/re. py\ , line 142, in &lt;module&gt;\n  class RegexFlag(enum. IntFlag):\nAttributeError: module 'enum' has no attribute 'IntFlag'\n ,   changed : false,   module_stdout :   ,   rc : 1,   msg :  MODULE FAILURE\nSee stdout/stderr for the exact error }Solution https://access. redhat. com/solutions/4282031 Error with nosuid when read/write files: sudo: effective uid is not 0, is /usr/bin/sudo on a file system with the 'nosuid' option set or an NFS file system without root privileges?Solution It isn’t possible to use Tower with local action to escalate to the root user. It will be necessary to alter your task to connect via SSH and then escalate to root using another user(not AWX). This is done purposefully to avoid security risks associated with our user having root level access to the system. NOTE : It is not recommended that sudo access be given to AWX user. You’ll need to adjust your playbook to SSH rather than use a local connection. Root Cause: The AWX service user is intentionally restricted from sudo operations. https://access. redhat. com/solutions/3223501 "
    }, {
    "id": 22,
    "url": "http://localhost:4000/ansible-use-case-gallery",
    "title": "Ansible Use Case Gallery",
    "body": " Immutable Infrastructure Provisioning     AWS    Application Deployment Security Management Manage network configurations Weekly system reboot Enforce security guidelines Configuration Management Disaster recovery Adhoc Commands Database     Ansible Oracles Modules   Database binary patching    Service license agreementsImmutable Infrastructure:  Configure your re-buildable replicas of server set or application stack.  Use Ansible for Infrastructure as Code (IaC)Provisioning: Provision your bare-metal, private cloud and public cloud infrastructure using modules and configure them to use for day 2 operations. AWS:  Instance Provisioning LoadBalancer / TargetGroup Creation VPC CreationApplication Deployment: Manage your applications deployments, re-deployments, migration using simple ansible playbooks; and implement DevSecOps methods in your infrastructure operations. Security Management:  Incident Response Manage network configurations:  Configure new network devices with same configurations as we need Add new rules or changes in configurations (eg: new VLAN, new Virtual Interface) Add/Remove rule in firewallWeekly system reboot: Eliminate repetitive, manual processes with automation. Enforce security guidelines: Rules are rules. It’s best to automate in an effort to achieve strict security standards. Configuration Management: Keep your configurations as a code and also can check if someone made chages to the same. Disaster recovery: Disaster recovery can involve a wide range of components. Act across different variables of the technology stack to identify problems and eliminate cross team dependencies. Adhoc Commands: Remarkably easy to write, you can run commands across your environment for any number of servers. Database: Ansible Oracles Modules:  ansible-oracle-modulesDatabase binary patching: Several databases use outdated binary sets. Patch the binaries in accordance with the release of the latest patch. Service license agreements: Mistakes cost time and money. Eliminate errors that can crop up in detailed software contracts. "
    }, {
    "id": 23,
    "url": "http://localhost:4000/ansible-wait-for-samples",
    "title": "Ansible wait for methods",
    "body": "Ansible sample methods to wait for a machine to boot or a port to be ready.   - name: Waits for SSH, don't start checking for 5 seconds   wait_for:    host:  {{ inventory_hostname }}     port: 22    delay: 10    timeout: 300    state: started  - name: Wait for VMs to boot up (300, 5)   wait_for_connection:    delay: 5    timeout: 300    sleep: 5  - name: check SSH access   local_action: shell ansible -u {{ ansible_user_id }} -m ping {{ inventory_hostname }}   register: result   until: result. rc == 0   retries: 30   delay: 10  - name: Wait for VMs to boot with SSH   local_action:    module: wait_for host={{ inventory_hostname }} port=22 delay=10 connect_timeout=5 timeout=300"
    }, {
    "id": 24,
    "url": "http://localhost:4000/ansible-windows",
    "title": "Ansible for Windows",
    "body": "Patching windows servers through Ansible "
    }, {
    "id": 25,
    "url": "http://localhost:4000/ansible",
    "title": "Ansible - Learning Docs & References",
    "body": "Ansible for Absolute Beginners:  Ansible for Absolute Beginners - 30 Days of Ansible (YouTube) Ansible Real Life Scenarios (YouTube)Ansible Guides: Automation with Ansible – All You Want to Learn You have hundreds and thousands of online documentation and wiki pages for learning Ansible, and this is my attempt to cover Ansible in a simple and beginner level approach. Access all chapters on techbeatly. com.  Introduction to Ansible Installing Ansible Deploying Ansible Managing Ansible Inventory Running Ad-Hoc commands Play with Playbooks Managing Ansible Variables Managing Ansible Facts (Task Control &lt;In Progress … &gt;) (Jinja2 Templates&lt;In Progress … &gt;) Implementing Roles Deploying Roles With Ansible Galaxy (In progres)Table of Contents  Ansible for Absolute Beginners     Ansible Guides    Ansible - Frequently Asked Questions Ansible Automation Platform (Ansible Tower)     Installing Ansible Automation Platform (Ansible Tower)         Enable RHEL and AAP Subscriptions     Prepare for Ansible Automation Platform Installation     Get Registry Credential     Configure Ansible Automation Platform Installation Inventory     Run Setup Script          Troubleshooting   References    Ansible Playbook References Tools for Ansible Ansible for Network Automation Ansible and Python Modules     Ansible and Python 3    Ansible Blog/Articles to Follow Ansible Interview Questions Ansible Modules Other References Ansible for IBM Power Ansible for Arista Networks Ansible for CIS Hardening/CIS CheckAnsible - Frequently Asked Questions:  What is Ansible? What is IaC? What can Ansible do? What are the advantages of Ansible? How does Ansible work? What is a Playbook? Are there any requirements for using Ansible? What is DevOps? How does Ansible fit into DevOps? Who is Ansible for? Who should learn Ansible? What are prerequisites to learning Ansible?Ansible Automation Platform (Ansible Tower): Installing Ansible Automation Platform (Ansible Tower): Note : The Ansible Automation Platform installer only supports Red Hat Enterprise Linux and CentOS. Enable RHEL and AAP Subscriptions: Make sure you subscribed to Red Hat and added RHEL Subscription. # subscription-manager register# subscription-manager attach --pool=&lt;pool_id of RHEL subscription&gt;# subscription-manager list --consumedNow, search for Ansible Automation Platform subscription and attach the pool ID. # subscription-manager list --available --all | grep  Ansible Automation Platform  -B 3 -A 6# subscription-manager attach --pool=&lt;pool_id&gt;# subscription-manager list --consumedUpdate required repo for AAP 2. 1 # subscription-manager repos \ --disable=ansible-automation-platform-2. 0-early-access-for-rhel-8-x86_64-rpms# subscription-manager repos \ --enable=ansible-automation-platform-2. 1-for-rhel-8-x86_64-rpmsPrepare for Ansible Automation Platform Installation:  Download the latest Ansible Automation Platform Installation Program from access. redhat. com/downloads or from releases. ansible. com. Refer Download the Ansible Automation Platform Installation Program for more details. Note: You can download either installation package or bundled package (for disconnected setup).  Extract the content$ tar xvzf ansible-tower-setup-latest. tar. gz$ cd ansible-tower-setup-&lt;tower_version&gt;Get Registry Credential: You need access to Red Hat Container Registry to fetch the continer images for automation controller and execution environment. You can create a service account in Red Hat registry for the same and use the credential in AAP installation inventory. # Execution Environment Configuration# Credentials for container registry to pull execution environment images from,# comment out registry_username if authentication is not requiredregistry_url='registry. redhat. io'registry_username='YOUR_SERICE_ACCOUNT_USERNAME'registry_password='YOUR_SERICE_ACCOUNT_PASSWORD'Read more Registry Service Account Management Application Configure Ansible Automation Platform Installation Inventory: Run Setup Script: # . /setup. shYou can ignore ignore_preflight_errors if you are testing or preparing home labs. # . /setup. sh ignore_preflight_errors=trueTroubleshooting:  Resolve error occurred while running Ansible Tower installationReferences:  Explore Ansible Automation Platform - Hands on Lab/Practices via instruqt.  Red Hat Ansible Automation Platform installation guide - 2.  Red Hat Ansible Automation Platform installation guide - 2. 0(2. 0-EA) Ansible Automation Platform 2. 0 Early Access Homepage What’s New in Ansible Automation Controller 4. 0   What Happens to an Ansible Tower Installation After the Subscription/License has Expired?   Red Hat Ansible Automation Platform Life Cycle Red Hat Ansible Tower Life Cycle - Details about Ansible Tower and Ansible Automation Platform Life Cycle, End of Life, End of Support etc.  Ansible Tower Installation and Reference Guide Preparing for the Ansible Automation Platform Installation Ansible Automation Platform Quick Installation Guide v3. 8. 0 Installing Ansible Automation Platform Ansible Tower Administration Guide (3. 8. 3) [PDF] Which Ports and Services Should I Monitor for Ansible Tower? Ansible Automation Platform Installation and Reference Guide v3. 8. 0 INSTALLING PRIVATE AUTOMATION HUB Tower - Previous versions Guidelines, Troubleshooting, and Recommended Configurations for Ansible TowerAnsible Playbook References:  Ansible Examples(Forked from @ansible) Ansible for DevOps (Forked from geerlingguy) Ansible NXOS Samples (Network Automation)Tools for Ansible:  Ansible SiloAnsible for Network Automation:  Network Device Authentication with Ansible 2. 3Ansible and Python Modules:  How to set up and use Python virtual environments for Ansible Running in a virtualenv How to install pip on Red Hat Enterprise Linux?pip install --target=/your/pyinstalldir loremipsumpip install fortiosapi --target=/var/lib/awx/venv/ansible/lib/python2. 7/site-packagespip list --target=/var/lib/awx/venv/ansible/lib/python2. 7/site-packages# subscription-manager repos --enable rhel-server-rhscl-7-rpms# yum install python27-python-pipSwitch to a normal user and check the pip $ scl enable python27 bash$ which pip$ pip -VFor Existing Tower # source /var/lib/awx/venv/ansible/bin/activate# umask 0022# pip install --upgrade pywinrm# deactivateAnsible and Python 3:  Python 3 Support How Can I Use Virtual Environment with Python3 on RHEL 7 for Ansible Tower? How do I use Python 3 in Ansible Tower? Ansible and Python 3 (docs. ansible. com/)Ansible 2. 5 and above work with Python 3. [ansible@vm-ans-02 ~]$ ansible --version |grep python ansible python module location = /usr/lib/python2. 7/site-packages/ansible python version = 2. 7. 5 (default, Sep 26 2019, 13:23:47) [GCC 4. 8. 5 20150623 (Red Hat 4. 8. 5-39)]Ansible Blog/Articles to Follow:  MyDailyTutorials Ansible Blog Ansible &amp; Cisco ansiblejunky. com ansible linting - Abhijeet KambleAnsible Interview Questions:  Question Bank 1 Question Bank 2 Question Bank 3Ansible Modules:  Foreman Ansible ModulesOther References:  Red Hat Ansible Tower Life Cycle Red Hat Ansible Automation Platform Workshops DEEP DIVE INTO ANSIBLE NETWORK RESOURCE MODULEAnsible for IBM Power:  IBM Power Systems AIX collectionAnsible for Arista Networks:  Ansible Modules for Arista CloudVision PlatformAnsible for CIS Hardening/CIS Check:  DevSec Hardening Framework / devops + security - Server Hardening Automation"
    }, {
    "id": 26,
    "url": "http://localhost:4000/author-gini",
    "title": "gini",
    "body": "                        {{page. title}} Follow:         {{ site. authors. gini. site }}         {{ site. authors. gini. bio }}                                   Posts by {{page. title}}:       {% assign posts = site. posts | where: author , gini  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 27,
    "url": "http://localhost:4000/awesome-tools",
    "title": "Awesome DevSecSysOps Tools on Internet",
    "body": "This is a page where I keep all those list of tools on internet, which we use for our day-to-day tasks.  Kubernetes Security and Scanning Utilities Cloud Management Network Tools     Unattened Remote Access    Diagram and Flowcharts Maker Collaboration Tools Email/SMTP Servers with Free Trials Meeting ToolsKubernetes:  Refer Kubernetes Tools Collection - Tools, Learning Guides, ReferencesSecurity and Scanning:  Snort - Network Intrusion Detection &amp; Intrusion Prevention SystemUtilities:  git. io - Shorten any github. com url (URL Shortner) jsonpath tester - Test your Jsonpath queries easily with jason data. Very useful when you handle output in Ansible, Kubernetes, OpenShift etc.  asciinema. org - Record and share your terminal sessions, the right way. Cloud Management:  ManageIQ - Hybrid Cloud Management cloudbolt - Hybrid Cloud ManagementNetwork Tools:                Websites For Subnet Calculation - Calculate and Understand Network subnets             Subnet Masks Sheet - Quick Reference Sheet     ngrok. com - public dns for your local machines   Dynamic IP with noip. comUnattened Remote Access:  zerotier. com - Easily connect cloud, mobile, desktop, and data center resources anywhere.  Apache Guacamole™Diagram and Flowcharts Maker:  Draw. io - Online tool Diagrams and Flowchart asciiflow - Online tool for ASCII Charater Flowchart / Diagrams Cloudcraft - Tool for making AWS Related diagrams and charts AWWAPP - Online Whiteboard - Quick Whiteboards for explanations.  Carbon - Create and share beautiful images of your source code. Start typing or drop a file into the text area to get started. Collaboration Tools:  mattermost - Mattermost is an open-source, self-hostable online chat service with file sharing, search, and integrations. It is designed as an internal chat for organisations and companies, and mostly markets itself as an open-source alternative to Slack Open Source Catalogue Manager - Cloud Services Management Software where you can provide easy to access and self service poral for your underlying infrastructure. Email/SMTP Servers with Free Trials: This is useful for testing email functionality from your application or solutions as you dont need to setup an SMTP server by your own.  SendinBlue mailtrap HTML Minifier - Minify HTML and any CSS or JS included in your markupMeeting Tools:  Egg Timer - a simple countdown timer. Image : unsplash. com/@barnimages "
    }, {
    "id": 28,
    "url": "http://localhost:4000/aws-summit-online-2020",
    "title": "AWS Summit Online - 2020",
    "body": "Hands on Labs:  Build, Run and Deploy a Containerized Web Application using Docker and Amazon Elastic Container Service (ECS) - https://github. com/bikrambora/docker-ecs-lab Building Organization Service Health Check Solution with Cloud9 and QuickSight - https://github. com/JerryChenZeyun/aws-health-api-organization-view Build a contact center for booking and checking appointment with AWS Connect/Lex/Lambda - https://github. com/phonghuule/connect-lex-integration-bookappointment Deploy and Expose Game 2048 on EKS using Kubernetes Ingress with AWS ALB Ingress Controller - https://github. com/starchx/devlab-eks-alb-2048game Creating a simple recommendation engine with Amazon Neptune - https://github. com/tsengsy/aws-labs/blob/master/neptune/neptune. md Amazon Personalize Batch Recommendations Lab - https://github. com/dalacan/personalize-batch-recommendations Dev Labs: Build a Sentiment Analysis App in minutes using Amplify Framework - https://github. com/rahulbaisla/sentimentAnalysisLab EKS FARGATE DEV LAB - https://github. com/kmhabib/fargatedevlab/blob/master/FargateDevLab. md A/B testing with lambda@edge - https://github. com/justasitsounds/lambda-edge-lab Predicting Customer Churn with Amazon SageMaker - https://github. com/phonghuule/sagemaker-churn Build a Website Login page with AWS ELB and AWS Cognito - https://github. com/YecineA/elb-authentication-cognito Predicting Customer Churn with Amazon SageMaker - https://github. com/phonghuule/sagemaker-churn Deploy a Locust load generator in ECS using CDK Python - https://github. com/tynooo/python-cdk-locust Build a modern serverless web application in minutes using the AWS Amplify Framework - https://github. com/phonghuule/voterocket-lab Creating a recommendation using Amazon Personalize - https://github. com/tsengsy/aws-labs/blob/master/personalise/personalise. md"
    }, {
    "id": 29,
    "url": "http://localhost:4000/opensource-tools-best-practices",
    "title": "Best Practices for using OpenSource Tools in Enterprises",
    "body": "These are purely personal notes - but publicly available - and do not blindly follow the items.  Consider paid Opensource services as well Be a contributor to the Community Implement the best class Application monitoring System Check in Vulnerability Database Check the Long Term Support (LTS) before implementing Practice Software Inventory Implement Least Privilege Policies Follow the Patching and Upgrade schedules Test, Stage and push to Production NO Default Passwords Check the License of Opensource Tools Keep Source Codes in local repositories for scanning ReferecesConsider paid Opensource services as well:  Opensource doesn’t mean everything free. Whatever community versions are free ($) but do consider paid optiions for better support.  Paid services will be better in terms of QA, Stability, Release Cycle and Support.  Organizations who are ready to take care of above items can think about community supported editions easily. Be a contributor to the Community:  Report bugs on time Raise feature requests and contribute back Be part of opensource development and testing teamImplement the best class Application monitoring System:  make sure the applciation are under monitoring observability is the key reporting mechanism and follow up tracking should be in placeCheck in Vulnerability Database:  Check in all Vulnerability databases available.  Check in poduct Vulnerability page regularly. Check the Long Term Support (LTS) before implementing:  Make sure the tool is having long term support.  Check the popularity and community strength. Practice Software Inventory:  Keep your software and tools database uptodate.  Keep the version information, Vulnerability tracker, upgrade options, End of Life (EOL)Implement Least Privilege Policies:  Allow access based on demand and request Block all access unless it is neededFollow the Patching and Upgrade schedules:  Do not wait for an issue Upgrade and patch systems based on availabilityTest, Stage and push to Production:  Do not try anything in production as we do not know what all tests and QA has been completed from the opensource community level.  Refer the release notes carfully before implementation/upgrade process. NO Default Passwords:  Change all default passwords to secure passwords (follow organization password policies) Keep passwords in vault or password manager. Check the License of Opensource Tools:  Make sure the license is acceptable as per organization policies Check what all data will be collected and send to external systems (if any) for improvement/analytics. Keep Source Codes in local repositories for scanning:  Scan the source code by yourself using tools Test fixes and bugs and propose to communityRefereces:  5 ways to keep open source-based apps secure Three Myths Debunked About Open Source Software Security A Guide to Open-Source Software Security Risks &amp; Best Practices"
    }, {
    "id": 30,
    "url": "http://localhost:4000/books",
    "title": "FREE e-Books",
    "body": "Note: This page contains links to FREE eBooks and site urls. But some links might be expired and you may need to purchase from the publishers. Web:  The Complete NGINX Cookbook - Derek DeJonghe / O’Reilly - FREEContainers, Kubernetes, OpenShift:  Cloud Native DevOps With Kubernetes - FREE Kubernetes Operators - FREETerraform:  Infrastructure as Code - FREE Terraform: Up &amp; RunningPublishers:  Packt Green Tea Press IT Ebooks smtebooks. com All IT Books / All IT EBOOKS z-lib. org"
    }, {
    "id": 31,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 32,
    "url": "http://localhost:4000/certifications",
    "title": "DevOps Certifications and Promotions",
    "body": "Note: These are collected list of FREE Certifications and Promotions on Certifications and Courses. I am not updating this page everyday, hence please pardon me if you find any wrong or expired information below. Feel free to share new or updated information via gini@techbeatly. com       Certification &amp; Description   Expense         Certified Rancher Operator: Level 1 Learn how to use Rancher with Kubernetes and other cloud-native technologies to accelerate how you build and deploy applications today.    FREE       Aviatrix Certified Engineer Multi-Cloud Networking &amp; Security Course and Certification (ACE)   FREE for Limited time       Certified Calico Operator: Level 1 Learn Kubernetes networking and security fundamentals using Calico.    FREE       JumpCloud Core Certification The Core certification validates the strong foundational knowledge needed to execute standard implementation, configurations, and daily operations across the JumpCloud platform.    $150 (currently waiving fees)    "
    }, {
    "id": 33,
    "url": "http://localhost:4000/cheatsheet-ansible",
    "title": "Ansible Cheatsheet",
    "body": "Ansible Cheat SheetAnsible is an open-source IT automation engine which can help you to automate most of your repetitive tasks in your work life. Ansible can also improve the consistency, scalability, reliability and easiness of your IT environment. Ansible Cheat Sheet for Quick Reference and understanding Visit techbeatly. com for more articles. Variables:       Item   Description         host_vars   directory for host variable files       group_vars   directory for group variable files       facts   collecting the host specific data       register   registered variables       vars   in playbook       vars_files   in playbook       include_vars   module       include_tasks: stuff. yml   include a sub task file   Task Control &amp; Loops:       Item   Description         with_items   then “item” inside action       with_nested   for nested loops       with_file           with_fileglob           with_sequence           with_random_choice           when   meet a condition   Modules:       Item   Description         copy   copy file or content       get_url   download file       file   manage file/directories       yum   manage package       service   manage services       firewalld   firewall service       lineinfile   add a line to dest file       template   to template file with variables       debug   to debug and display       add_host   add host to inventory while play       wait_for   use for flow control       apt   manage apt-packages       shell   execute shell commands on targets   Playbooks:       Item   Description         ansible-playbook &lt;YAML&gt;   Run on all hosts defined       ansible-playbook &lt;YAML&gt; -f 10   Fork - Run 10 hosts parallel       ansible-playbook &lt;YAML&gt; --verbose   Verbose on successful tasks       ansible-playbook &lt;YAML&gt; -C   Test run       ansible-playbook &lt;YAML&gt; -C -D   Dry run       ansible-playbook &lt;YAML&gt; -l &lt;host&gt;   Limit to run on single host   Handlers:       Item   Description         notify   to notify the handler       handlers   define handler   Tags:       Item   Description         tags   add tags to the tasks       --tags ‘&lt;tag&gt;’   during playbook execution       --skip-tags   for skipping those tags       tagged   run any tagged tasks       untagged   any untagged items       all   all items   Handling Errors:       Item   Description         ignore_errors   proceed or not if any error on current task       force_handlers   call handler even the play failed       failed_when   mark the task as failed if a condition met       changed_when   set “ok” or “failed” for a task       block   logical grouping of tasks (can use with when)       rescue   to run if block clause fails       always   always run even block success or fails   Jinja2 Templates: To be added later with examples Roles: main file in sub-directories should be main. ymlRole variable can define under roles directive Role Directories       Item   Description         defaults   default value of role variables       files   static files referenced by role tasks       handlers   role’s handlers       meta   role info like Author, Licence, Platform etc       tasks   role’s task defenition       templates   jinja2 templates       tests   test inventory and test. yml       vars   role’s variable values       pre_tasks   tasks before role       post_tasks   tasks after role   Ansible Galaxy: https://galaxy. ansible. com       Item   Description         ansible-galaxy search ‘install git’ --platform el   search for a role       ansible-galaxy info &lt;role-name&gt;   display role information       ansible-galaxy install &lt;role-name&gt; -p &lt;directory&gt;   install role from galaxy       ansible-galaxy list   to list local roles       ansible-galaxy remove &lt;role-name&gt;   remove role       ansible-galaxy init --offline &lt;role-name&gt;   initiate a role directory   Delegation:       Item   Description         delegate_to: localhost   run the task on localhost instead of inventory item       delegate_facts   assign the gathered facts from the tasks to the delegated host instead of current host   Parallelism:       Item   Description         forks   number of forks or parallel machines       --forks   when using ansible-playbook       serial   control number parallel machines       async: 3600   wait 3600 seconds to complete the task       poll: 10   check every 10 seconds if task completed       wait_for   module to wait and check if specific condition met       async_status   module to check an async task status   Ansible Vault:       Item   Description         ansible-vault create newfile   create a new vault file       ansible-vault view newfile   view file which is already ansible vaulted       ansible-vault edit newfile   Edit file       ansible-vault view --vault-password-file . secret newfile   Provide vault password as file       ansible-vault decrypt newfile   Remove encryption or vault       ansible-vault rekey newfile   change vault password       --ask-vault-pass or --vault-password-file &lt;secret-password-file&gt;   ask for vault password for ansible-playbook   Troubleshooting:       Item   Description         log_path   where logs are saved       debug   module for debugging       --syntax-check   syntax checking for playbooks before they run       --step   run playbook step by step       --start-at-task   run a playbook but start at specific task       --check   check mode       --diff   will show the expected changes if you run the playbook, but will not do any changes (kind of dry run)       uri   module for testing url       script   module for running script and return success code       stat   module to check the status of files/dir       assert   check file exist   if you find any mistakes, please feel free to update or let me know Thanks LinkedIn | techbeatly. com "
    }, {
    "id": 34,
    "url": "http://localhost:4000/cheatsheet-terraform",
    "title": "Terraform Cheatsheet",
    "body": "About Terraform CLI Terraform, a tool created by Hashicorp in 2014, written in Go, aims to build, change and version control your infrastructure. This tool have a powerfull and very intuitive Command Line Interface.  Installation     Install through curl    OR install through tfenv: a Terraform version manager First of all, download the tfenv binary and put it in your PATH. Installation: Install through curl: ```bash$ curl -O https://releases. hashicorp. com/terraform/0. 11. 10/terraform_0. 11. 10_linux_amd64. zip$ sudo unzip terraform_0. 11. 10_linux_amd64. zip -d /usr/local/bin/$ rm terraform_0. 11. 10_linux_amd64. zip OR install through tfenv: a Terraform version manager: First of all, download the tfenv binary and put it in your PATH. : $ git clone https://github. com/Zordrak/tfenv. git ~/. tfenv$ echo ‘export PATH=”$HOME/. tfenv/bin:$PATH”’ » $HOME/bashrc ##Then, you can install desired version of terraform:$ tfenv install 0. 11. 10UsageShow version$ terraform –versionTerraform v0. 11. 10Init Terraform$ terraform initIt’s the StatePull remote state in a local copy$ terraform state pull &gt; terraform. tfstatePush state in remote backend storage$ terraform state pushThis command is usefull if for example you riginally use a local tfstate and then you de "
    }, {
    "id": 35,
    "url": "http://localhost:4000/cheatsheet-vim-editor",
    "title": "Vim Cheatsheet",
    "body": "Vim CheatsheetOriginal Reference : https://github. com/hackjutsu/vim-cheatsheet Global: :help keyword # open help for keyword:o file    # open file:saveas file # save file as:close    # close current paneCursor movement: h    # move cursor leftj    # move cursor downk    # move cursor upl    # move cursor rightH    # move to top of screenM    # move to middle of screenL    # move to bottom of screenw    # jump forwards to the start of a wordW    # jump forwards to the start of a word (words can contain punctuation)e    # jump forwards to the end of a wordE    # jump forwards to the end of a word (words can contain punctuation)b    # jump backwards to the start of a wordB    # jump backwards to the start of a word (words can contain punctuation)0    # jump to the start of the line^    # jump to the first non-blank character of the line$    # jump to the end of the lineg_    # jump to the last non-blank character of the linegg    # go to the first line of the documentG    # go to the last line of the document5G    # go to line 5fx    # jump to next occurrence of character xtx    # jump to before next occurrence of character x}    # jump to next paragraph (or function/block, when editing code){    # jump to previous paragraph (or function/block, when editing code)zz    # center cursor on screenCtrl + b # move back one full screenCtrl + f # move forward one full screenCtrl + d # move forward 1/2 a screenCtrl + u # move back 1/2 a screenInsert mode - inserting/appending text: i    # insert before the cursorI    # insert at the beginning of the linea    # insert (append) after the cursorA    # insert (append) at the end of the lineo    # append (open) a new line below the current lineO    # append (open) a new line above the current lineea    # insert (append) at the end of the wordEsc   # exit insert modeEditing: r    # replace a single characterJ    # join line below to the current onecc    # change (replace) entire linecw    # change (replace) to the start of the next wordce    # change (replace) to the end of the next wordcb    # change (replace) to the start of the previous wordc$    # change (replace) to the end of the lines    # delete character and substitute textS    # delete line and substitute text (same as cc)xp    # transpose two letters (delete and paste).     # repeat last commandu    # undoCtrl + r # redoMarking text (visual mode): v    # start visual mode, mark lines, then do a command (like y-yank)V    # start linewise visual modeo    # move to other end of marked areaO    # move to other corner of blockaw    # mark a wordab    # a block with ()aB    # a block with {}ib    # inner block with ()iB    # inner block with {}Esc   # exit visual modeCtrl + v # start visual block modeVisual commands: &gt;    # shift text right&lt;    # shift text lefty    # yank (copy) marked textd    # delete marked text~    # switch caseCut and paste: yy    # yank (copy) a line2yy   # yank (copy) 2 linesyw    # yank (copy) the characters of the word from the cursor position to the start of the next wordy$    # yank (copy) to end of linep    # put (paste) the clipboard after cursorP    # put (paste) before cursordd    # delete (cut) a line2dd   # delete (cut) 2 linesdw    # delete (cut) the characters of the word from the cursor position to the start of the next wordD    # delete (cut) to the end of the lined$    # delete (cut) to the end of the lined^    # delete (cut) to the first non-blank character of the lined0    # delete (cut) to the begining of the linex    # delete (cut) characterSearch and replace: /pattern    # search for pattern?pattern    # search backward for pattern\vpattern   # 'very magic' pattern: non-alphanumeric characters are interpreted as special regex symbols (no escaping needed)n       # repeat search in same directionN       # repeat search in opposite direction:%s/old/new/g # replace all old with new throughout file:%s/old/new/gc # replace all old with new throughout file with confirmations:noh      # remove highlighting of search matchesSearch in multiple files: :vimgrep /pattern/ {file} # search for pattern in multiple files:cn            # jump to the next match:cp            # jump to the previous match:copen          # open a window containing the list of matchesExiting: :w       # write (save) the file, but don't exit:w !sudo tee % # write out the current file using sudo:wq or :x or ZZ # write (save) and quit:q       # quit (fails if there are unsaved changes):q! or ZQ    # quit and throw away unsaved changesWorking with multiple files: :e file    # edit a file in a new buffer:bnext or :bn # go to the next buffer:bprev or :bp # go to the previous buffer:bd      # delete a buffer (close a file):ls      # list all open buffers:sp file   # open a file in a new buffer and split window:vsp file   # open a file in a new buffer and vertically split windowCtrl + ws   # split windowCtrl + ww   # switch windowsCtrl + wq   # quit a windowCtrl + wv   # split window verticallyCtrl + wh   # move cursor to the left window (vertical split)Ctrl + wl   # move cursor to the right window (vertical split)Ctrl + wj   # move cursor to the window below (horizontal split)Ctrl + wk   # move cursor to the window above (horizontal split)Tabs: :tabnew or :tabnew file # open a file in a new tabCtrl + wT        # move the current split window into its own tabgt or :tabnext or :tabn # move to the next tabgT or :tabprev or :tabp # move to the previous tab&lt;number&gt;gt       # move to tab &lt;number&gt;:tabmove &lt;number&gt;    # move current tab to the &lt;number&gt;th position (indexed from 0):tabclose or :tabc   # close the current tab and all its windows:tabonly or :tabo    # close all tabs except for the current one:tabdo command     # run the command on all tabs (e. g. :tabdo q - closes all opened tabs)"
    }, {
    "id": 36,
    "url": "http://localhost:4000/cheatsheets",
    "title": "CheatSheets for DevOps Tools",
    "body": " Ansible Cheatsheet OpenShift Kubernetes Docker Cheatsheet Vim Editor Cheatsheet"
    }, {
    "id": 37,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 38,
    "url": "http://localhost:4000/container-tips",
    "title": "Container Tips",
    "body": " How to install ping utility inside container ? How to allow continers to execute ping ? Error - “/” is not a shared mountHow to install ping utility inside container ?: apt-get updateapt-get install iputils-ping## fedoradnf install iputilsHow to allow continers to execute ping ?: Run container with additional capabilities. $ sudo podman run --cap-add net_raw --cap-add net_admin -it ubuntu bashError - “/” is not a shared mount: ERROR on rootless mode $ podman psWARN[0000]  /  is not a shared mount, this could cause issues or missing mounts with rootless containersError: cannot setup namespace using newuidmap: exit status 1resolve the rootless mode problem sudo chmod 4755 /usr/bin/newgidmapsudo chmod 4755 /usr/bin/newuidmap"
    }, {
    "id": 39,
    "url": "http://localhost:4000/containers",
    "title": "Containers - Reference Notes",
    "body": "Containers - Reference NotesSingularity: Singularity is a relatively new container software originally developed by Greg Kurtzer while at Lawrence Berkley National labs. It was developed with security, scientific software, and HPC systems in mind.  Singularity Tutorial"
    }, {
    "id": 40,
    "url": "http://localhost:4000/_pages/devops-certifications/",
    "title": "DevOps Certifications",
    "body": "DevOps CertificationsGuides and References: These are collection of reference documents and blog posts from different experts around.  DevSecOps Engineering (DSOE)"
    }, {
    "id": 41,
    "url": "http://localhost:4000/dns-server",
    "title": "Configure DNS Server for Lab",
    "body": " DNS Server using bind     Install bind   Configure /etc/named. conf   Remove existing wrong zone if any   Create Forward Zone - /etc/named. conf   Create Reverse Zone - /etc/named. conf   Create fwd zon file - /var/named/fwd. ansible. lab. local. db   Create reverse zone db - /var/named/103. 168. 192. db   Start named service   Open firewall ports   Add in network and resolv. conf    Ansible Roles DNS Server using dnsmasq     Enable firewall ports    Enable DHCP Server Using dnsmasqDNS Server using bindInstall bind: yum -y install bind bind-utilsConfigure /etc/named. conf: listen-on port 53 { 127. 0. 0. 1; };allow-query   { localhost; 192. 168. 103. 0/24; };Remove existing wrong zone if any: Comment out if you are facing issue with default zone part. #zone  .   IN {#    type hint;#    file  named. ca ;#};Create Forward Zone - /etc/named. conf: zone  ansiblelab. local  IN {     type master;     file  fwd. ansiblelab. local. db ;     allow-update { none; };     allow-query {any; };};Create Reverse Zone - /etc/named. conf: zone  103. 168. 192. in-addr. arpa  IN {     type master;     file  103. 168. 192. db ;     allow-update { none; };     allow-query {any; };};Create fwd zon file - /var/named/fwd. ansible. lab. local. db: @  IN SOA   fjpocans01. ansiblelab. local. root. ansiblelab. local. (                        1001  ;Serial                        3H   ;Refresh                        15M   ;Retry                        1W   ;Expire                        1D   ;Minimum TTL                        );Name Server Information@   IN NS   fjpocans01. ansiblelab. local. ;IP address of Name Serverprimary IN A    192. 168. 103. 10fjpocans01 IN A   192. 168. 103. 10;Mail exchangeransiblelab. local. IN MX 10  mail. ansiblelab. local. ;A - Record HostName To IP Addresswww   IN A    192. 168. 103. 100mail  IN A    192. 168. 103. 150;CNAME recordftp   IN CNAME    www. ansiblelab. local. Whenever you update the zone lookup file, you need to change/increment the serial like 1002 ;Serial. Create reverse zone db - /var/named/103. 168. 192. db: @  IN SOA   fjpocans01. ansiblelab. local. root. nsiblelab. local. (                        1001  ;Serial                        3H   ;Refresh                        15M   ;Retry                        1W   ;Expire                        1D   ;Minimum TTL                        );Name Server Information@ IN NS   fjpocans01. ansiblelab. local. ;Reverse lookup for Name Server10    IN PTR   fjpocans01. ansiblelab. local. ;PTR Record IP address to HostName100   IN PTR   www. ansiblelab. local. 150   IN PTR   mail. ansiblelab. local. Whenever you update the zone lookup file, you need to change/increment the serial like 1002 ;Serial. Start named service: systemctl restart namedsystemctl enable namedOpen firewall ports: firewall-cmd --permanent --add-port=53/udpfirewall-cmd --reloadAdd in network and resolv. conf: Tip: Install BIND utilities ‘yum install -y bind-utils’ package to get nslookup or dig command. Ansible Roleshttps://galaxy. ansible. com/bertvv/bind DNS Server using dnsmasqhttps://www. tecmint. com/setup-a-dns-dhcp-server-using-dnsmasq-on-centos-rhel/ yum install dnsmasqsystemctl start dnsmasqsystemctl enable dnsmasqsystemctl status dnsmasqconfigure # cat /etc/dnsmasq. conf |grep -v ^# |grep -v ^$domain-neededbogus-privserver=10. 6. 2. 11server=8. 8. 8. 8address=/lab. local/127. 0. 0. 1address=/lab. local/10. 6. 1. 202interface=eth0listen-address=::1,127. 0. 0. 1,10. 6. 1. 202bind-interfacesexpand-hostsdomain=lab. localconf-dir=/etc/dnsmasq. d,. rpmnew,. rpmsave,. rpmorig# dnsmasq --testdnsmasq: syntax check OK. Defining DNS Hosts and NamesThe Dnsmasq reads all the DNS hosts and names from the /etc/hosts file, so add your DNS hosts IP addresses and name pairs as shown. # 5. cat /etc/hosts127. 0. 0. 1  localhost localhost. localdomain localhost4 localhost4. localdomain4::1     localhost localhost. localdomain localhost6 localhost6. localdomain610. 6. 1. 194 node1. lab. local node110. 6. 1. 195 node2. lab. local node210. 6. 1. 196 node3. lab. local node310. 6. 1. 197 node4. lab. local node410. 6. 1. 199 ovirt-engine-02. lab. local ovirt-engine-0210. 6. 1. 200 vm-ansible-01. lab. local vm-ansible-0110. 6. 1. 202 vm-utils-01. lab. local vm-utils-01 ns1. labl. local ns1 dns. lab. local dns repo. lab. local repo10. 6. 1. 196 node3. lab. local node310. 6. 1. 198 vcenter. lab. local vcenterEnable firewall ports: firewall-cmd --add-service=dns --permanentfirewall-cmd --add-service=dhcp --permanentfirewall-cmd --reloadEnable DHCP Server Using dnsmasq# cat /etc/dnsmasq. conf |grep -v ^# |egrep  dhcp-range|dhcp-option|dhcp-leasefile|dhcp-authoritative dhcp-range=10. 6. 1. 207,10. 6. 1. 210,255. 255. 255. 128,12hdhcp-option=3,10. 6. 1. 129dhcp-leasefile=/var/lib/dnsmasq/dnsmasq. leasesdhcp-authoritativesudo systemctl restart dnsmasq "
    }, {
    "id": 42,
    "url": "http://localhost:4000/gcp-nested-virtualization",
    "title": "GCP Nested Virtualization",
    "body": "Note: This is purely based on GCP Documentation and for my own quick reference with additonal notes. ## Create a boot disk from a public image $ gcloud compute disks create disk1 --image-project debian-cloud --image-family debian-9 --zone us-central1-b## Create image$ gcloud compute images create nested-vm-image \ --source-disk disk1 --source-disk-zone us-central1-b \ --licenses  https://www. googleapis. com/compute/v1/projects/vm-options/global/licenses/enable-vmx ## remove the disk since it not needed anymore## Create VM with new Image$ gcloud compute instances create example-nested-vm --zone us-central1-b \       --min-cpu-platform  Intel Haswell  \       --image nested-vm-image## SSH Access to the VM$ gcloud compute ssh example-nested-vm## and check the nested virtualization inside VM$ grep -cw vmx /proc/cpuinfoStarting a nested VM ## Update the VM instance and install some necessary packages$ sudo apt update &amp;&amp; sudo apt install qemu-kvm -y## Download OS Image$ wget https://people. debian. org/~aurel32/qemu/amd64/debian_squeeze_amd64_standard. qcow2## Run inside screen and press enter$ screen## SStart the nested VM. When prompted, login in with user: root, password: rootsudo qemu-system-x86_64 -enable-kvm -hda debian_squeeze_amd64_standard. qcow2 -m 512 -cursesStarting a private bridge between the host and nested VMs ## insall necessary pacakges$ sudo apt update &amp;&amp; sudo apt install uml-utilities qemu-kvm bridge-utils virtinst libvirt-daemon-system libvirt-clients -y## Start the default network that comes with the libvirt package:$ sudo virsh net-start defaultAppendix: Install GNOME on Debian: $ sudo tasksel install desktop gnome-desktop$ sudo tasksel install laptop## To tell Debian 10 to start the graphical desktop environment by default on boot, run the following command:$ sudo systemctl set-default graphical. target$ sudo reboot"
    }, {
    "id": 43,
    "url": "http://localhost:4000/gcp-vm-migration",
    "title": "Ansible for VMware",
    "body": "VM Migration: Planning: terraform -v git clone https://github. com/morgante/terraform-codelab. gitcd terraform-codelab/lab-networking edit terraform. tfvars -&gt; open editor project_id=”qwiklabs-gcp-01-eb85e88d83fc” gcloud iam service-accounts create terraform –display-name terraform gcloud iam service-accounts list gcloud iam service-accounts keys create . /credentials. json –iam-account terraform@qwiklabs-gcp-01-eb85e88d83fc. iam. gserviceaccount. com gcloud projects add-iam-policy-binding qwiklabs-gcp-01-eb85e88d83fc –member=serviceAccount:terraform@qwiklabs-gcp-01-eb85e88d83fc. iam. gserviceaccount. com –role=roles/owner — setup remote state gsutil mb gs://qwiklabs-gcp-01-eb85e88d83fc-state-bucket – edit backend. tf terraform { backend “gcs” {  bucket = “qwiklabs-gcp-01-eb85e88d83fc-state-bucket”    # Change this to -state-bucket  prefix =  terraform/lab/network  }} — run terraform terraform init terraform plan terraform apply terraform show – add a subnet edit network. tf # # Create the networkmodule  vpc  { source =  terraform-google-modules/network/google  version =  ~&gt; 0. 4. 0  # Give the network a name and project project_id  =  ${google_project_service. compute. project}  network_name =  my-custom-network  subnets = [  {   # Creates your first subnet in us-west1 and defines a range for it   subnet_name  =  my-first-subnet    subnet_ip   =  10. 10. 10. 0/24    subnet_region =  us-west1   },  {   # Creates a dedicated subnet for GKE   subnet_name  =  my-gke-subnet    subnet_ip   =  10. 10. 20. 0/24    subnet_region =  us-west1   },  # Add your subnet here ] # Define secondary ranges for each of your subnets secondary_ranges = {  my-first-subnet = []  my-gke-subnet = [   {    # Define a secondary range for Kubernetes pods to use    range_name  =  my-gke-pods-range     ip_cidr_range =  192. 168. 64. 0/24    },  ]  # Add your subnet’s secondary range below this line.  }}terraform apply "
    }, {
    "id": 44,
    "url": "http://localhost:4000/jekyll",
    "title": "How to Configure and Use GitHub Jekyll Local Environment",
    "body": "Install Jekyll &amp; BundlerRefer the documentation based on your Operating System.       Ubuntu   MacOS   Linux   Windows   Instructions for MacOSInstall Command Line Tools: xcode-select --installset SDKROOT (only macOS Catalina or later)Permalink export SDKROOT=$(xcrun --show-sdk-path)Install Ruby: Jekyll requires Ruby v2. 5. 0 or higher but MacOS Big Sur 11. x is already installed with Ruby 2. 6. 3. Check the version and confirm. % ruby -vruby 3. 0. 2p107 (2021-07-07 revision 0db68f0233) [x86_64-darwin20]If you’re running a previous version of macOS, you’ll have to install a newer version of Ruby. Install Ruby using rbenv (Multiple Ruby versions): This is very useful when you need to be able to run a given Ruby version on a project. # Install Homebrew/bin/bash -c  $(curl -fsSL https://raw. githubusercontent. com/Homebrew/install/HEAD/install. sh) # Install rbenv and ruby-buildbrew install rbenv# Set up rbenv integration with your shellrbenv init# Check your installationcurl -fsSL https://github. com/rbenv/rbenv-installer/raw/main/bin/rbenv-doctor | bashRe-Open the terminal and the version ## Install 3. 0. 0$ rbenv install 3. 0. 0$ rbenv global 3. 0. 0$ ruby -vruby 3. 0. 0p0 (2020-12-25 revision 95aff21468)## Install 2. 7. 0$rbenv install 2. 7. 0## set global version$ rbenv global 2. 7. 0$ ruby -vruby 3. 0. 0p0 (2020-12-25 revision 95aff21468)## check versions% rbenv versions   * system (set by /Users/gini/. rbenv/version) 2. 7. 0 3. 0. 0## set local version% rbenv local 2. 7. 0## show current local version% rbenv version  2. 7. 0 (set by /Users/gini/codes/ginigangadharan. github. io/. ruby-version)gini@greenmango ginigangadharan. github. io % Install Jekyll: (Use localinstall rathr than Global Install) gem install --user-install bundler jekyll## --[no-]user-install     Install in user's home directory insteadAppend your path file with the following, replacing the X. X with the first two digits of your Ruby version: # If you're using Zshecho 'export PATH= $HOME/. gem/ruby/X. X. 0/bin:$PATH ' &gt;&gt; ~/. zshrc# If you're using Bashecho 'export PATH= $HOME/. gem/ruby/X. X. 0/bin:$PATH ' &gt;&gt; ~/. bash_profile# Unsure which shell you are using? Typeecho $SHELLCheck that GEM PATHS: points to your home directory: gem envInstructions for UbuntuInstall packages: ## Ubuntusudo apt-get install ruby-full build-essential zlib1g-dev## RHEL8/CentOS8sudo dnf install ruby ruby-develsudo dnf group install  Development Tools Configure gem path: echo '# Install Ruby Gems to ~/gems' &gt;&gt; ~/. bashrcecho 'export GEM_HOME= $HOME/gems ' &gt;&gt; ~/. bashrcecho 'export PATH= $HOME/gems/bin:$PATH ' &gt;&gt; ~/. bashrcsource ~/. bashrcInstall jekyll &amp; bundler: gem install --user-install bundler jekyllTest your siteGoto your project directory bundle exec jekyll serveUpdating the GitHub Pages gemIf the github-pages gem on your computer is out of date with the github-pages gem on the GitHub Pages server, your site may look different when built locally than when published on GitHub. To avoid this, regularly update the github-pages gem on your computer.  If you installed Bundler, run bundle update github-pages.  If you don’t have Bundler installed, run gem update github-pages. If no Gemfilebundle initbundle add jekyllbundle install --path vendor/bundleJekyll-Admin - GUI Editor: Add Plugin in Gemfile: . . group :jekyll_plugins do  .   .   .   gem 'jekyll-admin'endAdd configuration in _config. yaml: jekyll_admin: homepage:  posts  hidden_links:  #- posts  #- pages  #- staticfiles  - datafiles  #- configurationRefer Jekyll Admin - Documentation and GitHub for details. "
    }, {
    "id": 45,
    "url": "http://localhost:4000/gitlab",
    "title": "GitLab Notes",
    "body": "Install GitLab Runner (Binary): Reference # Download binarysudo curl -L --output /usr/local/bin/gitlab-runner  https://gitlab-runner-downloads. s3. amazonaws. com/latest/binaries/gitlab-runner-linux-amd64 # Give it permissions to execute:sudo chmod +x /usr/local/bin/gitlab-runner# Create a GitLab CI user:sudo useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bash# Install and run as service:sudo gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runnersudo gitlab-runner startRun GitLab Runner in Docker: docker run -d --name gitlab-runner --restart always \   -v /srv/gitlab-runner/config:/etc/gitlab-runner \   -v /var/run/docker. sock:/var/run/docker. sock \   gitlab/gitlab-runner:latestsudo docker run -d --name gitlab-runner --restart always \   -v /home/devops/gitlab-runner/config:/etc/gitlab-runner \   -v /var/run/docker. sock:/var/run/docker. sock \   gitlab/gitlab-runner:latest   How to Install GitLab CE in a disconnected environment Download the package from: https://packages. gitlab. com/gitlab/gitlab-ce     click on the RPM link for the OS and download the RPM file.    $ wget --content-disposition https://packages. gitlab. com/gitlab/gitlab-ce/packages/el/8/gitlab-ce-14. 5. 2-ce. 0. el8. x86_64. rpm/download. rpm   And transfer to server.   Follow https://about. gitlab. com/install for the instructions enable repos needed install dependencies$ sudo dnf install -y \  curl \  policycoreutils \  openssh-server \  perl \  postfix \  openssl \  tzdata \  yum-utils# Enable OpenSSH server daemon if not enabled: sudo systemctl status sshdsudo systemctl enable sshd sudo systemctl start sshd# Check if opening the firewall is needed with: sudo systemctl status firewalld$ sudo firewall-cmd --permanent --add-service=http &amp;&amp; \  sudo firewall-cmd --permanent --add-service=https &amp;&amp; \  sudo systemctl reload firewalldPostfix $ sudo dnf install postfix &amp;&amp; \  sudo systemctl enable postfix &amp;&amp; \  sudo systemctl start postfixInstall Git  make sure RHEL repos are enabled (local or satellite)$ sudo EXTERNAL_URL= http://gitlab. lab. local  \  GITLAB_ROOT_PASSWORD= gitlabadmin  \  dnf localinstall -y gitlab-ce-14. 5. 2-ce. 0. el8. x86_64. rpmNote:  If any error with gpgchec, then disable gpgcheck by adding --nogpgcheck at the end of dnf or yum command.  If you missed to mention the root password while installing, then get the default randomg password generated by installaer. (This password will be removed by gitlab-ctl reconfigure later. )$ sudo cat /etc/gitlab/initial_root_passwordReferences Improve Your Docker Build Time in GitLab CI"
    }, {
    "id": 46,
    "url": "http://localhost:4000/gmail-access-deletegated-account-from-mobile",
    "title": "How to Access Delegated Gmail Account from Mobile",
    "body": "Instead of sharing password of your gmail account, you can easilty setup email delegation by which the delegated user can access same email account with his primary gmail login. Refer How to Set up mail delegation in Gmail. By default, delegated account will not be able to access from mobile apps but you can achive this using a mobile browser. Open gmail. com in Chrome or Firefox: Tap on Setting bar on left top.  Request Desktop Site: Open Settings and Choose Gmail Desktop: Scroll to bottom and Choose Desktop.  Switch to Standard View: Gmail Desktop will open classic gmail (html) desktop version.  Scroll to bottom and Choose Standard.  Access Delegated Account: Tap on your profile picture and see all accounts you are already login.  Scroll down find all those delegated accounts you have access.  "
    }, {
    "id": 47,
    "url": "http://localhost:4000/gns3-device-setup",
    "title": "Configure Devices in GNS3",
    "body": "Configure Cisco Device in GNS3References:  Nexus 7K/N7K - Cisco NX-OSv applianceConfigure Fortigate Device in GNS3Download Fortios VM for GNS3:  https://support. fortinet. com/ Login https://support. fortinet. com/Download/VMImages. aspx Download VM Zip -&gt; Extract and get fortios. qcow2 in GNS3 -&gt; Preferences -&gt; QEMU -&gt; Qemu VMs -&gt; Add New (give 1024MB) username : admin , password : Configure Fortios Device: FortiGate-VM64-KVM login: adminPassword: You are forced to change your password. Please input a new password. New Password: Confirm Password: Welcome!FortiGate-VM64-KVM # FortiGate-VM64-KVM # show system interfaceFortiGate-VM64-KVM # config system interface FortiGate-VM64-KVM (interface) # edit port1FortiGate-VM64-KVM (port1) # set mode static FortiGate-VM64-KVM (port1) # set ip 10. 1. 10. 70 255. 255. 255. 0FortiGate-VM64-KVM (port1) # append allowaccess httpFortiGate-VM64-KVM (port1) # append allowaccess sshFortiGate-VM64-KVM (port1) # endFortiGate-VM64-KVM # config system globalFortiGate-VM64-KVM (global) # set hostname GNS3-FortiGateFortiGate-VM64-KVM (global) # endGNS3 References for Fortinet:  https://www. linkedin. com/pulse/fortigate-inside-gns3-cli-web-access-issa-itani/ https://support. fortinet. com/Download/VMImages. aspx https://www. gns3network. com/how-to-deploy-fortigate-virtual-firewall-in-gns3/ https://www. fortinetguru. com/2017/01/fortigate-vm-initial-configuration/"
    }, {
    "id": 48,
    "url": "http://localhost:4000/gns3-infra-setup",
    "title": "GNS3 Infra Setup",
    "body": "Step 1  setup virtualbox or VMWare ESXi nodeStep 2  Download GNS3 image (OVA or suitable format)Step 3  Create a VM in ESXi (or in VirtualBox) with downloaded ova file configure IP as you needStep 4  Download and install GNS3 GUI on a machine (your workstation or laptop) https://www. gns3. com/ Refer install instruction - linuxStep 5  Open GNS3 GUI -&gt; Connect to GNS3 VM/ImageStep 6  Create Project Step 7  Download ISO images for devices or IOSConnect to real network  Connecting GNS3 to Real Network in Windows 10 Home GNS3 Talks: How to connect GNS3 to a physical network (Part 1).  Connect GNS3 to the Internet (local server)install gns3-server locally: sudo apt install gns3-serverPrepare GNS3 VM for oVirtInstall GNS3 on a remote server: Ref: Install GNS3 on a remote server cd /tmpcurl https://raw. githubusercontent. com/GNS3/gns3-server/master/scripts/remote-install. sh &gt; gns3-remote-install. sh## without VPNbash gns3-remote-install. sh --with-iou --with-i386-repository## with VPNbash gns3-remote-install. sh --with-openvpn --with-iou --with-i386-repositoryAccess the GNS3 Server on - http://YOUR_IP:3080/ [NOT Tried Yet] Convert VirtualBox to KVM format:  Download GNS3 VM for VirtualBOx Convert to qcow2 formatqemu-img convert -f vdi -O qcow2 disk-1. vdi disk-2. qcow2 "
    }, {
    "id": 49,
    "url": "http://localhost:4000/hashicorp-consul",
    "title": "HashiCorp Consul",
    "body": " Networking Developement Security Operations/bin/start_consul. sh"
    }, {
    "id": 50,
    "url": "http://localhost:4000/how-to-set-number-of-pods-per-node-openshift",
    "title": "How to set number of pods per node in OpenShift ?",
    "body": "## get the machineconfigpool$ oc get machineconfigpool## see details$ oc describe machineconfigpool worker## add label to the machineconfigpool if not exist$ oc label machineconfigpool worker custom-kubelet=small-pods#eg:apiVersion: machineconfiguration. openshift. io/v1kind: MachineConfigPoolmetadata: creationTimestamp: 2019-02-08T14:52:39Z generation: 1 labels:  custom-kubelet: small-pods ## Create a custom resource (CR) for your configuration change. ## Setting podsPerCore to 0 disables this limit. apiVersion: machineconfiguration. openshift. io/v1kind: KubeletConfigmetadata: name: set-max-pods spec: machineConfigPoolSelector:  matchLabels:   custom-kubelet: small-pods  kubeletConfig:  podsPerCore: 10   maxPods: 250 ## check the machines and wait for update$ oc get machineconfigpool ## eg:NAME   CONFIG            UPDATED  UPDATING  DEGRADEDmaster  master-9cc2c72f205e103bb534  False   False   Falseworker  worker-8cecd1236b33ee3f8a5e  False   True    False"
    }, {
    "id": 51,
    "url": "http://localhost:4000/",
    "title": "automate...containerize...",
    "body": "  {% if page. url ==  /  %}                            Gineesh Madapparambath                                    Helping customers on Automation and Containerization journey using Ansible, Kubernetes, OpenShift and Terraform.                            YouTube. com/techbeatly                           Talks about #devops, #ansible, #openshift, #terraform, and #kubernetes                 Read More                  	                  {% for post in site. pages %}             {% if post. titleshort %}              {{ post. titleshort | downcase }}            {% endif %}          {% endfor %}                           {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; background-position: center; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}{% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 52,
    "url": "http://localhost:4000/infoblox-virtual-nios-setup",
    "title": "Infoblox Virtual Appliance Setup (NIOS)",
    "body": " Installing the NIOS Virtual or Reporting Virtual Appliance  Infoblox Guide Automate Infoblox Infrastructure Using Ansible Ansible and Infoblox: Roles Deep Dive Infoblox and Ansible Integration"
    }, {
    "id": 53,
    "url": "http://localhost:4000/jasonpath-query",
    "title": "JSON PATH Query",
    "body": "Ref: https://github. com/json-path/JsonPath Basic Dictionary: $. car. wheels[2]          # 2rd item$. [0,3]              # 1st and 4th elements from array$. [0:4]              # 1st to 4th elements from array                  excluding 4th element$. [0:6:2]             # 1st to 4th elements with steps 2 (skip one element each time)            $. [-1:0]             # last item$. [-8:-2]             # from bottom$. [0:5]. phone           # first five  $. [-5:]. age            # last 5 entries  Wild Card: $. [*]               $. [*]. color            # color property of any parent items$. car. wheels[*]. model$. [*]. wheels[*]. model$. employee. payslips[*]. amount$. prizes[*]. laureates[*]. firstname$. prizes[?(@. year == 2014)]. laureates[*]. firstname                  # all firstname but for year 2014             JSON PATH in kubernetes: "
    }, {
    "id": 54,
    "url": "http://localhost:4000/jenkins",
    "title": "Jenkins",
    "body": " Pipeline Introduction     PROJECT TYPES   JENKINS PIPELINE   PIPELINE BENEFITS (1/2)   PIPELINE-AS-CODE   Basic Jenkins Pipeline Sections   Jenkins Parallel Pipeline   SCRIPTED PIPELINE   Multi-environment Pipeline   Post Section in Jenkins Pipeline   Environment Directive   Notifications         Conditional Notificaton          WHEN DIRECTIVE   GIT ENVIRONMENT VARIABLES   CREDENTIALS   OPTIONS AND CONFIGURATIONS         SET TIMEOUT          PARAMETERS   BUILD TOOLS TO CREATE CODE FOR STEPS   SHARED LIBRARIES   BEST PRACTICES    Start Jenkins inside a container     Run jenkins as a docker container   Get the Password   Login to Jenkins GUI for the first time   Sample docker-compose. yml    Create Multibranch Pipeline with Git Learn Jenkins ReferencePipeline IntroductionPROJECT TYPES: The Continuous Delivery build flow is defined as a project, of one of the following types:  Freestyle Projects Pipeline Projects Declarative Pipeline Scripted PipelineJENKINS PIPELINE: https://jenkins. io/doc/book/pipeline/ PIPELINE BENEFITS (1/2): The pipeline functionality is:  Durable: The Jenkins master can restart and the Pipeline continues to run Pausable: can stop and wait for human input or approval Versatile: supports complex real-world CD requirements (fork, join, loop, parallelize) Extensible: supports custom extensions to its “DSL” (Domain-specific Language) Reduces number of jobs Easier maintenance Decentralization of job configuration Easier specification through codePIPELINE-AS-CODE:  A Pipeline is defined in a Jenkinsfile - Uses a DSL based on Apache Groovy syntax Deployment flow is expressed as code - Can express complex flows, conditionals and such The Jenkinsfile is stored on an SCM - Works with SCM conventions such as Branches and Git Pull RequestsJenkins Componenets  Master: Computer, VM or container where Jenkins is installed and run. Serves requests and handles build tasks Agent: (formerly “slave”) Computer, VM or container that connects to a Jenkins Master. Executes tasks when - directed by the Master. Has a number and scope of operations to perform.  Node: is sometimes used to refer to the computer, VM or container used for the Master or Agent; be careful because “Node” has another meaning for Pipeline Executor: Computational resource for running builds. Performs Operations, Can run on any Master or Agent, although running builds on masters, can degrade performance and opens up serious security vulnerabilities, Can be parallelized on a specific Master or AgentBasic Jenkins Pipeline Sections: pipeline { agent { label 'linux' } stages {  stage('MyBuild') {   steps {    sh '. /jenkins/build. sh'   }  }   stage('MySmalltest') {   steps {    sh '. /jenkins/smalltest. sh'   }  } }}Jenkins Parallel Pipeline: pipeline { agent any stages {  stage('Fluffy Build') {   steps {    sh '. /jenkins/build. sh'    archiveArtifacts 'target/*. jar'   }  }  stage('Fluffy Test') {   parallel {    stage('Backend') {     steps {      sh '. /jenkins/test-backend. sh'      junit 'target/surefire-reports/**/TEST*. xml'     }    }    stage('Frontend') {     steps {      sh '. /jenkins/test-frontend. sh'      junit 'target/test-results/**/TEST*. xml'     }    }    stage('Performance') {     steps {      sh '. /jenkins/test-performance. sh'     }    }    stage('Static') {     steps {      sh '. /jenkins/test-static. sh'     }    }   }  }  stage('Fluffy Deploy') {   steps {    sh '. /jenkins/deploy. sh staging'   }  } }}SCRIPTED PIPELINE: stage('Build') {  parallel linux: {    node('linux') {      checkout scm      try {        sh 'make'      }      finally {        junit '**/target/*. xml'      }    }  },  windows: {    node('windows') {      /* . . snip . . */    }  }}Multi-environment Pipeline: pipeline { agent none stages {  stage('Fluffy Build') {   parallel {    stage('Build Java 8') {     agent {      node {       label 'java8'      }     }     steps {      sh '. /jenkins/build. sh'      stash(name: 'Java 8', includes: 'target/**')     }    }    stage('Build Java 7') {     agent {      node {       label 'java7'      }     }     steps {      sh '. /jenkins/build. sh'      archiveArtifacts 'target/*. jar'      stash(name: 'Java 7', includes: 'target/**')     }    }   }  }  stage('Fluffy Test') {   parallel {    stage('Backend Java 8') {     agent {      node {       label 'java8'      }     }     steps {      unstash 'Java 8'      sh '. /jenkins/test-backend. sh'      junit 'target/surefire-reports/**/TEST*. xml'     }    }    stage('Frontend') {     agent {      node {       label 'java8'      }     }     steps {      unstash 'Java 8'      sh '. /jenkins/test-frontend. sh'      junit 'target/test-results/**/TEST*. xml'     }    }    stage('Performance Java 8') {     agent {      node {       label 'java8'      }     }     steps {      unstash 'Java 8'      sh '. /jenkins/test-performance. sh'     }    }    stage('Static Java 8') {     agent {      node {       label 'java8'      }     }     steps {      unstash 'Java 8'      sh '. /jenkins/test-static. sh'     }    }    stage('Backend Java 7') {     agent {      node {       label 'java7'      }     }     steps {      unstash 'Java 7'      sh '. /jenkins/test-backend. sh'      junit 'target/surefire-reports/**/TEST*. xml'     }    }    stage('Frontend Java 7') {     agent {      node {       label 'java7'      }     }     steps {      unstash 'Java 7'      sh '. /jenkins/test-frontend. sh'      junit 'target/test-results/**/TEST*. xml'     }    }    stage('Performance Java 7') {     agent {      node {       label 'java7'      }     }     steps {      unstash 'Java 7'      sh '. /jenkins/test-performance. sh'     }    }    stage('Static Java 7') {     agent {      node {       label 'java7'      }     }     steps {      unstash 'Java 7'      sh '. /jenkins/test-static. sh'     }    }   }  }  stage('Confirm Deploy') {   steps {    input(message: 'Okay to Deploy to Staging?', ok: 'Let\'s Do it!')   }  }  stage('Fluffy Deploy') {   agent {    node {     label 'java7'    }   }   steps {    unstash 'Java 7'    sh '. /jenkins/deploy. sh staging'   }  } }}Post Section in Jenkins Pipeline: pipeline { stages {  stage('Buzz Build') {   parallel {    stage('Build Java 7') {     steps {      sh           echo I am a $BUZZ_NAME!       . /jenkins/build. sh              }     post {      always {       archiveArtifacts(artifacts: 'target/*. jar', fingerprint: true)      }      success {       stash(name: 'Buzz Java 7', includes: 'target/**')      }     }    } . . . }Environment Directive:  Examples include BUILD_NUMBER, JENKINS_URL and EXECUTOR_NUMBERpipeline {  agent any  environment {    CC = 'clang'  }  stages {    stage('Example') {      environment {        AN_ACCESS_KEY =  SECRET  //credentials('my-predefined-secret-text')      }      steps {        sh 'printenv'Notifications: stages { stage ('Start') {  steps {   // send build started notifications   slackSend (color: '#FFFF00', message:  STARTED: Job '${env. JOB_NAME} [${env. BUILD_NUMBER}]' (${env. BUILD_URL}) )  } }}// send to emailemailext ( subject:  STARTED: Job '${env. JOB_NAME} [${env. BUILD_NUMBER}]' , body:    &lt;p&gt;STARTED: Job '${env. JOB_NAME} [${env. BUILD_NUMBER}]':&lt;/p&gt;  &lt;p&gt;Check console output at &amp;QUOT;&lt;a href='${env. BUILD_URL}'&gt;${env. JOB_NAME} [${env. BUILD_NUMBER}]&lt;/a&gt;&amp;QUOT;&lt;/p&gt;   , recipientProviders: [[$class: 'DevelopersRecipientProvider']])Conditional Notificaton: // on successpost { success {  slackSend (color: '#00FF00', message:  SUCCESSFUL: Job '${env. JOB_NAME} [${env. BUILD_NUMBER}]' (${env. BUILD_URL}) )  emailext (   subject:  SUCCESSFUL: Job '${env. JOB_NAME} [${env. BUILD_NUMBER}]' ,   body:    &lt;p&gt;SUCCESSFUL: Job '${env. JOB_NAME} [${env. BUILD_NUMBER}]':&lt;/p&gt;    &lt;p&gt;Check console output at &amp;QUOT;&lt;a href='${env. BUILD_URL}'&gt;${env. JOB_NAME} [${env. BUILD_NUMBER}]&lt;/a&gt;&amp;QUOT;&lt;/p&gt;   ,   recipientProviders: [[$class: 'DevelopersRecipientProvider']]  ) }}// on failfailure { slackSend (color: '#FF0000', message:  FAILED: Job '${env. JOB_NAME} [${env. BUILD_NUMBER}]' (${env. BUILD_URL}) ) emailext (  subject:  FAILED: Job '${env. JOB_NAME} [${env. BUILD_NUMBER}]' ,  body:    &lt;p&gt;FAILED: Job '${env. JOB_NAME} [${env. BUILD_NUMBER}]':&lt;/p&gt;   &lt;p&gt;Check console output at &amp;QUOT;&lt;a href='${env. BUILD_URL}'&gt;${env. JOB_NAME} [${env. BUILD_NUMBER}]&lt;/a&gt;&amp;QUOT;&lt;/p&gt;   ,  recipientProviders: [[$class: 'DevelopersRecipientProvider']] )}WHEN DIRECTIVE: BUILT-IN CONDITIONS // branch — Execute the stage when the branch being built matches the branch pattern given:when { branch 'master'}//environment — Execute the stage when the specified environment variable is set to the given value:when { environment name: 'DEPLOY_TO', value: 'production'}//expression — Execute the stage when the specified expression evaluates to true:when { expression {  return params. DEBUG_BUILD }}BUILT-IN NESTED CONDITIONS // allOf — Execute the stage when all nested conditions are true:when { allOf {  branch 'master'  environment name: 'DEPLOY_TO', value: 'production' // AND }}//anyOf — Execute the stage when at least one of the nested conditions is truewhen { anyOf {  branch 'master'  branch 'staging' // OR }}// not — Execute the stage when the nested condition is false. when { not { branch 'master' } }. . . stage('Confirm Deploy to Staging') {  when {   branch 'master'  }  steps {   input(message: 'Deploy to Stage', ok: 'Yes, let\'s do it!')  } } stage('Deploy to Staging') {  agent {   node {    label 'java8'   }  }  when {   branch 'master'  }  steps {   unstash 'Buzz Java 8'   sh '. /jenkins/deploy. sh staging'  } }. . . GIT ENVIRONMENT VARIABLES: stage('Generate Reports') { steps {  sh '. /jenkins/generate-reports. sh'  sh 'tar -czv target/reports. tar. gz target/reports'  archiveArtifacts 'target/*. tar. gz'  echo  Finished run for commit ${ env. GIT_COMMIT. substring(0,6) }  }}CREDENTIALS: . . . stage('Deploy Reports')  steps {  . . .     withCredentials(bindings: [string(credentialsId: 'my-elastic-key', variable: 'ELASTIC_ACCESS_KEY')]) {      // Environment Variable available in the remote shell      sh  env | grep ELASTIC_ACCESS_KEY       sh  echo ${ELASTIC_ACCESS_KEY} &gt; secret-file. txt     }. . . }OPTIONS AND CONFIGURATIONS: SET TIMEOUT: //timeout for the entire Pipeline (Note the double quotes around DAYSoptions { timeout(time: 3, unit:  DAYS )}//timeout for an  input  stagesteps { timeout(time:3, unit: DAYS ) {   input(message:  Deploy to Stage , ok:  Yes, let's do it! ) }}PARAMETERS: pipeline { agent none parameters {  string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: '') } stages {  stage ('Deploy') {   echo  Deploying to ${DEPLOY_ENV}   } }}BUILD TOOLS TO CREATE CODE FOR STEPS:  Make Apache Ant Apache Maven Gradle NPMPipelines using when, post etc pipeline { agent none stages {  stage('Fluffy Build') {   parallel {    stage('Build Java 8') {     agent {      node {       label 'java8'      }     }     steps {      sh '. /jenkins/build. sh'     }     post {      success {       stash(name: 'Java 8', includes: 'target/**')      }     }    }    stage('Build Java 7') {     agent {      node {       label 'java7'      }     }     steps {      sh '. /jenkins/build. sh'     }     post {      success {       archiveArtifacts 'target/*. jar'       stash(name: 'Java 7', includes: 'target/**')      }     }    }   }  }  stage('Fluffy Test') {   parallel {    stage('Backend Java 8') {     agent {      node {       label 'java8'      }     }     steps {      unstash 'Java 8'      sh '. /jenkins/test-backend. sh'     }     post {      always {       junit 'target/surefire-reports/**/TEST*. xml'      }     }    }    stage('Frontend') {     agent {      node {       label 'java8'      }     }     steps {      unstash 'Java 8'      sh '. /jenkins/test-frontend. sh'     }     post {      always {       junit 'target/test-results/**/TEST*. xml'      }     }    }    stage('Performance Java 8') {     agent {      node {       label 'java8'      }     }     steps {      unstash 'Java 8'      sh '. /jenkins/test-performance. sh'     }    }    stage('Static Java 8') {     agent {      node {       label 'java8'      }     }     steps {      unstash 'Java 8'      sh '. /jenkins/test-static. sh'     }    }    stage('Backend Java 7') {     agent {      node {       label 'java7'      }     }     steps {      unstash 'Java 7'      sh '. /jenkins/test-backend. sh'     }     post {      always {       junit 'target/surefire-reports/**/TEST*. xml'      }     }    }    stage('Frontend Java 7') {     agent {      node {       label 'java7'      }     }     steps {      unstash 'Java 7'      sh '. /jenkins/test-frontend. sh'     }     post {      always {       junit 'target/test-results/**/TEST*. xml'      }     }    }    stage('Performance Java 7') {     agent {      node {       label 'java7'      }     }     steps {      unstash 'Java 7'      sh '. /jenkins/test-performance. sh'     }    }    stage('Static Java 7') {     agent {      node {       label 'java7'      }     }     steps {      unstash 'Java 7'      sh '. /jenkins/test-static. sh'     }    }   }  }  stage('Confirm Deploy') {   when {    branch 'master'   }   steps {    input(message: 'Okay to Deploy to Staging?', ok: 'Let\'s Do it!')   }  }  stage('Fluffy Deploy') {   when {    branch 'master'   }   agent {    node {     label 'java7'    }   }   steps {    unstash 'Java 7'    sh  . /jenkins/deploy. sh ${params. DEPLOY_TO}    }  } } parameters {  string(name: 'DEPLOY_TO', defaultValue: 'dev', description: '') }}SHARED LIBRARIES: BEST PRACTICES: IMPLEMENT A TRIGGER pipeline {  agent any  triggers {    cron('H */4 * * 1-5')  }  stages {    stage('Example') {      steps {        echo 'Hello World'      }    }  }}Start Jenkins inside a containerRefer Docker Hub Run jenkins as a docker container: docker run \ --name jenkins \ --detach \ --volume jenkins-data:/var/jenkins_home \ --publish 8080:8080 \ --publish 50000:5000 \ jenkins/jenkins:ltswhere,  --name - name of the container --detach or -d - detached mode (in background) --volume jenkins-data:/var/jenkins_home - bind named volume (will create a directory in /var/lib/docker/volumes/) --publish 8080:8080 or -p - map or export host port 8080 to container port 8080 jenkins/jenkins:lts - docker image to be used.   Get the Password:   First Admin password can be found at /var/jenkins_home/secrets/initialAdminPassword$ sudo docker exec -it 198b7deb5f7d /bin/bashjenkins@198b7deb5f7d:/$ cat /var/jenkins_home/secrets/initialAdminPassword8d53672878b24941a1cdf3df3c8ec8ccjenkins@198b7deb5f7d:/$ exitexit or check docker logs CONTAINER_ID and get the password. Login to Jenkins GUI for the first time:  Open a web browser and goto localhost:8080 enter the password collected from previous step.  Install Suggested plugins Create the first time user confirm jenkins urlSample docker-compose. yml: version: '3'services: jenkins:  tty: true  stdin_open: true  container_name: jenkins  image: jenkins/jenkins  ports:   -  8080:8080   volumes:   -  $PWD/jenkins_home:/var/jenkins_home   networks:   - netnetworks: net:Create Multibranch Pipeline with GitLearn Jenkins Jenkins Pipeline - Fundamentals - FREE Course from CloudBees University  Reference  Deploying Jenkins X in Linode Kubernetes Engine Installing Jenkins X using JX Boot (The Non CJXD edition) A collection of tutorials with JX salaboy/products-service salaboy/online-store salaboy/customers-service cd. foundationmore…                [Complete Jenkins Pipeline Tutorial     Jenkinsfile explained](https://www. youtube. com/watch?v=7KCS70sCoK0)           Jenkins-Nexus Demo Publishing Artifacts to Sonatype Nexus using Jenkins Pipelines"
    }, {
    "id": 55,
    "url": "http://localhost:4000/kind",
    "title": "kind - Kubernetes in Docker",
    "body": " Installating kind     Installating kind on Linux   Create a kind cluster   Create kind cluster using config file.     With ingress Installing kind on MacOS How to login to a “node” in kind cluster Removing clusters References TroubleshootingInstallating kind: Installating kind on Linux: Install Docker ## remove if any docker componentssudo apt-get remove docker docker-engine docker. io containerd runc## Update packagessudo apt-get updatesudo apt-get install \  ca-certificates \  curl \  gnupg \  lsb-release## Add Docker’s official GPG key:curl -fsSL https://download. docker. com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring. gpg## set repoecho \  deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring. gpg] https://download. docker. com/linux/ubuntu \ $(lsb_release -cs) stable  | sudo tee /etc/apt/sources. list. d/docker. list &gt; /dev/null## Install dockersudo apt-get updatesudo apt-get install docker-ce docker-ce-cli containerd. io## Add user to docker groupsudo usermod -a -G docker $USERInstall Kind curl -Lo . /kind https://kind. sigs. k8s. io/dl/v0. 11. 1/kind-linux-amd64chmod +x . /kind## move to any system path if neededsudo mv . /kind /usr/bin/kindCreate a kind cluster: kind create cluster will create a cluster with below specs.  Cluster name will be ‘kind’ Cluster will have only one node (control plane node only)kind create cluster --name test-kindMulti-node clusters Check kind-example-config sample. # three node (two workers) cluster configkind: ClusterapiVersion: kind. x-k8s. io/v1alpha4nodes:- role: control-plane- role: worker- role: worker# a cluster with 3 control-plane nodes and 3 workerskind: ClusterapiVersion: kind. x-k8s. io/v1alpha4nodes:- role: control-plane- role: control-plane- role: control-plane- role: worker- role: worker- role: workerSample kind configuration file # this config file contains all config fields with comments# NOTE: this is not a particularly useful config filekind: ClusterapiVersion: kind. x-k8s. io/v1alpha4name: kind-demo-multinodenetworking: # WARNING: It is _strongly_ recommended that you keep this the default # (127. 0. 0. 1) for security reasons. However it is possible to change this.  apiServerAddress:  127. 0. 0. 1  # By default the API server listens on a random open port.  # You may choose a specific port but probably don't need to in most cases.  # Using a random port makes it easier to spin up multiple clusters.  apiServerPort: 6443# patch the generated kubeadm config with some extra settingskubeadmConfigPatches:- | apiVersion: kubelet. config. k8s. io/v1beta1 kind: KubeletConfiguration evictionHard:  nodefs. available:  0% # patch it further using a JSON 6902 patchkubeadmConfigPatchesJSON6902:- group: kubeadm. k8s. io version: v1beta2 kind: ClusterConfiguration patch: |  - op: add   path: /apiServer/certSANs/-   value: my-hostname# 1 control plane node and 3 workersnodes:# the control plane node config- role: control-plane kubeadmConfigPatches: - |  kind: InitConfiguration  nodeRegistration:   kubeletExtraArgs:    node-labels:  ingress-ready=true  extraPortMappings: - containerPort: 80  hostPort: 80  protocol: TCP - containerPort: 443  hostPort: 443  protocol: TCP# the three workers- role: worker- role: workerCreate kind cluster using config file. : kind create cluster --config kind-example-config. yaml$ kubectl get nodesNAME         STATUS  ROLES         AGE   VERSIONkind-control-plane  Ready  control-plane,master  8m20s  v1. 21. 1kind-worker     Ready  &lt;none&gt;         7m49s  v1. 21. 1kind-worker2     Ready  &lt;none&gt;         7m48s  v1. 21. 1$ kubectl cluster-info --context kind-kindKubernetes control plane is running at https://127. 0. 0. 1:6443CoreDNS is running at https://127. 0. 0. 1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. With ingress: kind: ClusterapiVersion: kind. x-k8s. io/v1alpha4name: demonodes:- role: control-plane kubeadmConfigPatches: - |  kind: InitConfiguration  nodeRegistration:   kubeletExtraArgs:    node-labels:  ingress-ready=true    extraPortMappings: - containerPort: 80  hostPort: 80  protocol: TCP - containerPort: 443  hostPort: 443  protocol: TCPkubectl apply -f https://raw. githubusercontent. com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy. yamlInstalling kind on MacOS: ## Install on macbrew install kind## Start using podmanKIND_EXPERIMENTAL_PROVIDER=podman kind create clusterHow to login to a “node” in kind cluster: ## get docker container detailsdocker psdocker exec -it CONTAINER_ID /bin/shRemoving clusters: kind delete clusterkind delete cluster --name test-k8sReferences:  Kubernetes with kind - baeldung. comTroubleshooting: gini@greenmango ginigangadharan. github. io % kind create clusterCreating cluster  kind  . . . ✓ Ensuring node image (kindest/node:v1. 21. 1) 🖼 ✓ Preparing nodes 📦  ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ERROR: failed to create cluster: failed to get kubeconfig to merge: yaml: unmarshal errors: line 47: mapping key  apiVersion  already defined at line 1 line 48: mapping key  clusters  already defined at line 2 line 53: mapping key  contexts  already defined at line 11 line 58: mapping key  current-context  already defined at line 36 line 59: mapping key  kind  already defined at line 37 line 60: mapping key  preferences  already defined at line 38 line 61: mapping key  users  already defined at line 39"
    }, {
    "id": 56,
    "url": "http://localhost:4000/kubernetes-installation",
    "title": "Kubernetes Installation",
    "body": " 1. kubernetes-installation-using-kubeadm 2. Installing Kubernetes the Hardway     2. 1. Install virtualbox and vagarnt   2. 2. then….     3. Kubernetes Installation using Ansible1. kubernetes-installation-using-kubeadm https://kubernetes. io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin https://medium. com/containerum/4-ways-to-bootstrap-a-kubernetes-cluster-de0d5150a1e4Upgrade the version of kubeadm on the master and node01 to 1. 16. 0For easier installation, use apt-get install instead of upgrade sudo apt-get update &amp;&amp; sudo apt-get install -qy kubeadm=1. 16. 0-00Initialize Control Plane Node (Master) $ kubeadm initYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/. kube sudo cp -i /etc/kubernetes/admin. conf $HOME/. kube/config sudo chown $(id -u):$(id -g) $HOME/. kube/configYou should now deploy a pod network to the cluster. Run  kubectl apply -f [podnetwork]. yaml  with one of the options listed at: https://kubernetes. io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 172. 17. 0. 17:6443 --token baawxx. 9ez921jerna2rb6f \  --discovery-token-ca-cert-hash sha256:98cb7e30b0361da3e3eb689cd74af9288dfcc99d36c9b5541b1765f9a9f0bd20If you do not have the token, you can get it by running the following command on the control-plane node: kubeadm token listBy default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node: on node01 kubeadm join 172. 17. 0. 17:6443 --token baawxx. 9ez921jerna2rb6f \  --discovery-token-ca-cert-hash sha256:98cb7e30b0361da3e3eb689cd74af9288dfcc99d36c9b5541b1765f9a9f0bd20Install a Network Plugin. As a default, we will go with weave kubectl apply -f  https://cloud. weave. works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n') 2. Installing Kubernetes the Hardway GCP VirtualBox2. 1. Install virtualbox and vagarnt: 2. 2. then…. : 3. Kubernetes Installation using Ansible https://www. youtube. com/watch?v=kMKkpgB6IwM https://kubernetes. io/blog/2019/03/15/kubernetes-setup-using-ansible-and-vagrant/ https://medium. com/faun/how-to-create-your-own-kubernetes-cluster-using-ansible-7c6b5c031a5d https://www. azavea. com/blog/2014/10/30/running-vagrant-with-ansible-provisioning-on-windows/"
    }, {
    "id": 57,
    "url": "http://localhost:4000/kubernetes",
    "title": "Kubernetes - Tools, Guides, References",
    "body": "These are collection of reference documents and blog posts from different experts around. Check techbeatly. com/k8s for latest promos and articles.  Learn Kubernetes     Kubernetes Certification   CKS-Certified-Kubernetes-Security-Specialist   Learn Kubernetes from VMWare   Learn ECS and EKS    Kuberenets ToolBox     Kubernetes Cluster Management Tools   Kubernetes Development Tools   Kubernetes Monitoring Tools   Kubernetes Network Policy Tools   Kubernetes Service Mesh Tools   Kubernetes Security Tools   Kubernetes Tracing and Logging Tools   Kubernetes Command Line Utlities    Career in Kubernetes     Kubernetes Interview Questions    Tanzu Kubernetes Grid - TNG ReferencesLearn KubernetesKubernetes Certification:  Read my article on Certification &amp; Exam Tips, Learning Paths - CKA, CKAD &amp; CKS – Learning Path and CertificationRecommended Courses  Certified Kubernetes Administrator (CKA) - Kodekloud / Udemy Certified Kubernetes Application Developer (CKAD) - Kodekloud / Udemy Certified Kubernetes Security Specialist (CKS) - Kodekloud / UdemyCKS-Certified-Kubernetes-Security-Specialist:  CKSS-Certified-Kubernetes-Security-SpecialistLearn Kubernetes from VMWare:  KubeAcademyLearn ECS and EKS:  ECS Workshop by AWS AWS CodeDeploy now supports linear and canary deployments for Amazon ECS AWS Container Security Serverless Workshops Building Serverless Web Applications with React and AWS Amplify WILD RYDESKuberenets ToolBoxKubernetes Cluster Management Tools:  kubespray - Deploy a Production Ready Kubernetes Cluster using Ansible.      Deploying Kubernetes with Kubespray - Video Guide    kubeadm - Kubeadm is a tool built to provide kubeadm init and kubeadm join as best-practice “fast paths” for creating Kubernetes clusters.  kops - kOps - Kubernetes Operations k9s - K9s is a terminal based UI to interact with your Kubernetes clusters. The aim of this project is to make it easier to navigate, observe and manage your deployed applications in the wild. K9s continually watches Kubernetes for changes and offers subsequent commands to interact with your observed resources.  kube-up. sh - deprecated.  Cluster API - Cluster API is a Kubernetes sub-project focused on providing declarative APIs and tooling to simplify provisioning, upgrading, and operating multiple Kubernetes clusters.  metalk8s - An opinionated Kubernetes distribution with a focus on long-term on-prem deployments Rancher - Rancher is an open source project that provides a container management platform built for organizations that deploy containers in production. Rancher makes it easy to run Kubernetes everywhere, meet IT requirements, and empower DevOps teams.  kind - Kubernetes IN Docker - local clusters for testing Kubernetes KubeSphere - Enterprise-grade container platform tailored for multicloud and multi-cluster management Kubernetes Instance Calculator by learnk8s - You can use the calculator to explore the best instance types for your cluster based on your workloads.   Kubernetes Development Tools:   k8slens. dev - Kuberenetes IDE for developers portworx. com - Kubernetes spec generator containerlabs. kubedaily. com 50+ Useful Kubernetes Tools for 2020 copper - Copper is a simple tool for validate your configuration files. This is specifically useful with Kubernetes configuration files to enforce best practices, apply policies and compliance requirements. Kubernetes Monitoring Tools:  thanos- Thanos is a set of components that can be composed into a highly available metric system with unlimited storage capacity, which can be added seamlessly on top of existing Prometheus deployments.  prometheus - Prometheus, a Cloud Native Computing Foundation project, is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts when specified conditions are observed.  grafana - The open and composable observability and data visualization platform. Visualize metrics, logs, and traces from multiple sources like Prometheus, Loki, Elasticsearch, InfluxDB, Postgres and many more. Kubernetes Network Policy Tools:  kubepox - Kubernetes network Policy eXploration tool: A simple tools that allows you to query all the defined network policies, and associated affected Pods.  calico - Cloud native networking and network security kokotap - Tools for kubernetes pod network tapping Kubernetes Service Mesh Tools:  traefik - The Cloud Native Application Proxy istio - Connect, secure, control, and observe services.  kubernetes-ingress - NGINX and NGINX Plus Ingress Controllers for Kubernetes autopilot - Autopilot is an SDK and toolkit for developing and deploying service mesh operators.  Network Policy Editor - by CiliumKubernetes Security Tools:  kubestriker - A Blazing fast Security Auditing tool for kubernetes!! terrascan - Detect compliance and security violations across Infrastructure as Code to mitigate risk before provisioning cloud native infrastructure.  kube-bench- Checks whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark kubescape - kubescape is the first tool for testing if Kubernetes is deployed securely as defined in Kubernetes Hardening Guidance by to NSA and CISA Kubernetes Hardening GuidanceKubernetes Tracing and Logging Tools:  Loki - like Prometheus, but for logs.  elastic - Bring Kubernetes logs, metrics, and traces together Kiali- Kiali provides answers to the questions: What microservices are part of my Istio service mesh and how are they connected? jaeger - Distributed Tracing SystemKubernetes Command Line Utlities:  Kubectl Command Cheatsheet kuttle - kubectl wrapper for sshuttle without SSHCareer in KubernetesKubernetes Interview Questions:  30 Best Kubernetes Interview Questions and AnswersTanzu Kubernetes Grid - TNG VMWare TanzuReferences Kubernetes Configuration &amp; Best Practices Kubernetes NodePort vs LoadBalancer vs Ingress? When should I use what? Goodbye Docker Desktop, Hello Minikube!"
    }, {
    "id": 58,
    "url": "http://localhost:4000/linkedin-hacks",
    "title": "Linkedin Hacks",
    "body": "References : Recruitment Geeks "
    }, {
    "id": 59,
    "url": "http://localhost:4000/linux-os-hardening",
    "title": "Linux - OS Hardening & Rule Samples",
    "body": "https://www. pluralsight. com/blog/it-ops/linux-hardening-secure-server-checklist https://www. cyberciti. biz/tips/linux-security. html (image: @nathdah) "
    }, {
    "id": 60,
    "url": "http://localhost:4000/minikube",
    "title": "Minikube",
    "body": " 1. Introduction     Install minikube    2. Just to test ? Use this Lab 3. Deploy minikube     3. 1. Setup Docker or Virtualization   3. 2. Install and Configure kubectl   3. 3. Download and Install minikube   3. 4. Configure to Run docker without sudo   3. 5. Start minikube         3. 5. 1. Start minikube cluster     3. 5. 2. Verify cluster information          3. 6. Kubernetes Dashboard         3. 6. 1. If you are running minikube inside a VM          3. 7. Get your hands dirty !         3. 7. 1. If you are on Remote VM          3. 8. LoadBalancer deployments   3. 9. Finished Testing ?    4. Troubleshooting     4. 1. minikube start exits with error on GUEST_MISSING_CONNTRACK    5. References1. IntroductionInstall minikube: curl -LO https://storage. googleapis. com/minikube/releases/latest/minikube-linux-amd64sudo install minikube-linux-amd64 /usr/local/bin/minikubeNote: Before you start minikube cluster minikube will need a hypervisor (VirtualBox or KVM) but if you are already inside a virtual machine (and nested virtualization is not possible), then it is possible to skip the creation of an additional VM layer by using the none driver. But please note, you need to install and setup docker environment in that vm. To make none the default driver: $ sudo minikube config set vm-driver none❗ These changes will take effect upon a minikube delete and then a minikube start## You can also mention the kubernetes version you need to install. $ minikube start --kubernetes-version v1. 16. 0## or $ minikube start --vm-driver=docker --wait=false \  --kubernetes-version=v1. 18. 32. Just to test ? Use this LabThere is a simple lab to see and test minikube; use it here 3. Deploy minikube3. 1. Setup Docker or Virtualization: Here we are using docker instead of Virtualization (VirtualBox etc. ). Hence we need to install docker for the same. Below steps are for Debian and you can refer documentation for other operating systems. (or Install Docker on Debian) ## remove any existing package$ sudo apt-get remove docker docker-engine docker. io containerd runc## Update the apt package index and install packages ## to allow apt to use a repository over HTTPS:$ sudo apt-get updatesudo apt-get install \  apt-transport-https \  ca-certificates \  curl \  gnupg \  lsb-release## Add Docker’s official GPG key:$ curl -fsSL https://download. docker. com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring. gpg $ echo \  deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring. gpg] https://download. docker. com/linux/debian \ $(lsb_release -cs) stable  | sudo tee /etc/apt/sources. list. d/docker. list &gt; /dev/null## Install docker and tools$ sudo apt-get update$ sudo apt-get install docker-ce docker-ce-cli containerd. io3. 2. Install and Configure kubectl: You need kubectl to manage your cluster resources. ## Download kubectlcurl -LO  https://dl. k8s. io/release/$(curl -L -s https://dl. k8s. io/release/stable. txt)/bin/linux/amd64/kubectl ## Download the kubectl checksum file and Validate$ curl -LO  https://dl. k8s. io/$(curl -L -s https://dl. k8s. io/release/stable. txt)/bin/linux/amd64/kubectl. sha256 $ echo  $(&lt;kubectl. sha256) kubectl  | sha256sum --check## Install $ sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectlNote: if you have issue with sudo access or other thing try below. # Make the kubectl binary executable. chmod +x . /kubectl# Move the binary in to your PATH. sudo mv . /kubectl /usr/local/bin/kubectlTest to ensure the version you installed is up-to-date: kubectl version --client3. 3. Download and Install minikube: # download and install package$ curl -LO https://storage. googleapis. com/minikube/releases/latest/minikube-linux-amd64 $ sudo install minikube-linux-amd64 /usr/local/bin/minikube# check version$ minikube versionminikube version: v1. 21. 0commit: 76d74191d82c47883dc7e1319ef7cebd3e00ee113. 4. Configure to Run docker without sudo: Add your user to the docker group: $ sudo usermod -aG docker $USER &amp;&amp; newgrp docker3. 5. Start minikube: 3. 5. 1. Start minikube cluster: $ sudo minikube start --vm-driver=none --wait=false😄 minikube v1. 5. 2 on Debian 9. 11🤹 Running on localhost (CPUs=2, Memory=7483MB, Disk=10013MB) . . . ℹ️  OS release is Debian GNU/Linux 9 (stretch)⚠️ VM may be unable to resolve external DNS records🐳 Preparing Kubernetes v1. 16. 2 on Docker '19. 03. 5' . . . 💾 Downloading kubelet v1. 16. 2💾 Downloading kubeadm v1. 16. 2🚜 Pulling images . . . 🚀 Launching Kubernetes . . . 🤹 Configuring local host environment . . . ⚠️ The 'none' driver provides limited isolation and may reduce system security and reliability. ⚠️ For more information, see:👉 https://minikube. sigs. k8s. io/docs/reference/drivers/none/⚠️ kubectl and minikube configuration will be stored in /root⚠️ To use kubectl or minikube commands as your own user, you may need to relocate them. For example, to overwrite your own settings, run:  ▪ sudo mv /root/. kube /root/. minikube $HOME  ▪ sudo chown -R $USER $HOME/. kube $HOME/. minikube💡 This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=true⌛ Waiting for: apiserver🏄 Done! kubectl is now configured to use  minikube 💡 For best results, install kubectl: https://kubernetes. io/docs/tasks/tools/install-kubectl/Now we have a running kubernetes cluster and let us see the status. 3. 5. 2. Verify cluster information: $ kubectl cluster-info$ kubectl get nodesNAME    STATUS  ROLES         AGE  VERSIONminikube  Ready  control-plane,master  15m  v1. 20. 73. 6. Kubernetes Dashboard: You can enable the dashboard as below. (Take another console to run it) $ sudo minikube dashboard🤔 Verifying dashboard health . . . 🚀 Launching proxy . . . 🤔 Verifying proxy health . . . http://127. 0. 0. 1:39067/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/Usually minikube will open the browser with the dashboard URL if you are with GUI. Access the url and check. 3. 6. 1. If you are running minikube inside a VM: Then accessing Dashboard from host machine (or from your laptop) is bit tricky. By default minikube dashboard will exposed as localhost:PORT. In above example, I have deployed the minikube inside a GCP (Google Cloud Platform) instance and see below how I am accessing the Dashboard. on minikube VM ## $ minikube dashboard🤔 Verifying dashboard health . . . 🚀 Launching proxy . . . 🤔 Verifying proxy health . . . 🎉 Opening http://127. 0. 0. 1:36959/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser. . . 👉 http://127. 0. 0. 1:36959/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/On my Laptop/Workstation ## Open an SSH tunnel ## ssh -L LOCAL_PORT:localhost:REMOTE_PORT user@REMOTE_VM_IP$ ssh -L 36959:localhost:36959 gini@123. 123. 234. 234. . gini@minikube-vm:~$ Open a browser and access the same url (output of minikube dashboard command) http://127. 0. 0. 1:36959/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ Bingo ! 3. 7. Get your hands dirty !: ## Deploy an app$ kubectl create deployment hello-minikube --image=k8s. gcr. io/echoserver:1. 4$ kubectl expose deployment hello-minikube --type=NodePort --port=8080$ kubectl get services hello-minikube## let minikube open the browser$ minikube service hello-minikube## Alternatively, use kubectl to forward the port:kubectl port-forward service/hello-minikube 7080:80803. 7. 1. If you are on Remote VM: $ ssh -L 7080:localhost:7080 gini@123. 123. 234. 234And access the url fromlocal browser - localhost:7080 3. 8. LoadBalancer deployments: To access a LoadBalancer deployment, use the “minikube tunnel” command. Here is an example deployment: kubectl create deployment balanced --image=k8s. gcr. io/echoserver:1. 4 kubectl expose deployment balanced --type=LoadBalancer --port=8080## In another window, start the tunnel to create a routable IP for the ‘balanced’ deployment:minikube tunnelTo find the routable IP, run this command and examine the EXTERNAL-IP column:kubectl get services balancedYour deployment is now available at :8080 3. 9. Finished Testing ?: ## Pause Kubernetes without impacting deployed applications$ minikube pause## Halt the cluster:$ minikube stop4. Troubleshooting4. 1. minikube start exits with error on GUEST_MISSING_CONNTRACK: Error❌ Exiting due to GUEST_MISSING_CONNTRACK: Sorry, Kubernetes 1. 20. 7 requires conntrack to be installed in root’s path Solution sudo apt-get install -y conntrack5. References https://github. com/kubernetes/minikube https://minikube. sigs. k8s. io/docs/start/"
    }, {
    "id": 61,
    "url": "http://localhost:4000/monitoring",
    "title": "Prometheus and Grafana",
    "body": " Monitoring and Alerting on your Kubernetes Cluster with Prometheus and Grafana"
    }, {
    "id": 62,
    "url": "http://localhost:4000/network-basic-commands",
    "title": "Basic Network Device Commands and Setup",
    "body": "These are very basic network commands and setup instructions for my Ansible Network Automation labs. I am keeping those here for quick reference in case I am in trouble with a quick lab setup in GNS3. autoauto- Enable SSH on HPE Comware7 Devicesauto- Assign IP Addressauto- Enable SSH/tty and login on Cisco Devicesauto- Add a user for remote accessauto- Configure Router interfaceauto- Configure routesauto - Enabled Terminal monitoringautoauto Enable SSH on HPE Comware7 DevicesAssign IP AddressEnable SSH/tty and login on Cisco Devices(config)# line vty 0 4(config-line)# transport input all(config-line)# login local(config)# ip domain-name mydomain. com(config)# crypto key generate rsa# enable ssh v2(config)# ip ssh version 2 Add a user for remote accessconfigure terminalusername user1 password mypassword                # create a user with passwordusername user1 privilege 15                 # configure privilege   enable password mypassword                # setting enable password             ### Sampleusername cisco password C!sco123username cisco privilege 15#or username cisco role priv-15enable password C!sco123#orfeature privilegeenable secret 0 admin priv-lvl 15Configure Router interfaceR1#conf tEnter configuration commands, one per line.  End with CNTL/Z. R1(config)#int f0/0              R1(config-if)#ip address 10. 1. 1. 100 255. 255. 255. 0R1(config-if)#int loop 0             R1(config-if)#ip address 1. 1. 1. 1 255. 255. 255. 0  R1(config)#int f0/0R1(config-if)#no shutConfigure routesR1(config)#router ospf 1R1(config-router)#network 0. 0. 0. 0 255. 255. 255. 255 area 0R2#sh ip ospf neighborNeighbor ID   Pri  State      Dead Time  Address     Interface1. 1. 1. 1      1  FULL/DR     00:00:34  10. 1. 1. 101   FastEthernet0/0R2#sh ip routeCodes: C - connected, S - static, R - RIP, M - mobile, B - BGP    D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area    N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2    E1 - OSPF external type 1, E2 - OSPF external type 2    i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2    ia - IS-IS inter area, * - candidate default, U - per-user static route    o - ODR, P - periodic downloaded static routeGateway of last resort is not set   1. 0. 0. 0/32 is subnetted, 1 subnetsO    1. 1. 1. 1 [110/11] via 10. 1. 1. 101, 00:01:06, FastEthernet0/0   2. 0. 0. 0/32 is subnetted, 1 subnetsC    2. 2. 2. 2 is directly connected, Loopback0   10. 0. 0. 0/24 is subnetted, 1 subnetsC    10. 1. 1. 0 is directly connected, FastEthernet0/0Enabled Terminal monitoring: # ter mon"
    }, {
    "id": 63,
    "url": "http://localhost:4000/nexus-sonatype",
    "title": "Nexus Sonatype",
    "body": "## Start a Nexus Repo Containerdocker pull sonatype/nexus3docker run -d --name nexus_repo -p 8081:8081 sonatype/nexus3## wait for message stating Started Sonatype Nexus OSS 3. 20. 1-01docker logs nexus_repo -f## The default username is admin## get the initial passworddocker exec -i nexus_repo cat /nexus-data/admin. password## access the portal on http://your-ip-addr:8081```bash dockerfile for ansible: "
    }, {
    "id": 64,
    "url": "http://localhost:4000/nginx",
    "title": "nginx configuration samples",
    "body": " Basic website configuration nginx proxy_pass - site2. com -&gt; site1. com/site2-page Basic Reverse Proxy ConfigurationBasic website configuration: server {  listen 8080;  listen [::]:8080;  server_name example. com;  root /var/www/example;  index index. php index. html index. htm;  access_log /var/log/nginx/access_example. log;  error_log /var/log/nginx/error_example. log;}nginx proxy_pass - site2. com -&gt; site1. com/site2-page: Scenarios  nginx and wordpress - site2. com need to load content of of site1. com/site2 without url change show a landing page for subdomain (sub. site1. com) by using site1. com/sub-page How to Redirect a Domain without Changing the URL on browser. Sample nginx configuration file for site2. com server {    index index. php index. html index. htm;  server_name site2. com www. site2. com;  access_log /var/log/nginx/access_site2. log;  error_log /var/log/nginx/error_site2. log;  location / {    proxy_pass https://www. site1. com/site2/;    proxy_ssl_server_name on;  }  listen [::]:443 ssl ipv6only=on; # managed by Certbot  listen 443 ssl; # managed by Certbot  ssl_certificate /etc/letsencrypt/live/site2. com/fullchain. pem; # managed by Certbot  ssl_certificate_key /etc/letsencrypt/live/site2. com/privkey. pem; # managed by Certbot  include /etc/letsencrypt/options-ssl-nginx. conf; # managed by Certbot  ssl_dhparam /etc/letsencrypt/ssl-dhparams. pem; # managed by Certbot}server {  if ($host = www. site2. com) {    return 301 https://$host$request_uri;  } # managed by Certbot  if ($host = site2. com) {    return 301 https://$host$request_uri;  } # managed by Certbot  listen 80;  listen [::]:80;  server_name site2. com www. site2. com;  return 404; # managed by Certbot}Basic Reverse Proxy Configuration: server { listen 80; listen [::]:80; access_log /var/log/nginx/reverse-access. log; error_log /var/log/nginx/reverse-error. log; location / {  proxy_pass http://127. 0. 0. 1:8080; }}(Photo by luis gomes from Pexels) References How to Redirect a Domain without Changing the URL(. htaccess and apache) My post on serverfault. com"
    }, {
    "id": 65,
    "url": "http://localhost:4000/oci-cert-developer-associate",
    "title": "Oracle Cloud Infrastructure Developer 2020 Associate (1Z0-1084-20)",
    "body": " Cloud Native Fundamentals     CAP Theorem   Cloud Native Building Blocks   Service Communication Patterns   Messaing Protcols    Oracle Functions Overview Oracle Functions Core Concept     Function Metrics    Oracle Functions - Use cases, References API Gateway     Use cases    Resource Manager Overview Advanced Terraform     Terraform Modules   Terraform provisioners    OCI Streaming Services Oracle Kubernetes Engine (OKE) Creating OKE Cluster Accessing OKE Cluster using kubectl OCI Service Broker OCI API, SDK, CLI Key Management Overview IAM Overview     Authentication   Autherization    IAM Mangement - Compartments IAM Mangement - Policies IAM Mangement - Tags Testing Cloud Native Applications OCI Registry Service Managing Repos in OCIR Pulling an image from OCIR for Kubernetes Image Retention Policies Monitoring Service Overview Events - Overview and Key Features Events - Core concepts Events - Use cases, Reference ArchCloud Native Fundamentals: Cloud Native vs Traditional Arch  Stateful vs Statelsss Service Orchestration vs Service Chroreography Dealing with failuresCAP Theorem: Distributed Database system  reliable network zero latency infinite bandwidth secure network no change in topology one admin zero transport cost homogenrous networkCloud Native Building Blocks:  Microservices     service oriented arch   Loosly coupled services   organized around business capabilties   Defence in Depth Approach  source code - track, audit, in repo Container image - include only bare minimum needed Container Registry - use private registry, vulnerability scanning (twistlock) pods - image from approved registry, use pod security policies, host ports, networking Cluster/Orchestrator - secure access, Enable RBAC, Enable Audit logs Containers FunctionsService Communication Patterns:  External Communication - to/from external services   Internal Communication - service to service (within cluster)     Sync vs Async Comm Protocols   HTTP, HTTPv2, WebSocket, gRPCMessaing Protcols:  MQTT (Message Que Telemetry Transport)   AMQP (Advance Message Queuing Protocol)   Pub/Sub   Idempotency - multiple lines but single effect. message can process more than once   Serialization     JSON - readable, large memory footprint   Protobuf - binary format, schema defined in . proto files   Oracle Functions Overview: fnproject. io  Function as a Service Oracle Cloud integrated Container Native Opensource Engine Multi tenant   Secure   run only when triggered   pay for code execution only   Function Development kit - FDK - Python, Java,Go, node. js, rubyOracle Functions Core Concept:  grouped into applications   built as a docker image and pushed to a specified docker registry   invoke on cli, SDK, HTTP Req, other OCI services docker will be pulled and run; after some idle perioed container will be removed.  Also time based functions define policy for permissionsFunction Metrics:  FunctionExecutionDuration FunctionInvocationCount FunctionResponseCountOracle Functions - Use cases, References:  Glue Cloud Services, Event Driven Web, Mobile, IoT backends Realtime file, stream process DevOps, Batch processAPI Gateway:  single gateway to multiple API services Routing Rate limiting Cross region resource sharing (CORS) Metrics Use cases:  RESTful API for functions Custom Development SaaS servicesResource Manager Overview:  Terraform as a service Stack - a set of OCI resources you want to create in a compartment job - a request to take a terraform action on a stack (plan, apply, destroy)Advanced Terraform:  stores the state of managed ifnra from the last time terraform was run use this state to create plans and make changes to your infraterraform. tfstateTerraform Local State File  stored locally on local machine in JSON format small and individual team not to scale for large teams require a more mono-repo patternTerraform Remote State File  write state to remote data store   can manage by large team   use target flag to create only that resourceTerraform Modules: Terraform provisioners:  ansible, che, puppet, shellInstance Principal Config OCI Streaming Services:  logs, web/IoT/mobile data 99. 95% SLA message - 64 bit encoded record or array of bytes key - and identifier stream - an append only log of messages topic - message category partitions - topic broken into partitions producer - create message consumer - subscribe and read consumer group - a groupDesign Considerations  retention max 7 days max message size 1 MB each partition 1000Emit API call per second, 5 Read API call per second each partition max total write rate 1 MB per second, read rate 2 MB per second each tenancy has a limit of 5 partitionsOracle Kubernetes Engine (OKE):  your own/DIY pre-built but managed by user managed serviceTerraform Kubernetes Installation for OCI Creating OKE Cluster:  monthly 3 cluster oer OCI regions, with 1000 nodes pay as you go model got only 1 clusterAccessing OKE Cluster using kubectl:  need oci cli then setup kubeconfig in oci cli  create serviceaccount with clusteradmin role to access dashboard use token to loginOCI Service Broker:  software to implement the Open service broker API enable cloud service lifecycle through devops tools (provision,bind,deprovision) specific to cloud vendor CI/CD -&gt; OCI Registry -&gt; OKE CLuster &lt;- Service broker &lt;- OCI Cloud servicesOCI API, SDK, CLI: Diff ways to access OCI  GUI Console REST API - References and end points Terraform SDK- custom solutions based on java, python, ruby, go; need OCI account, user ID, key paid ANsible CLI - required keys and OCID. oci setup to configure, bash or pip install oci Resource ManagerKey Management Overview:  managed service to encrypt data FIPS 140-2, Security level 3 create keys, disable keys, re-enable keys with key vault rotate keys to meet your security governance and regulatory compliance needs; store in version   with IAM users and groups   vaults - logical entities to store keys seperate compartment for keys users/groups need access to keysDesign Considerations  regional service, repicate keys across 3 AD block volumes and object storage are integrated with key managedment rotaing key will not re-encrypt data that was previously encrypted with old key version; only when customer modify data no import or export of keys cannot delete keys, bud disabe. delete key vault schedule vault deletion - 7-30 days cannot recover once deletedpricing on vault - per hour IAM Overview: OCID - Oracle Cloud ID Instance principal - for instance to make api calls Authentication:  authenticate principal by     username /password   API Signining Key   Auth Tokens - no expire   Autherization:  specified various actions an authenticatedIAM Mangement - Compartments:  sub compartments upto six level deep atleast one policy to access it subcompartments inherits access permisions from compartments higher up its hierarchyCompartment Quotas  similar to service limit but set by administrator using policies set, unset, zeroIAM Mangement - Policies: Verbs  inspect - ability to list read - inspect + get user specified metadata use - read + ability to work with existing resources manage - include all permission for resourceAggregated - all-resource, database-family, instance-family etcIndividual - individual resource Permissions  VOLUME_INSPECT, READ, VOLUME_WRITE,API Operation  ListVolume, GetVolume,CreateVolumeCommon Policies IAM Mangement - Tags: Tag Namespace - a container for set of tag keys with tag key definitions Testing Cloud Native Applications:  mock - for testing fake - with working implementation stub - some data returnsBuilding block of testing  unit tests service test/component level user interface testsOCI Registry Service: Oracle Cloud Infrastructure Registry - OCIR  Fully managed HA Docker v2 registry private or public repo   full integration with OKE   OCI Registry is FREE Only charges for OCI resourcesManaging Repos in OCIR:  need proper permissions by policies users need username and auth token beore being able to push/pull image docker login . ocir. ioPulling an image from OCIR for Kubernetes:  create a secret and use it as imagePullSecretImage Retention Policies:  not tagged for a certain period not pulled for a certain period   not given particular tag   Hourly process to reomve image policies to retain/remove imagesMonitoring Service Overview:  monitor cloud resources supports metrics and alarms compite, VCN, LB, block, object storage, notification, streaming   health, capacity, performance   email or pagerduty   metric query language (MQL)     metric is a measurement related to health, capacity or performance of a given resource     metric stream - aggregated data of multiple metric data   internval 01,5,60 min etc dimension - name-value pairs Grouping - groupBy() aggregates results by groups statistics - count, max, mean, rat, min, sum , percetileAlarms &amp; States  Firing - alarm triggered Reset - alarm not detecting the metric firing, metric no longer being emitted Suppress - avoid publishing messages during the specified time range eg maintenancemetric + alarm + topic Events - Overview and Key Features:  fully managed event-routing platform   using CNCF’s cloudevent (opensource) standard     integration with oracle functons, streaming, notification   Events - a structured and schematized message that denotes a change in resource Rule - the object where a user defines which event they care about and trgigger an actions an actions if it occurs Actions - the user-defined response to when an event occurs eg: triggering a functions or wrtingin to a stream, send notificaiton etcEvents - Core concepts: name &amp; compartment -&gt; trigger condition -&gt; Action    max rules limit 50 / tenancy (can request more)   user defined response to a rule being matched multiple actions canEvents - Use cases, Reference Arch: "
    }, {
    "id": 66,
    "url": "http://localhost:4000/oci-cert-foundations",
    "title": "Oracle Cloud Infrastructure Foundations",
    "body": " Start OCI     Cloud Concepts   OCI Architecture    Core OCI Services     OCI Computer Services   OCI Storage Services   OCI Network Services         VCN to VCN Comms     Load Balancer          OCI IAM         Principals     Compartment     Authentication     Authorization          OCI Database Services         OCI DB Options          OCI Security         Shared Security Model     Federation     Data Protection     Key Management     OS Management Service     Network Protection     OCI Web Application Firewall     Compliance Certifications          OCI Pricing and Billing         Pricing Models          Exam : Oracle Cloud Infrastructure Foundations 2020 Associate (1Z0-1085-20)105 Minuts/60 Questions/ 68% to pass Start OCICloud Concepts:  21 Regions AD - Availability Domains, isolated each other within region 3 AD (some 2 or 1) FD - Fault Domain (Logical Domain)OCI Architecture:  Compartment - logical collection of resourcesCore OCI ServicesOCI Computer Services:  baremetal Dedicated VM VM COntainer Enginer FunctionsOCI Storage Services:  block volume     Highly Durable - 3 replicas   Tiers - can be upto 32 GB (and maximum 32 volumes/disks)         Basic     Balanced     High Performance                local NVMe - temp storage, non-persistent     high performance       File Storage - files     NFS and SMB   Highly durable   can take snapshot    Object Storage     all in root folder, no folder   unstrctured   hot         standard storage tier          cold         archive, backup     10x cheaper than standard     90 days retention minimum     4 hr recovery/retrieve          OCI Network Services:  VCN - Virtual Cloud Network Internet Gateway - 2 way NAT Gateway - only inwards Public Subnet /DMZ DRG - Virtual Router to connect On-Prem or Other destination     IPSec   Fast Connect    Service Gateway - to connect internal objects like Storage; connection via VCN Firewall rules to restrict traffic (port/subnet)     Ingress   Egress    Network Security Group - NSG apply to VCNVCN to VCN Comms:  VCN Peering     Local VCN Peering - same regions   Remoet VCN Peering - different regions   Peering is not transit; need dedicated peering   Load Balancer:  Service Discovery Health Check AlgorithmOCI IAM: Principals: IAM entity that allowed to interact with OCI resources.  IAM Users and Groups Instance Principal - for instances, API, etcCompartment:  Collection of resourcesAuthentication:  Who OCI IAM service authenticate a Principal by     username, password   API Signing Key   Auth Tokens   Authorization:  Specifies various actions an authenticated principal can do OCI Authorization = policies Written in Human-readable format     Allow group GROUP_NAME to VERB RESOURCE_TYPE in tenancy         verb - read, inspect, use, manage     resource type - all,database, instance,virtual network etc                Policy attachment - attach to compartment or tenancyOCI Database Services: OCI DB Options:  VM DB systems - DB running on VM Bare Metal - on physical server Oracle RAC - Cluster, HA Exadata DB Autonomous - share Autonomous - dedicated     Self driving, Self securing, Self repairing   Operations  Start, stop, reboot Scale - CPU, Storage Patch - 2 steps process, DB first, then system.      Exadata and RAC - rolling patching   Backup/Restore  Manual or Auto Retention periodDR Systems  Oracle Data Guard Active Data Guard - extends data guard by providing advanced features for data protection and availability Switchover - Planned migratons, no data losss Failover - unplanned, minimal data lossDB Systems HA and DR  primary and standby can be single instance or RACAutonomous DB  Autonomous Transation Process (ATP) Autonomous Data Warehouse (ADW)OCI Security: Shared Security Model:  OCI manage infra Customer manage VMs and AppsFederation: Data Protection:  Data encrypted at-rest Data encrypted in-transit Bring your own keyson  block volume file storage object storage - pre-auth requests database - transparent data encryption, data safe, data vaultKey Management:  BYOK - Bring your own keys   HSM (hardware security modules)   data safeOS Management Service:  execute and automate common and complex management tasks package management, config management security and complianceNetwork Protection:  Tiered subnet strategy     DMZ   Public   Private    Gateways     NAT   Service   Dynamic Routing    Security List, NSGOCI Web Application Firewall: Compliance Certifications: OCI Pricing and Billing: Pricing Models:  Pay as you go (PAYG) Monthly Flex Bring your own lincense"
    }, {
    "id": 67,
    "url": "http://localhost:4000/openldap",
    "title": "Installing OpenLDAP",
    "body": "Install LDAP ServerInstall Packages: yum install *openldap* -yStart Service: # systemctl start slapd# systemctl enable slapdenable firewall port: # firewall-cmd --add-service=ldap Configure LDAP ServerUpdate admin password: # slappasswdNew password: Re-enter new password:{SSHA}8hG0fZ8xxDE8tcp2g0YAhurCy3qIZI+Icreate an LDIF file (ldaprootpasswd. ldif) which is used to add an entry to the LDAP directory. : # vim ldaprootpasswd. ldif# cat ldaprootpasswd. ldif dn: olcDatabase={0}config,cn=configchangetype: modifyadd: olcRootPWolcRootPW: {SSHA}8hG0fZ8xxDE8tcp2 olcDatabase: indicates a specific database instance name and can be typically found inside /etc/openldap/slapd. d/cn=config.  cn=config: indicates global config options.  PASSWORD: is the hashed string obtained while creating the administrative user. Add entry from file: # ldapadd -Y EXTERNAL -H ldapi:/// -f ldaprootpasswd. ldif SASL/EXTERNAL authentication startedSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=authSASL SSF: 0modifying entry  olcDatabase={0}config,cn=config Configure LDAP DBCopy Sample DB: # cp /usr/share/openldap-servers/DB_CONFIG. example /var/lib/ldap/DB_CONFIG# chown -R ldap:ldap /var/lib/ldap/DB_CONFIG # systemctl restart slapdimport some basic LDAP schemas from the /etc/openldap/schema directory as follows. : $ sudo ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine. ldif $ sudo ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis. ldif$ sudo ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson. ldifAdd domain info: # cat ldapdomain. ldif dn: olcDatabase={1}monitor,cn=configchangetype: modifyreplace: olcAccessolcAccess: {0}to * by dn. base= gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth  read by dn. base= cn=Manager,dc=lab,dc=com  read by * nonedn: olcDatabase={2}hdb,cn=configchangetype: modifyreplace: olcSuffixolcSuffix: dc=lab,dc=comdn: olcDatabase={2}hdb,cn=configchangetype: modifyreplace: olcRootDNolcRootDN: cn=Manager,dc=lab,dc=comdn: olcDatabase={2}hdb,cn=configchangetype: modifyadd: olcRootPWolcRootPW: {SSHA}8hG0fZ8xxDE8tcp2dn: olcDatabase={2}hdb,cn=configchangetype: modifyadd: olcAccessolcAccess: {0}to attrs=userPassword,shadowLastChange by dn= cn=Manager,dc=lab,dc=com  write by anonymous auth by self write by * noneolcAccess: {1}to dn. base=   by * readolcAccess: {2}to * by dn= cn=Manager,dc=lab,dc=com  write by * readUpdate DB # ldapmodify -Y EXTERNAL -H ldapi:/// -f ldapdomain. ldif Create baseldapdomain. ldif with more details: dn: dc=example,dc=comobjectClass: topobjectClass: dcObjectobjectclass: organizationo: example comdc: exampledn: cn=Manager,dc=example,dc=comobjectClass: organizationalRolecn: Managerdescription: Directory Managerdn: ou=People,dc=example,dc=comobjectClass: organizationalUnitou: Peopledn: ou=Group,dc=example,dc=comobjectClass: organizationalUnitou: Group and # ldapadd -Y EXTERNAL -x -D cn=Manager,dc=example,dc=com -W -f baseldapdomain. ldifConfigure Client using SSSD## Install packagesyum install sssd sssd-client## Configure Systemauthconfig --enablesssd --enablesssdauth --ldapserver= &lt;ldapserver&gt;  --ldapbasedn= &lt;ldap-base-dn&gt;  --enableldaptls --update## eg:authconfig --enablesssd --enablesssdauth --ldapserver= ldap. jumpcloud. com  --ldapbasedn= ou=Users,o=5ffd6b642482a16659721d5c,dc=jumpcloud,dc=com  --enableldaptls --updateAppendixReferencehttps://www. tecmint. com/install-openldap-server-for-centralized-authentication/ "
    }, {
    "id": 68,
    "url": "http://localhost:4000/openshift-3x-installation-gcp",
    "title": "OCP Origin 3.x Installation - on GCP",
    "body": " Master Server Configuration     Step 1 : Update system   Step 2 : Install Required Packages   Step 3 : Clone the package from github   Step 4: Configure your DNS server   Step 5 : Create a user with admin privilege   Step 6 : Start and Enable Docker   Step 7 : Configure Storage   Step 8 : Configure the inventory file   Step 9: Verify Pre-Req   Step 10: Deploy Cluster   Step 11: Configure cluster admin login   Step 12: Verify Cluster Status   Step 13: Verify basic pod status   Step 14: Verify Web Console   Restart or Check Status of openshift service   Some more configs - Configuring Host Variables   Verify PublicURL   Uninstalling OCP Cluster    Appendix     References   This is a quick guide to setup OpenShift Origin Cluster with 1 Master and 2 Nodes. We will use ansible playbooks provided by OpenShift-ansible. You need to setup a separate DNS server to manage internal DNS traffic. Master server hosted on GCP Components running on the server  kubernetes API Server etcd Controller Manager Server Ansible Docker Service OpenShift Container Registry Web console if any CLI ClientNode 1 &amp; 2: Hosted on GCP:  Docker service kubelet service proxyImportant Notes:    In ansible inventory file, ansible_service_broker_install=False; if this entry is true then playbook run will fail always if your DNS server didn’t configure properly.     If DNS server is ready, use wild card entry as *. master. sun. com   eg: *. apps. master. sun. com. IN A 10. 148. 0. 2     Configure static IP address on your Master servers and nodes.  Master Server ConfigurationBelow see the steps to follow. Step 1 : Update system: yum updaterebootStep 2 : Install Required Packages: # yum install -y wget git zile nano net-tools \	docker bind-utils iptables-services bridge-utils \	bash-completion kexec-tools sos psacct openssl-devel \	httpd-tools NetworkManager python-cryptography \	python2-pip python-devel python-passlib \	java-1. 6. 0-openjdk python-rhsm-certificates# yum group install  Development Tools  -y# yum install centos-release-openshift-origin39 #use correct version like 3. 6 or 3. 9# yum install epel* [epel repository]# sed -i -e  s/^enabled=1/enabled=0/  /etc/yum. repos. d/epel. repo #&lt;====&gt; to use epel repo only# systemctl start NetworkManager# systemctl enable NetworkManager# yum install --enablerepo=epel install ansible pyOpenSSL #&lt;===Only on masterStep 3 : Clone the package from github: git clone https://github. com/OpenShift/OpenShift-ansible. gitcd OpenShift-ansiblegit fetchgit checkout --track origin/release-3. 9 #&lt;====== Change branch to 3. 9 release If you find issue check files in master branch itself. Step 4: Configure your DNS server:  Configure DNSStep 5 : Create a user with admin privilege: Grant full sudo access via sudoers file. eg: # cat /etc/sudoers. d/ansibleansible ALL=(ALL:ALL) NOPASSWD:ALLStep 6 : Start and Enable Docker: sudo systemctl stop dockersudo systemctl restart dockersudo systemctl enable dockerStep 7 : Configure Storage: # cat /etc/sysconfig/docker-storage-setupSTORAGE_DRIVER=overlay2DOCKER_ROOT_VOLUME=yesDEVS=/dev/sdbVG=vg_origin01Step 8 : Configure the inventory file: Amend the entries in inventory file [OSEv3:children]mastersnodesetcd[OSEv3:vars]ansible_ssh_user=ansibleansible_become=truedebug_level=2deployment_type=originopenshift_deployment_type=originopenshift_release=v3. 6openshift_pkg_version=-3. 6. 0openshift_image_tag=v3. 6. 0openshift_service_catalog_image_version=v3. 6. 0template_service_broker_image_version=v3. 6. 0ansible_service_broker_install=False#htpasswd authopenshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]openshift_master_default_subdomain=cloudapps. techbeatly. local##containerizedcontainerized=trueopenshift_disable_check=docker_image_availability,disk_availability,docker_storage,memory_availabilityopenshift_node_kubelet_args={'pods-per-core': ['10']}# allow unencrypted connection within clusteropenshift_docker_insecure_registries=172. 30. 0. 0/16osm_use_cockpit=trueosm_etcd_image=registry. fedoraproject. org/f26/etcdopenshift_router_selector='region=primary'openshift_registry_selector='region=primary'# to fix the No package matching 'origin-docker-excluder-3. 7. 0 error# Ref: https://github. com/openshift/origin/issues/17199#issuecomment-342494842openshift_repos_enable_testing=trueopenshift_ip=10. 142. 0. 4[masters]ocp-master02 openshift_public_hostname=35. 227. 22. 165 openshift_hostname= {{ inventory_hostname }}  openshift_ip=10. 142. 0. 4 openshift_public_ip=35. 227. 22. 165 #openshift_node_group_name=ocpgce[etcd]ocp-master02 #openshift_node_group_name=ocpgce[nodes]ocp-master02 openshift_node_labels= {'region': 'infra', 'zone': 'default'}  openshift_ip=10. 142. 0. 4 openshift_public_ip=35. 227. 22. 165 #openshift_node_group_name=ocpgce #openshift_schedulable=falseocp-node03 openshift_node_labels= {'region': 'primary', 'zone': 'east'}  openshift_ip=10. 142. 0. 5 openshift_public_ip=35. 237. 132. 132 #openshift_node_group_name=ocpgce #openshift_schedulable=trueocp-node04 openshift_node_labels= {'region': 'primary', 'zone': 'west'}  openshift_ip=10. 142. 0. 8 openshift_public_ip=104. 196. 132. 75 #openshift_node_group_name=ocpgce #openshift_schedulable=trueStep 9: Verify Pre-Req: Release 3. 6, run below, ansible-playbook playbooks/byo/openshift-preflight/check. yml -i inventory/ocpIf you are facing below error, fatal: [localhost]: FAILED! =&gt; { attempts : 3,  changed : false,  msg :  No package matching 'origin-docker-excluder-3. 11**' found available, installed or updated ,  rc : 126,  results : [ No package matching 'origin-docker-excluder-3. 11**' found available, installed or updated ]} then use https://storage. googleapis. com/origin-ci-test/releases/openshift/origin/master/origin. repo. Step 10: Deploy Cluster: If pre-check is successful then run the main deployment. ansible-playbook playbooks/byo/config. yml -i inventory/ocpIf you face issues as below, refer This or This TASK [openshift_hosted_facts : Set hosted facts] *************************************************************************************************************************task path: /home/ansible/openshift-ansible/roles/openshift_hosted_facts/tasks/main. yml:9fatal: [ocp-master1]: FAILED! =&gt; {}MSG:|failed expects to merge two dictsAlso try by downgrading your ansible version. Step 11: Configure cluster admin login: Once the cluster is up and online log on to the master node and setup a user and password. [root@ocp-master1 ~]# htpasswd -b /etc/origin/master/htpasswd admin openshiftUpdating password for user admin[root@ocp-master1 ~]# openshift admin policy add-cluster-role-to-user cluster-admin admincluster role  cluster-admin  added:  admin Step 12: Verify Cluster Status: From the master node verify the cluster is up with no errors by running the oc status command. [root@ocp-master1 ~]# oc statusIn project default on server https://10. 142. 0. 2:8443https://docker-registry-default. cloudapps. techbeatly. local (passthrough) (svc/docker-registry) dc/docker-registry deploys docker. io/openshift/origin-docker-registry:v3. 6. 0	deployment #1 deployed 28 hours ago - 1 podsvc/kubernetes - 172. 30. 0. 1 ports 443-&gt;8443, 53-&gt;8053, 53-&gt;8053https://registry-console-default. cloudapps. techbeatly. local (passthrough) (svc/registry-console) dc/registry-console deploys docker. io/cockpit/kubernetes:latest	deployment #1 deployed 28 hours ago - 1 podsvc/router - 172. 30. 96. 77 ports 80, 443, 1936 dc/router deploys docker. io/openshift/origin-haproxy-router:v3. 6. 0	deployment #1 deployed 28 hours ago - 2 podsView details with 'oc describe &lt;resource&gt;/&lt;name&gt;' or list everything with 'oc get all'.  [root@ocp-master1 ~]# oc get allNAME         DOCKER REPO                         TAGS 	UPDATEDis/registry-console  docker-registry. default. svc:5000/default/registry-console  latest  28 hours agoNAME         REVISION  DESIRED  CURRENT  TRIGGERED BYdc/docker-registry  1   	1   	1   	configdc/registry-console  1   	1   	1   	configdc/router       1   	2   	2   	configNAME          DESIRED  CURRENT  READY   AGErc/docker-registry-1  1   	1   	1   	1drc/registry-console-1  1   	1   	1   	1drc/router-1       2   	2   	2   	1d NAME           HOST/PORT                       PATH 	SERVICES    	PORT 	TERMINATION  WILDCARDroutes/docker-registry  docker-registry-default. cloudapps. techbeatly. local     	docker-registry	&lt;all&gt;   passthrough  Noneroutes/registry-console  registry-console-default. cloudapps. techbeatly. local     	registry-console  &lt;all&gt;   passthrough  NoneNAME          CLUSTER-IP  	EXTERNAL-IP  PORT(S)        	AGEsvc/docker-registry  172. 30. 190. 177  &lt;none&gt;  	5000/TCP       	1dsvc/kubernetes     172. 30. 0. 1  	&lt;none&gt;  	443/TCP,53/UDP,53/TCP 	1dsvc/registry-console  172. 30. 212. 204  &lt;none&gt;  	9000/TCP       	1dsvc/router	     172. 30. 96. 77 	&lt;none&gt;  	80/TCP,443/TCP,1936/TCP  1dNAME             READY 	STATUS	RESTARTS  AGEpo/docker-registry-1-410r0  1/1  	Running  2     1dpo/registry-console-1-knq88  1/1  	Running  1     1dpo/router-1-6n4r7       1/1  	Running  1     1dpo/router-1-k65vw       1/1  	Running  2     1dStep 13: Verify basic pod status: Verify your metric pods are running without any errors. [root@master ~]# oc get pods -n openshift-infraStep 14: Verify Web Console: Log on to your OpenShift Origin cluster via the browser using your master node as the URL with port 8443 - https://master. dev. enron. com:8443 Note : Make sure you have added firewall rules to allow traffic using 8443. (Both inside OS as well as on your infra like GCP/AWS or Openstack) Restart or Check Status of openshift service: [root@ocp-master02 ~]# systemctl status origin-master. service[root@ocp-master02 ~]# systemctl restart origin-master. serviceSome more configs - Configuring Host Variables: To assign environment variables to hosts during the Ansible installation, indicate the desired variables in the /etc/ansible/hosts file after the host entry in the [masters] or [nodes] sections. For example: [masters]ec2-52-6-179-239. compute-1. amazonaws. com openshift_public_hostname=ose3-master. public. example. com   openshift_hostname - This variable overrides the internal cluster host name for the system. Use this when the system’s default IP address does not resolve to the system host name.   openshift_hostname=ose3-master. example. com     openshift_public_hostname - This variable overrides the system’s public host name. Use this for cloud installations, or for hosts on networks using a network address translation (NAT).   openshift_public_hostname=ose3-master. public. example. com     openshift_ip - This variable overrides the cluster internal IP address for the system. Use this when using an interface that is not configured with the default route. This variable can also be used for etcd.     openshift_public_ip - This variable overrides the system’s public IP address. Use this for cloud installations, or for hosts on networks using a network address translation (NAT).     openshift_master_cluster_hostname - This variable overrides the host name for the cluster, which defaults to the host name of the master.   openshift_master_cluster_hostname=openshift-internal. example. com     openshift_master_cluster_public_hostname - This variable overrides the public host name for the cluster, which defaults to the host name of the master.   openshift_master_cluster_public_hostname=openshift-cluster. example. com  Refer : Advanced Installation Verify PublicURL: [root@ocp-master02 ~]# grep PublicURL /etc/origin/master/master-config. yaml masterPublicURL: https://35. 227. 22. 165:8443masterPublicURL: https://35. 227. 22. 165:8443 assetPublicURL: https://35. 227. 22. 165:8443/console/ masterPublicURL: https://35. 227. 22. 165:8443Uninstalling OCP Cluster: If you need to uninstall the cluster and start over. Run the following command. ansible-playbook -i . /inventory/byo/hosts playbooks/adhoc/uninstall. ymlDeleting all your pods and services in your project in order to start over. oc delete all --all -n test01AppendixReferences:  https://www. server-world. info/en/note?os=CentOS_7&amp;p=OpenShift37 https://www. projectatomic. io/blog/2016/12/part1-install-origin-on-f25-atomic-host/ https://gauvain. pocentek. net/openshift-single-node. htmldebug command: curl -L -k https://&lt;your ip&gt;:8443/consoles "
    }, {
    "id": 69,
    "url": "http://localhost:4000/openshift-crc",
    "title": "CodeReady Containers - CRC",
    "body": " Installing CRC on GCP (WIP)     Create a GCP Instance with Nested Virtualization    Installing CodeReady Containers     Download CRC Package   Required software packages   Enable sudo for user   Setup Cluster   Access your Cluster   Clean up    Troubleshooting References:Red Hat CodeReady Containers Installing CRC on GCP (WIP): Create a GCP Instance with Nested Virtualization: # create the instance with nested virtualization# add other parameters as neededgcloud compute instances create VM_NAME \ --enable-nested-virtualization \ --zone=ZONE \ --min-cpu-platform= Intel Haswell  # login to VMgcloud compute ssh VM_NAME# check nested virtualizationgrep -cw vmx /proc/cpuinfoInstalling CodeReady Containers: Download CRC Package: Refer : Install OpenShift on a laptop with CodeReady Containers  Visit https://www. openshift. com/try Choose install on Laptop -&gt; https://cloud. redhat. com/openshift/install/crc/installer-provisioned Download for your OS choise (Windows10, MacOS, Linux) Move package to your machine folder   extract the package tar -xf FILENAME. tar. xz   Download and keep the pull secret from same location. You need that later during crc startRequired software packages: CodeReady Containers requires the libvirt and NetworkManager packages. yum install NetworkManagerwget https://mirror. openshift. com/pub/openshift-v4/clients/crc/latest/crc-linux-amd64. tar. xzEnable sudo for user: [devops@node1 ~]$ sudo cat /etc/sudoers. d/devops[sudo] password for devops:devops ALL=(ALL) NOPASSWD: ALLSetup Cluster: $ cd crc-linux-1. 11. 0-amd64$ export PATH=/home/devops/crc-linux-1. 11. 0-amd64:$PATH## set up your host operating system for the CodeReady Containers virtual machine. ## Use normal user account$ crc setup# Configure CRCcrc config set cpus 4crc config set memory ## Start cluster## if you are on a terminal without GUI, you will have difficulty to copy/paste pull secret content. in that you can mention the pull secret ful$ crc start -p /path-to/pull-secretAccess your Cluster: $ eval $(crc oc-env)$ oc login -u developer -p developer## see credentials$ crc console --credentialsTo login as a regular user, run 'oc login -u developer -p developer https://api. crc. testing:6443'. To login as an admin, run 'oc login -u kubeadmin -p 8rynV-SeYLc-h8Ij7-YPYcz https://api. crc. testing:6443'## Access Console$ crc consoleOpening the OpenShift Web Console in the default browser. . . Clean up: $ crc cleanupTroubleshooting:    Issue : After crc start and crc console, oc login fails with Internal error occurred: unexpected response: 503 for a while #740     Issue : Unable to connect to console  $ crc ip$ nmcli conn showReferences::  CodeRead Containers - Red Hat OpenShift 4 on your laptop Install OpenShift Container Platform 4 CodeReady Containers on Ubuntu Trying out CRC - Jeff RED HAT CODEREADY CONTAINERS Accessing CodeReady Containers on a Remote Server"
    }, {
    "id": 70,
    "url": "http://localhost:4000/openshift-installation",
    "title": "OpenShift Installation Methods - Examples",
    "body": " Installing an OKD 4. x Cluster Install OpenShift 4. x Method I - Setup an OpenShift All-In-One     Install required packages   Disable Firewall   Installing OpenShift CLI         Method 1 - Standard CentOS repositories     Method 2 - Download and extract openshift origin             Add the directory you untarred the release into to your path:                     Configure the Docker daemon with an insecure registry parameter of 172. 30. 0. 0/16         Restart docker service          Initiate cluster         Ref:     Add a user           Method II - Setup minishift     Setup Virtual Environment   Install minishift   Start minishift cluster    Method III - OpenShift 4 - OKD - All in One Quick Cluster Method IV - OpenShift Full Cluster CodeReady Containers - CRC (OpenShift 4. x)     Download CRC Package   Required software packages         Enable sudo for user          Setup Cluster   Access your Cluster   Troubleshooting    OpenSHift 4. x OpenShift 4. 2 Installation Red Hat OpenShift 4. x Installation (Evaluation)     Baremetal Installation    OpenShift on Baremetal (UPI) OpenShift All-In-One - OKD Using Ansible Extras OpenSHift 4. x     OpenShift 4. 2 Installation   OpenShift 4. 1 Installation   Baremetal Installation    Install OpenShift 4. x Cluster on VMWare     Install Pre-Req   Download VMWare root CA certificates to System trust   Install Terraform   Add VMWare Credential   Configure DNS   Create install-config. yaml   Init Cluster   Monitoring OpenShift Installations   Deleting a Cluster    baremetal Deploy OpenShift Using OpenShift Installer (AWS)     Using OpenShift Installer   Setup Bastion Host         Configure Bastion VM to Run OpenShift Installer     Install OpenShift Container Platform     multi-step installation.      Clean Up Cluster     Validate Cluster           OpenShift Installation on Red Hat Virtualization (RHV)/oVirt     Requirements    Troubleshooting OpenShift Installation Installing Red Hat Advanced Cluster Management (ACM) for Kubernetes     Setup environment for the ACM Installation   Create a new OpenShift Project/Namespace for ACM   Create an image-pull secret   Install ACM and subscribe to the ACM Operator group   Install ACM and subscribe using the OpenShift web console   Create the MultiClusterHub resource   Verify the ACM installation    References  Installing an OKD 4. x Cluster ReferenceInstalling an OKD 4. 5 Cluster Install OpenShift 4. xHow to install OpenShift 4 on Bare Metal - (UPI) (Video) Method I - Setup an OpenShift All-In-OneInstall required packages: yum install ansible docker wget -ysystemctl enable dockersystemctl start dockerDisable Firewall: Or you have to open required ports. systemctl disable firewalldsystemctl stop firewalldInstalling OpenShift CLI: Method 1 - Standard CentOS repositories: yum -y install centos-release-openshift-origin39yum -y install origin-clientsMethod 2 - Download and extract openshift origin: Create a directory for data (anywhere) mkdir /datacd /dataCheck the latest version if you need. wget  https://github. com/openshift/origin/releases/download/v3. 11. 0/openshift-origin-client-tools-v3. 11. 0-0cbc58b-linux-64bit. tar. gz tar -xzvf openshift-origin-client-tools-v3. 11. 0-0cbc58b-linux-64bit. tar. gz cd openshift-origin-client-tools-v3. 11. 0-0cbc58b-linux-64bitAdd the directory you untarred the release into to your path:: export PATH=$PATH:/data/openshift-origin-client-tools-v3. 11. 0-0cbc58b-linux-64bit/# orexport PATH=$PATH:`pwd`Configure the Docker daemon with an insecure registry parameter of 172. 30. 0. 0/16: cat &gt; /etc/docker/daemon. json &lt;&lt;DELIM{   insecure-registries : [    172. 30. 0. 0/16   ]}DELIMRestart docker service: service docker restartInitiate cluster: oc cluster up --base-dir= /data/clusterup  --public-hostname=&lt;IP&gt; --base-dir=BASE_DIR : Directory on Docker host for cluster up configuration(oc cluster up –public-hostname=35. 239. 51. 76 –routing-suffix=35. 239. 51. 76. xip. io)openshift. local. clusterup/openshift-controller-manager/openshift-master. kubeconfig Ref::  https://github. com/openshift/origin/blob/release-3. 11/docs/cluster_up_down. md https://medium. com/@fabiojose/working-with-oc-cluster-up-a052339ea219Add a user: # oc create user redhatuser. user. openshift. io/redhat created# oc adm policy add-cluster-role-to-user cluster-admin redhatcluster role  cluster-admin  added:  redhat Method II - Setup minishiftSetup Virtual Environment:  setup KVM or virtualbox (or other virtulization)Ref: Set up your virtualization environmentInstall minishift:  Download and manually install minishift.  On mac brew cask install minishift (in case of issue to install, try export HOMEBREW_NO_ENV_FILTERING=1)Start minishift cluster: minishift start --vm-driver virtualbox## setup VirtualBox Permanentlyminishift config set vm-driver virtualboxOnce started, access the console using url or open in browser minishift consoleSetup oc access $ minishift oc-envexport PATH= /home/john/. minishift/cache/oc/v1. 5. 0:$PATH ## Run this command to configure your shell:# eval $(minishift oc-env)Method III - OpenShift 4 - OKD - All in One Quick Clusterhttps://github. com/openshift/okd/releases Method IV - OpenShift Full Cluster Installing OpenShift 4. 1 Using Libvirt and KVMOpenSHift 4. xhttps://github. com/openshift/okd Create clouds. yaml clouds: ocp4-dev:  auth:   auth_url: http://10. 6. 1. 209:35357/   project_name: ocp4-dev   username: ocpadmin   password: ocpadmin  region_name: RegionOneOpenShift 4. 2 Installationhttps://docs. openshift. com/container-platform/4. 2/installing/installing_bare_metal/installing-bare-metal. html Red Hat OpenShift 4. x Installation (Evaluation)https://access. redhat. com/documentation/en-us/openshift_container_platform/4. 1/html/installing/index  Get Evaluation InstallationBaremetal Installation: https://access. redhat. com/documentation/en-us/openshift_container_platform/4. 1/html/installing/installing-on-bare-metalhttps://docs. openshift. com/container-platform/4. 1/installing/installing_bare_metal/installing-bare-metal. htmlhttps://blog. openshift. com/openshift-4-bare-metal-install-quickstart/ OpenShift on Baremetal (UPI) Installing on OpenShift on BaremetalOpenShift All-In-One - OKD Using AnsibleRef:https://github. com/Gepardec/ansible-role-okdhttps://galaxy. ansible. com/gepardec/okdhttps://computingforgeeks. com/setup-openshift-origin-local-cluster-on-centos/ Extras OpenSHift 4. xhttps://github. com/openshift/okd Create clouds. yaml clouds: ocp4-dev:  auth:   auth_url: http://10. 6. 1. 209:35357/   project_name: ocp4-dev   username: ocpadmin   password: ocpadmin  region_name: RegionOneOpenShift 4. 2 Installation: https://docs. openshift. com/container-platform/4. 2/installing/installing_bare_metal/installing-bare-metal. html OpenShift 4. 1 Installation: https://access. redhat. com/documentation/en-us/openshift_container_platform/4. 1/html/installing/index Baremetal Installation: https://access. redhat. com/documentation/en-us/openshift_container_platform/4. 1/html/installing/installing-on-bare-metalhttps://docs. openshift. com/container-platform/4. 1/installing/installing_bare_metal/installing-bare-metal. htmlhttps://blog. openshift. com/openshift-4-bare-metal-install-quickstart/ https://blog. openshift. com/revamped-openshift-all-in-one-aio-for-labs-and-fun/ Install OpenShift 4. x Cluster on VMWarehttps://labs. consol. de/container/platform/openshift/2020/01/31/ocp43-installation-vmware. html Install Pre-Req: $ sudo yum install wget git vimDownload VMWare root CA certificates to System trust: $ wget https://vcenter. lab. local/certs/download. zip$ unzip download. zip$ sudo cp certs/lin/* /etc/pki/ca-trust/source/anchors/$ sudo update-ca-trust extractRef : Adding vCenter root CA certificates to your system trust Install Terraform: $ sudo yum install -y yum-utils$ sudo yum-config-manager --add-repo https://rpm. releases. hashicorp. com/RHEL/hashicorp. repo$ sudo yum -y install terraformRefer Terraform Installation Methods Add VMWare Credential: $ cat . config/ocp/vsphere. yamlvsphere-user: administrator@vsphere. lanvsphere-password:  123! vsphere-server: 192. 168. 1. 100vsphere-dc: DC1vsphere-cluster: AZ1Configure DNS: https://blog. ktz. me/configure-unbound-dns-for-openshift-4/ Create install-config. yaml: apiVersion: v1baseDomain: lab. localcompute:- hyperthreading: Enabled name: worker replicas: 0controlPlane: hyperthreading: Enabled name: master replicas: 3metadata: name: ocp4platform: vsphere:  vcenter: 10. 6. 1. 198  username: administrator@vcenter. lab. local  password: supersecretpassword  datacenter: DC1  defaultDatastore: AZ1fips: false pullSecret: 'YOUR_PULL_SECRET'sshKey: 'YOUR_SSH_PUBKEY'Init Cluster: ## Creating Installer Configuration File$ . /openshift-install create install-config --dir=ocp46 --log-level=debug## Generating Kubernetes Manifests$ . /openshift-install create manifests --dir=ocp46 --log-level=debug## Make master nodes schedulable## edit ocp46/manifests/cluster-scheduler-02-config. yml and make $ cat manifests/cluster-scheduler-02-config. ymlapiVersion: config. openshift. io/v1kind: Schedulermetadata: creationTimestamp: null name: clusterspec: mastersSchedulable: true policy:  name:   status: {}## Generating Ignition Configuration Files$ . /openshift-install create ignition-configs --dir=ocp46 --log-level=debug## Create the cluster$ . /openshift-install create cluster --dir=ocp46 --log-level=debug## NOTE; In a pre-existing infrastructure installation, you cannot use the openshift-install ## create cluster command to deploy the cluster because you have already installed the cluster nodes. ## The bootstrap node installation triggers the cluster installation, so execute the following command ## sequence to monitor the cluster installation:[user@demo ~]$ openshift-install wait-for bootstrap-complete --dir=ocp46 --log-level=debug[user@demo ~]$ openshift-install wait-for install-complete --dir=ocp46 --log-level=debug## openshift-install wait-for commands do not trigger the cluster installation. ## It is a recommended practice to use them to monitor the cluster installation. Monitoring OpenShift Installations: $ export KUBECONFIG=ocp46/auth/kubeconfigDeleting a Cluster: ## destroy a cluster$ . /openshift-install destroy cluster --dir=$HOME/ocp46-clusterbaremetalhttps://docs. openshift. com/container-platform/4. 3/installing/installing_bare_metal/installing-bare-metal. html#cluster-entitlements_installing-bare-metal Deploy OpenShift Using OpenShift Installer (AWS)(27 Jan 2021)  OpenShift InstallerInstaller configures entire cloud infrastructure:  VMs Load balancers Storage Networking Other resourcesUsing OpenShift Installer: openshift-install create cluster --dir=$HOME/myclusterInstaller prompts for  SSH public key Platform: aws Region: Default in AWS is us-east-1 Base domain: Public route 53 domain, needs to exist prior to installation Cluster name: Must be unique within AWS account Pull secret: From Get Started with OpenShift as single-line JSONSetup Bastion Host:  Create a bastion node and login to the host. $ ssh user@bastion-host$ echo $GUIDd0ceConfigure Bastion VM to Run OpenShift Installer: Install AWS CLI # Download the latest AWS Command Line Interfacecurl  https://s3. amazonaws. com/aws-cli/awscli-bundle. zip  -o  awscli-bundle. zip unzip awscli-bundle. zip# Install the AWS CLI into /bin/aws. /awscli-bundle/install -i /usr/local/aws -b /bin/aws# Validate that the AWS CLI worksaws --version# Clean up downloaded filesrm -rf /root/awscli-bundle /root/awscli-bundle. zipDownload OpenShift Installation Binary OCP_VERSION=4. 6. 4wget https://mirror. openshift. com/pub/openshift-v4/clients/ocp/${OCP_VERSION}/openshift-install-linux-${OCP_VERSION}. tar. gztar zxvf openshift-install-linux-${OCP_VERSION}. tar. gz -C /usr/binrm -f openshift-install-linux-${OCP_VERSION}. tar. gz /usr/bin/README. mdchmod +x /usr/bin/openshift-installDownload and Install OC CLI wget https://mirror. openshift. com/pub/openshift-v4/clients/ocp/${OCP_VERSION}/openshift-client-linux-${OCP_VERSION}. tar. gztar zxvf openshift-client-linux-${OCP_VERSION}. tar. gz -C /usr/binrm -f openshift-client-linux-${OCP_VERSION}. tar. gz /usr/bin/README. mdchmod +x /usr/bin/oc Check that the OpenShift installer and CLI are in /usr/bin:ls -l /usr/bin/{oc,openshift-install}# Setup auto-completionoc completion bash &gt;/etc/bash_completion. d/openshift logout from rootConfigure AWS CLI Credential export AWSKEY=&lt;YOURACCESSKEY&gt;export AWSSECRETKEY=&lt;YOURSECRETKEY&gt;export REGION=us-east-2mkdir $HOME/. awscat &lt;&lt; EOF &gt;&gt; $HOME/. aws/credentials[default]aws_access_key_id = ${AWSKEY}aws_secret_access_key = ${AWSSECRETKEY}region = $REGIONEOF# test AWS Accessaws sts get-caller-identity Open https://www. openshift. com/try and select “Try it in the Cloud”, Select AWS Choose “Installer-Provisioned-Infrastructure”   Copy the Pull Secret (save to file for copy)   Create an ssh key pairssh-keygen -f ~/. ssh/cluster-${GUID}-key -N ''Install OpenShift Container Platform:  Installer will generate Ignition configs for the bootstrap, master, and worker machines.  The process for bootstrapping a cluster looks like the following:     The bootstrap machine boots and starts hosting the remote resources required for the master machines to boot.    The master machines fetch the remote resources from the bootstrap machine and finish booting.    The master machines use the bootstrap node to form an etcd cluster.    The bootstrap node starts a temporary Kubernetes control plane using the newly created etcd cluster.    The temporary control plane schedules the production control plane to the master machines.    The temporary control plane shuts down, yielding to the production control plane.    The bootstrap node injects OpenShift-specific components into the newly formed control plane.    The installer then tears down the bootstrap node.     The result of this bootstrapping process is a fully running OpenShift cluster. The cluster will then download and configure the remaining components needed for day-to-day operation, including the creation of worker machines on supported platforms. Run OpenShift Installer Run the OpenShift installer and answer the prompts:  Select your Public Key (which you have created earlier) Select aws as the Platform.  Select any Region near you.  Select cluster. yourdomain. com as the Base Domain.  For the Cluster Name, type cluster-101 (or any other name) When prompted, paste the contents of your Pull Secret in JSON format. Do not include any spaces or white characters and make sure it is in one line$ openshift-install create cluster --dir $HOME/cluster-101# Sample answers? SSH Public Key /home/user/. ssh/cluster-101-key. pub? Platform awsINFO Credentials loaded from the  default  profile in file  /home/user/. aws/credentials ? Region us-east-2 (Ohio)? Base Domain cluster. yourdomain. com? Cluster Name cluster-101? Pull Secret [? for help] ***************************************************************************************************************************************************************# wait for installer to finish*********************************************INFO Creating infrastructure resources. . . INFO Waiting up to 20m0s for the Kubernetes API at https://api. cluster-d0ce. d0ce. sandbox1072. opentlc. com:6443. . . INFO API v1. 19. 0+9f84db3 upINFO Waiting up to 30m0s for bootstrapping to complete. . . INFO Destroying the bootstrap resources. . .     INFO Waiting up to 40m0s for the cluster at https://api. cluster-d0ce. d0ce. sandbox1072. opentlc. com:6443 to initialize. . . INFO Waiting up to 10m0s for the openshift-console route to be created. . . INFO Install complete!INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/gineesh. madapparambath-fujitsu. c/cluster-d0ce/auth/kubeconfig'INFO Access the OpenShift web-console here: https://console-openshift-console. apps. cluster-d0ce. d0ce. sandbox1072. opentlc. comINFO Login to the console with user:  kubeadmin , and password:  6TVYn-33gLY-7ZjrA-eqRQE  INFO Time elapsed: 37m56s Make note of the following items from the output of the install command:     The location of the kubeconfig file, which is required for setting the KUBECONFIG environment variable and, as suggested, sets the OpenShift user ID to system:admin.    The kubeadmin user ID and associated password (GEveR-tBVTB-jJUJB-iC9Jn in the example).    The password for the kubeadmin user is also written into the auth/kubeadmin-password file.    The URL of the web console - (https://console-openshift-console. apps. cluster-. sandbox. opentlc. com in the example) and the credentials (again) to log into the web console.     Refer ${HOME}/cluster-101/. openshift_install. log for logs and troubleshooting. multi-step installation. : # Create the installation configuration: openshift-install create install-config --dir $HOME/cluster-${GUID}. # Update the generated install-config. yaml file—for example, change the AWS EC2 instance types. # Create the YAML manifest templates: openshift-install create manifests --dir $HOME/cluster-${GUID}# Changing the manifest templates is unsupported. # Create the YAML manifests:openshift-install create manifests --dir $HOME/cluster-${GUID}# Changing the manifests is unsupported. # Create the Ignition configuration files: openshift-install create ignition-configs --dir $HOME/cluster-${GUID}# Changing the Ignition configuration files is unsupported. # Install the cluster: openshift-install create cluster --dir $HOME/cluster-${GUID}. # To delete the cluster, use: openshift-install destroy cluster --dir $HOME/cluster-${GUID}. Clean Up Cluster: openshift-install destroy cluster --dir $HOME/cluster-${GUID}# Delete all of the files created by the OpenShift installer:rm -rf $HOME/. kuberm -rf $HOME/cluster-${GUID}Validate Cluster: # Setup CLIexport KUBECONFIG=$HOME/cluster-${GUID}/auth/kubeconfigecho  export KUBECONFIG=$HOME/cluster-${GUID}/auth/kubeconfig  &gt;&gt;$HOME/. bashrc$ oc whoamisystem:admin# get console and login with kubeadmin &amp; password from installation log$ oc whoami --show-consolehttps://console-openshift-console. apps. cluster-d0ce. d0ce. sandbox1072. opentlc. comOpenShift Installation on Red Hat Virtualization (RHV)/oVirtRefer same steps in VMWare Setup  Reference Installing a cluster quickly on RHV Installing a cluster on RHV with customizations Support Matrix for OpenShift Container Platform IPI Installation on Red Hat VirtualizationRequirements: DNS  API DNS - eg: api. ocp46. ocp4. lab. local Apps Wildcard - eg: *. apps. ocp46. ocp4. lab. localTroubleshooting OpenShift Installation Troubleshooting OpenShift Installation Troubleshooting installations Investigating pod issuesInstalling Red Hat Advanced Cluster Management (ACM) for KubernetesSetup environment for the ACM Installation:  Setup OpenShift cluster and verify$ oc get machinesets -n openshift-machine-api## you need to use larger instance but for demo we will skip this partCreate a new OpenShift Project/Namespace for ACM: $ oc new-project open-cluster-managementCreate an image-pull secret: $ oc create secret docker-registry YOUR_SECRET_NAME \ --docker-server=registry. access. redhat. com/rhacm1-tech-preview \ --docker-username=YOUR_REDHAT_USERNAME \ --docker-password=YOUR_REDHAT_PASSWORD$ oc create secret docker-registry image-pull-secret --docker-server=registry. access. redhat. com/rhacm1-tech-preview --docker-username=gineesh. madapparambath@fujitsu. com--docker-password='r@Dh#T#2019'Install ACM and subscribe to the ACM Operator group: ## create OperatorGroup - acm-operator. yamlapiVersion: operators. coreos. com/v1kind: OperatorGroupmetadata: name: acm-operatorspec: targetNamespaces: - open-cluster-management## create it$ oc apply -f acm-operator. yaml## Create ACM Subscription - acm-subscription. yamlapiVersion: operators. coreos. com/v1alpha1kind: Subscriptionmetadata: name: acm-operator-subscriptionspec: sourceNamespace: openshift-marketplace source: redhat-operators channel: release-1. 0 installPlanApproval: Automatic name: advanced-cluster-management## create it$ oc apply -f acm-subscription. yamlInstall ACM and subscribe using the OpenShift web console:  Open OpenShift Web Console OperatorHub -&gt; Search advanced cluster -&gt; Find Advanced Cluster Management for Kubernetes Select Project, Version and do install Wait for Operator Installation to be completedCreate the MultiClusterHub resource: ## Create the MultiClusterHub from the CLI## create the fileapiVersion: operators. open-cluster-management. io/v1beta1kind: MultiClusterHubmetadata: name: multiclusterhub namespace: open-cluster-managementspec: imagePullSecret: YOUR_SECRET_NAME## create it$ oc apply -f multicluster-acm. yamlFrom Console-&gt; Select installed operator-&gt; Select Advanced Cluster Management for Kubernetes-&gt; Goto MultiClusterHub -&gt; Create new Verify the ACM installation: -&gt; Check events in Advanced Cluster Management for Kubernetes-&gt; Check Route and Access it References  Installing Red Hat Advanced Cluster Management (ACM) for KubernetesReferences Installing a cluster quickly on AWS OpenShift Installer overview on GitHub"
    }, {
    "id": 71,
    "url": "http://localhost:4000/openshift-jenkins",
    "title": "Running Jenkins Pipelines on OpenShift",
    "body": " JenkinsPipelineStrategy - OpenShift provides this build type.  Entire build lifecycle can be monitored and managed from OpenShift.    Define pipelines and workflows in Jenkinsfile or a git repository   1. Deploy Jenkins in OpenShift Configure App Project create project for app de1. Deploy Jenkins in OpenShift: ## Check available jenkins template$ oc get templates -n openshift | grep jenkins## Create a new project $ oc new-project jenkins-project## Deploy jenkins and wait for the deployment up and running$ oc new-app --as-deployment-config \&gt; jenkins-ephemeral -p MEMORY_LIMIT=2048Mi$ oc get pod## find the jenkins app url$ oc get routeLogin with OpenShift credentials and Authorize Access. ## Configure App Project```bash## create project for app$ oc new-project cicd-app-demo## de"
    }, {
    "id": 72,
    "url": "http://localhost:4000/openshift-kubernetes-questions",
    "title": "OpenShift & Kubernetes - Questions and References",
    "body": "Note: These are questions I have received via chat groups and communities. This is a living document and I will update the page whenever there is a new question or better answer or references.  What are the features of OpenShift ? How to set number of pods per node in OpenShift ? What is a Distributed System ? TODO/ What is ODF ? TODO/ What is ServiceMesh ? What is Kiali ? What is Jeager ? TODO/ What is Kubernetes Admission Controller ? How Container Security Works ? What is --pod-eviction-timeout in Kubernetes ? TODO/ What is Red Hat Nooba ? What is Air-Gapped (disconnected) OpenShift Clusters What is new in Red Hat OpenShift 4 ? What is Quay ? TODO/ What is CoreOS and Why using CoreOS for OpenShift ? What is CRI-O ? TODO/ What is etcd ? OpenShift Hardering - What is Compliance Operator ? TODO/ What is Kubernetes Operator (or OpenShift Operator) ? What is Operator Framework ? What Red Hat Advanced Cluster Security for Kubernetes (RHACS) ? What are the components of OpenShift Cluster Monitoring, Logging and Telemetry ? What is cadvisor in kubernetes ? TODO/ What is Helm ? What are the points to note while architecting OpenShift Clusters ? How to Manage Roles and Permissons in OpenShift ? What is SRE and DevOps ? TODO/ How to Enabled OpenShift Node AutoScaling ? What is KeyStone ? Removing the kubeadmin user What is cgroup ? (CONTROL GROUPS) What is Multus Container Network Interface (CNI) ? What is blue green deployment? How to Size an OpenShift Cluster ? ReferenceWhat are the features of OpenShift ?:  High Availabilty Lightweight Operating System Load Balancing Automated Scaling Logging and Monitoring (based on prometheus, ElasticSearch) Service Discovery Storage Application Management Cluster ExtensibilityHow to set number of pods per node in OpenShift ?:  How to set number of pods per node in OpenShift ? - Quick Guide OpenShift Scale: Running 500 Pods Per Node Managing the maximum number of pods per nodeWhat is a Distributed System ?: A distributed system in its most simplest definition is a group of computers working together as to appear as a single computer to the end-user. These machines have a shared state, operate concurrently and can fail independently without affecting the whole system’s uptime. Read more : A Thorough Introduction to Distributed Systems  Fault Tolerance / High Availability Low LatencyTODO/ What is ODF ?: TODO/ What is ServiceMesh ?:  Installing Red Hat OpenShift Service Mesh     Install Elastic Operator   Install Jeager Operator   Install Kiali Operator   Install Red Hat OpenShift Service Mesh (OSSM) Operator   What is Kiali ?: Kiali is a management console for an Istio-based service mesh. It provides dashboards, observability, and lets you operate your mesh with robust configuration and validation capabilities. It shows the structure of your service mesh by inferring traffic topology and displays the health of your mesh.  Website / GitHubWhat is Jeager ?: Distributed Tracing System - Monitor and troubleshoot transactions in complex distributed systems  [website][https://www. jaegertracing. io]TODO/ What is Kubernetes Admission Controller ?:    MutatingAdmissionWebhook and ValidatingAdmissionWebhook     Using Admission ControllersUsing Admission Controllers  How Container Security Works ?:  Secure the container host Secure the networking environment - intrusion prevention system (IPS) and web filtering for traffic moving from north to south, and to and from the internet, in order to stop attacks and filter malicious content.  Secure your management stack - container registry, Kubernetes installation, network policies.  Build on a secure foundation - patching, updates Secure your build pipeline   Secure your application   Container Security in Six StepsWhat is --pod-eviction-timeout in Kubernetes ?: The grace period for deleting pods on failed nodes. - The default eviction timeout duration is five minutes. In some cases when the node is unreachable, the API server is unable to communicate with the kubelet on the node. The decision to delete the pods cannot be communicated to the kubelet until communication with the API server is re-established. TODO/ What is Red Hat Nooba ?: What is Air-Gapped (disconnected) OpenShift Clusters: Air-gapped environments are those that are physically isolated from other networks, but most importantly isolated from the Internet. No proxies, no jump hosts - nothing.  Is your Operator Air-Gap Friendly? OpenShift 4 in an Air Gap (disconnected) environment- Part1, Part2What is new in Red Hat OpenShift 4 ?:  Immutable Red Hat Enterprise Linux CoreOS Fullstack Automation OpenShift service mesh Operator framework Knative framework Managing Multiple Clusters Across Multiple CloudsRead : Introducing Red Hat OpenShift 4: Kubernetes for the Enterprise    What is Clair ?:  Clair is an open source project for the static analysis of vulnerabilities in application containers (currently including OCI and docker).  Clair GitHubWhat is Quay ?: Quay is a container image registry that enables you to build, organize, distribute, and deploy containers. This is the Community Distribution of Quay that powers Red Hat Quay and Quay. io  [ProjectQuay][https://www. projectquay. io/]TODO/ What is CoreOS and Why using CoreOS for OpenShift ?: CoreOS was founded in 2013 with the mission to improve the security and reliability of the internet. On May 26, 2020, CoreOS Container Linux reached its end of life and will no longer receive updates.  It’s the successor to both Fedora Atomic Host and CoreOS Container Linux Based on RHEL Controlled immutability CRI-O container runtime Set of container tools: podman, skopeo, buildah, crictlReferences  What was CoreOS Getting Started with CoreOS Producing an Ignition ConfigWhat is CRI-O ?: LIGHTWEIGHT CONTAINER RUNTIME FOR KUBERNETES CRI-O is an implementation of the Kubernetes CRI (Container Runtime Interface) to enable using OCI (Open Container Initiative) compatible runtimes. It is a lightweight alternative to using Docker as the runtime for kubernetes. It allows Kubernetes to use any OCI-compliant runtime as the container runtime for running pods.  cri - container runtime interface came in picture as new container engines supports included in kubernetes.  cni - container network interface is for networking support   csi - container storage interface is to support storage drivers and solutions   Website   TODO/ What is etcd ?:  etcd is a distributed key-value store. OpenShift Hardering - What is Compliance Operator ?: The Compliance Operator lets OpenShift Container Platform administrators describe the desired compliance state of a cluster and provides them with an overview of gaps and ways to remediate them.  Understanding the Compliance Operator How does Compliance Operator work for OpenShift? Part 1, Part 2 Compliance Operartor - GitHub Understanding the Compliance Operator Also read - kube-bench is a Go application that checks whether Kubernetes is deployed securely by running the checks documented in the CIS Kubernetes Benchmark. TODO/ What is Kubernetes Operator (or OpenShift Operator) ?: Use the Kubernetes API to create, configure, and automatically manage applications. Operator have 2 components;  CRD - Custom Resource Definition Controller - which controls the application deployment.  Use Helm/Ansible/Go OLM stands for Operator Lifecyle Management What is an Operator - OpenShift. com Troubleshooting OpenShift Operator issues Building Kubernetes Operators with the Operator Framework and Ansible  What is Operator Framework ?:  5 Levels of Operator Framework  Basic Install Seamless Upgrade Full Lifecycle Deep Insight Auto PilotOperator Components  Operator SDK - Provides tooling to build and package operators Operator Lifecyle Management (OLM) - Governs operator scope and lifecycle Metering - historical information to be gathered and reported on OperatorHub - marketplace for community operatorsWhat Red Hat Advanced Cluster Security for Kubernetes (RHACS) ?: Red Hat® Advanced Cluster Security for Kubernetes, powered by StackRox technology, protects your vital applications across build, deploy, and runtime.  Control the trusted sources of content Defend applications from attachs and vulnerabilities Extend secure services through standard interface and API’sAlso Read;  Red Hat Advanced Cluster Security for Kubernetes Red Hat OpenShift container security OpenShift Security Best Practices for Kubernetes Cluster DesignWhat are the components of OpenShift Cluster Monitoring, Logging and Telemetry ?:  collection - This is the component that collects logs from the cluster, formats them, and forwards them to the log store. The current implementation is Fluentd.  log store - This is where the logs are stored. The default implementation is Elasticsearch. You can use the default Elasticsearch log store or forward logs to external log stores. The default log store is optimized and tested for short-term storage.    visualization - This is the UI component you can use to view logs, graphs, charts, and so forth. The current implementation is Kibana.   Installing OpenShift LoggingTelemetry Sends a carefully chosen subset of the cluster monitoring metrics to Red Hat.  openshift/telemeterWhat is cadvisor in kubernetes ?: cAdvisor is an open-source agent integrated into the kubelet binary that monitors resource usage and analyzes the performance of containers. It collects statistics about the CPU, memory, file, and network usage for all containers running on a given node (it does not operate at the pod level)  Tools for Monitoring Resources Native Kubernetes Monitoring, Part 1: Monitoring and Metrics for Users MONITORING DOCKER CONTAINER METRICS USING CADVISORTODO/ What is Helm ?: What are the points to note while architecting OpenShift Clusters ?:  Enterprise OpenShift Deployment: What do you need to know?How to Manage Roles and Permissons in OpenShift ?:  Using RBAC to define and apply permissionsWhat is SRE and DevOps ?:  What is SRE (site reliability engineering)? DevOps vs. SRETODO/ How to Enabled OpenShift Node AutoScaling ?:  Applying autoscaling to an OpenShift Container Platform clusterWhat is KeyStone ?: Keystone is an OpenStack project that provides identity, token, catalog, and policy services. You can configure the integration with Keystone so that the new OpenShift Container Platform users are based on either the Keystone user names or unique Keystone IDs.  What is keystone federation?Removing the kubeadmin user: After you define an identity provider and create a new cluster-admin user, you can remove the kubeadmin to improve cluster security. Warning: If you follow this procedure before another user is a cluster-admin, then OpenShift Container Platform must be reinstalled. It is not possible to undo this command. Prerequisites  You must have configured at least one identity provider.  You must have added the cluster-admin role to a user.  You must be logged in as an administrator. -- Remove the kubeadmin secrets:$ oc delete secrets kubeadmin -n kube-systemWhat is cgroup ? (CONTROL GROUPS): cgroups is a Linux kernel feature that limits, accounts for, and isolates the resource usage of a collection of processes.  Understanding resource limits in kubernetes: memory NTRODUCTION TO CONTROL GROUPS (CGROUPS) Linux Control Groups and Process IsolationWhat is Multus Container Network Interface (CNI) ?: Multus CNI enables attaching multiple network interfaces to pods in Kubernetes.  k8snetworkplumbingwg/multus-cni Using the Multus CNI in OpenShiftWhat is blue green deployment?: Blue green deployment is an application release model that gradually transfers user traffic from a previous version of an app or microservice to a nearly identical new release—both of which are running in production.  What is blue green deployment?How to Size an OpenShift Cluster ?:  OpenShift sizing and subscription guide for enterprise Kubernetes Everything about Infra nodes - Video  Reference:  "
    }, {
    "id": 73,
    "url": "http://localhost:4000/openshift",
    "title": "OpenShift - Tools, Learning Guides and References",
    "body": "These are collection of reference documents and blog posts from experts around.  OpenShift Tools &amp; Terminologies     OpenShift odo   kube-burner    OpenShift Articles Azure Red Hat OpenShift (ARO)     Learn OpenShift   OpenShift Tools &amp; TerminologiesOpenShift odo: odo is a CLI tool for creating applications on OpenShift Container Platform and Kubernetes.  Understanding odo odo: Developers CLI for OpenShift and Kubernetes Installing odo odo CLI Referencekube-burner: Kube-burner is a tool designed to stress different OpenShift components basically by coordinating the creation and deletion of k8s resources.  Introducing kube-burner, A tool to Burn Down Kubernetes and OpenShiftOpenShift Articles Read OpenShift articles in techbeatly. com OpenShift Community of Practice - uncontained. ioAzure Red Hat OpenShift (ARO) Azure Red Hat OpenShift 4 Documentation Tutorial: Create an Azure Red Hat OpenShift 4 cluster   Learn OpenShift:   Ansible Operator in OpenShift"
    }, {
    "id": 74,
    "url": "http://localhost:4000/packer",
    "title": "Packer by Hashicorp",
    "body": "Packer use cases and samples are kept in this GitHub Repo. Photo by cottonbro from Pexels  Introduction Installing Packer Create your first Packer file     Init and Build Packer   Adding provisioner   Variables   Handling Sensitive Data    Packer Parallel Build Packer Post-Processors Packer with AWS     Manage AWS Credentials for Packer   Important: Managing the AMIs created by Packer   Add provisioner to template   Using Variable to Manage AMI Name    Creating Vagrant Boxes from AMIIntroductionPacker is an open source tool for creating identical machine images for multiple platforms from a single source configuration. Installing PackerDownload and install packer. # packer on linuxcurl -fsSL https://apt. releases. hashicorp. com/gpg | sudo apt-key add -sudo apt-add-repository  deb [arch=amd64] https://apt. releases. hashicorp. com $(lsb_release -cs) main sudo apt-get update &amp;&amp; sudo apt-get install packerCreate your first Packer fileSample docker-ubuntu. pkr. hcl packer { required_plugins {  docker = {   version =  &gt;= 0. 0. 7    source =  github. com/hashicorp/docker   } }}source  docker   ubuntu  { image =  ubuntu:xenial  commit = true}build { name  =  learn-packer  sources = [   source. docker. ubuntu  ]}Init and Build Packer: # init packer$ packer init . # format packer files$ packer fmt . # validate packer files$ packer validate . # build$ packer build . # or$ packer build docker-ubuntu. pkr. hclAdding provisioner: build { name =  learn-packer  sources = [   source. docker. ubuntu  ] provisioner  shell  {  environment_vars = [    FOO=hello world ,  ]  inline = [    echo Adding file to Docker Container ,    echo \ FOO is $FOO\  &gt; example. txt ,  ] } provisioner  shell  {  inline = [ echo This provisioner runs last ] }}Variables: variable  docker_image  { type  = string default =  ubuntu:xenial }$ touch example. pkrvars. hcldocker_image =  ubuntu:bionic # build with varible file$ packer build --var-file=example. pkrvars. hcl docker-ubuntu. pkr. hclPacker will automatically load any variable file that matches the name *. auto. pkrvars. hcl, without the need to pass the file via the command line. Build image with command line flag $ packer build --var docker_image=ubuntu:groovy . Handling Sensitive Data: local  secret_key  { key =  ${var. secret_key}  sensitive = true}Packer Parallel BuildAdd multiple sources source  docker   ubuntu  { image = var. docker_image commit = true}source  docker   ubuntu-bionic  { image =  ubuntu:bionic  commit = true}Add multiple sources in build build { name  =  learn-packer  sources = [   source. docker. ubuntu ,   source. docker. ubuntu-bionic , ]. . Packer Post-ProcessorsWhile provisioners are run against an instance while it is running, post-processors run only after Packer saves the instance as an image. eg:  compress artifacts upload artifacts create some filesAdding a docker tag using post-processor Post build { name  =  learn-packer  .  .  post-processor  docker-tag  {  repository =  learn-packer   tags    = [ ubuntu-xenial ,  packer-rocks ]  only    = [ docker. ubuntu ] } post-processor  docker-tag  {  repository =  learn-packer   tags    = [ ubuntu-bionic ,  packer-rocks ]  only    = [ docker. ubuntu-bionic ] }Sequential post-processing steps  Use post-processors instead of post-processor Use the post-processors (note the pluralization) block to create post-processing pipelines where the output of one post-processor becomes the input to another post-processor.  post-processors {  post-processor  docker-import  {   repository =  swampdragons/testpush    tag    =  0. 7   }  post-processor  docker-push  {} }Packer with AWSManage AWS Credentials for Packer: Configure environment variables. $ export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY$ export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEYImportant: Managing the AMIs created by Packer:  Packer only builds images, make sure you cleanup if AMI is not needed anymore Remove the AMI by first deregistering it on the AWS AMI management page.  Remember to delete the associated snapshot on the AWS snapshot management page. Add provisioner to template: Using provisioners allows you to completely automate modifications to your image. You can use,  shell scripts file uploads tools like Chef or PuppetInside the build block,  provisioner  shell  {  environment_vars = [    FOO=hello world ,  ]  inline = [    echo Installing Redis ,    sleep 30 ,    sudo apt-get update ,    sudo apt-get install -y redis-server ,    echo \ FOO is $FOO\  &gt; example. txt ,  ] }Using Variable to Manage AMI Name: Remember to clean up exising AMI or use different AMI name when you create as AMI name should be unique in AWS. Automate the AMI naming using local variable and timestamp. variable  ami_prefix  { type  = string default =  learn-packer-linux-aws-redis }locals { timestamp = regex_replace(timestamp(),  [- TZ:] ,   )}and source  amazon-ebs   ubuntu  {  ami_name   =  ${var. ami_prefix}-${local. timestamp}   ## . . . }Creating Vagrant Boxes from AMIAdd vagrant post-processor build { name  =  learn-packer  sources = [   source. amazon-ebs. ubuntu ,   source. amazon-ebs. ubuntu-focal  ] .  .  provisioner  shell  {  inline = [ echo This provisioner runs last ] } post-processors {  post-processor  vagrant  {}  post-processor  compress  {} }}"
    }, {
    "id": 75,
    "url": "http://localhost:4000/podman",
    "title": "Podman Cheat Sheet",
    "body": "Also see,  OpenShift-Kubernetes-Docker Cheatsheet Skopeo Cheat Sheet for details.  Podman Cheat Sheet for details. Note : This is s a living document and I will update whenever needed $ podman --version          # Check version$ sudo podman login -u USER_NAME REGISTRY_URL                  # Login to Registry$ sudo podman login -u USER_NAME \ -p ${TOKEN} \ REGISTRY_URL                   # Login with token or password                  # eg: in OpenShift, token can retrive as                  # $ TOKEN=$(oc whoami -t)$ podman logout quay. io       # Remove login credentials for registry. redhat. io$ podman logout --all        # Remove login credentials for all registries$ podman search REGISTRY_URL/IMAGE_NAME                  # search for an image in registry$ sudo podman run --name test -u 1234 \ -p 8080:8080 -d s2i-sample-app$ sudo podman run -d --name TEST \ quay. io/USER_NAME/IMAGE_NAME:VERSION                  # Create a container $ podman run --privileged quay. io/podman/stable podman run ubi8 echo hello                  # The easiest way to run Podman inside of a container is to use the --privileged flag. $ sudo podman ps          # List running containers$ sudo podman stop CONTAINER_NAME  # STOP running containers$ sudo podman rm CONTAINER_NAME   # remove running containers# sudo podman rmi IMAGE_NAME    # delete container image$ sudo podman logs CONTAINER_NAME                            # check logs of running container$ sudo podman build -t NAME .    # build container image from Dockerfile and spec$ sudo podman images        # see available imagesUsing Podman inside Container:  How to use Podman inside of a containerSide notes # add DNS, enable and start systemd-resolvedsudo systemctl enable systemd-resolvedsudo systemctl start systemd-resolved## Rootful Podman in rootful Podman with --privilegedpodman run --privileged quay. io/podman/stable podman run ubi8 echo hello## added volumepodman run --privileged -v . /mycontainers:/var/lib/containers quay. io/podman/stable podman run ubi8 echo hello## Rootless Podman in rootful Podman with --privilegedpodman run --user podman --privileged quay. io/podman/stable podman run ubi8 echo hello## Rootful Podman in rootful Podman without --privilegedpodman run --cap-add=sys_admin,mknod --device=/dev/fuse --security-opt label=disable quay. io/podman/stable podman run ubi8-minimal echo helloSample ## Run podman inside podman and check podman version$ podman run --privileged \ quay. io/podman/stable \ podman version## Run podman inside podman and using ubi8 image inside. $ podman run --privileged \ quay. io/podman/stable \ podman run ubi8 echo hello$ podman run -it --privileged \ docker. io/mysticrenji/podman \ podman version$ podman run -it --privileged \ docker. io/mysticrenji/podman \ podman run -d docker. io/library/node:12-alpine$ podman run -it --privileged \ docker. io/mysticrenji/podman \ podman version &amp;&amp; git version uptime\ uptime;\ git version;\ git clone https://github. com/mysticrenji/podman-experiments. git;\ cd podman-experiments;\ podman-compose up -d;\ podman-compose down podman images apiVersion: v1kind: Podmetadata: name: podman-in-podmanspec: # specification of the pod’s contents restartPolicy: Never containers: - name: podipod  image:  docker. io/mysticrenji/podman   command: [ /bin/sh ]  args:   - -c   - &gt;-     podman version &amp;&amp;     git clone https://github. com/mysticrenji/podman-experiments. git &amp;&amp;     cd podman-experiments &amp;&amp;     podman-compose up -d &amp;&amp;     podman-compose down &amp;&amp;     podman images  securityContext:    privileged: truePodman Machine on MacOS: ## Intall podman and qemubrew install podman qemupodman machine initpodman machine startReference:  Replacing Docker with Podman - Power of Podman Podman on MacOS How to use the –privileged flag with container engines How to use Podman inside of Kubernetes"
    }, {
    "id": 76,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 77,
    "url": "http://localhost:4000/redhat-kb",
    "title": "Red Hat Knowledge Base",
    "body": "Reference Documents for Red Hat Products  Red Hat Subscription Management Cheatsheet     Subscription Reference    Repository &amp; Satellite Useful Links AppendixRed Hat Subscription Management Cheatsheet: Refer SUBSCRIPTION-MANAGER COMMAND CHEAT SHEET # subscribesubscription-manager register# subscribe using username and password in line and auto-attach poolsubscription-manager register --username &lt;username&gt; --password &lt;password&gt; --auto-attach# Pull the latest subscription data from the serversubscription-manager refresh# Set a role for your system. subscription-manager role --set= Red Hat Enterprise Linux Server # List available subscriptionssubscription-manager list --available# attach subscription automatically with availalbe poolsubscription-manager attach --auto# Use the pool ID of the subscription to attach the pool to the system. subscription-manager attach --pool=&lt;subscription-pool-id&gt;# check consumed subscrptionssubscription-manager list --consumed# Subscription detailssubscription-manager identitysubscription-manager status# listing repositoriessubscription-manager repos --list# enabling server repossubscription-manager repos \ --enable=rhel-7-server-rpms \ --enable=rhel-server-rhscl-7-2. 9. rpms# enabling ansible reposubscription-manager repos \ --enable ansible-2-for-rhel-8-x86_64-rpms# Unregistering a systemsubscription-manager remove --allsubscription-manager unregistersubscription-manager cleanYUM Download only yum install --downloadonly --downloaddir=&lt;directory&gt; &lt;package&gt;Subscription Reference:  How to register and subscribe a system offline to the Red Hat Customer Portal How to register and subscribe a system to the Red Hat Customer Portal using Red Hat Subscription-Manager Common administrative commands in Red Hat Enterprise Linux 5, 6, 7, and 8 (Subscription Details, RHN ID, UUID)Repository &amp; Satellite:    How to create a local mirror of the latest update for Red Hat Enterprise Linux 5, 6, 7, 8 without using Satellite server?     Creating a Local Repository and Sharing With Disconnected/Offline/Air-gapped Systems [Master Article]     Yum Repository Configuration Helper   This tool will help you set up a simple Yum repository for your local machine or a small number of other machines to use.     How to create local repository distributed through apache of Red Hat Enterprise Linux 5/6/7/8 using DVD iso for update or installation?  Useful Links:  Red Hat on GitHubAppendix: How to fix Repos not Visible on Client issue ? goto https://access. redhat. com/labs/rhpc/ select the product -&gt; version and arch-&gt; follow scripted or manual "
    }, {
    "id": 78,
    "url": "http://localhost:4000/satellite-server",
    "title": "Red Hat Satellite Server",
    "body": "Reference Documents Red Hat Satellite Quick Start - Installing, configuring, and provisioning physical and virtual hosts from Red Hat Satellite Servers.  INSTALLING SATELLITE SERVER ASSOCIATING OBJECTS WITH THE DEFAULT ORGANIZATION AND LOCATION IMPORTING SUBSCRIPTIONS AND SYNCHRONIZING CONTENT MANAGING AND PROMOTING CONTENT PATCHING YOUR SYSTEMSAfter installing Satellite with the satellite-installer command, all Satellite services are started and enabled automatically. View the list of these services by executing: # satellite-maintain service listTo see the status of running services, execute:# satellite-maintain service statusTo stop the satellite-maintain services, execute:# satellite-maintain service stopTo start the satellite-maintain services, execute:# satellite-maintain service startTo restart the satellite-maintain services, execute:# satellite-maintain service restartRefer: Starting and stopping red hat satellite "
    }, {
    "id": 79,
    "url": "http://localhost:4000/apache-guacamole",
    "title": "Remote Access without VPN - using Apache Guacamole",
    "body": "https://www. youtube. com/watch?v=Ti6fQQcGy-Y&amp;feature=youtu. behttps://guacamole. apache. org/releases/1. 1. 0/ Download Package and extract: https://guacamole. apache. org/releases/1. 1. 0/ $ tar -xzf guacamole-server-1. 1. 0. tar. gz$ cd guacamole-server-1. 1. 0/Install Pre-Req: Install needed packages sudo apt-get install libpng-devsudo apt-get install -y libjpeg-devsudo apt-get install libcairo-devsudo apt-get install libiossp-uuid-dev freerdp2-dev Configure, Make and install: Make sure all required options are enabled with packages $ . /configure --with-init-dir=/etc/init. d$ make$ make install$ ldconfigInstall ClientDownload and extract: tar -zxf guacamole-client-1. 1. 0. tar. gzcd guacamole-client-1. 1. 0/sudo apt install mavenmv package"
    }, {
    "id": 80,
    "url": "http://localhost:4000/resume",
    "title": "Resume",
    "body": ""
    }, {
    "id": 81,
    "url": "http://localhost:4000/rhel-kickstart-installation",
    "title": "RHEL 7/8 Kickstart Installation",
    "body": " 1. Pre-Requisites 2. Prepare kickstart file 3. Configure Utility Services     3. 1. Configure DHCP and DNS         Sample dhcpd. conf     Using DNSMASQ          3. 2. Configure a Web Server   3. 3. Download and Preapre RHEL ISO   3. 4. Configure Repo Server and ISO   Setup a tftp server    4. Setup a PXE Server     Configure Firewall    5. Boot from ISO and Use kickstart configuration     5. 1. Automated Booting and Installation    6. Appendix     6. 1. References   6. 2. Validating Kickstart File - ksvalidator   6. 3. Sample ks. cfg files   1. Pre-Requisites2. Prepare kickstart file Generate kickstart file using Kickstart Generator Add/modify content as needed (like post-scripts, Software RAID etc)3. Configure Utility Services3. 1. Configure DHCP and DNS:  Configure DHCP server in same network : We need a dhcp server to provide initial IP for the machine to use, so that it can access the http servers for ISO, Packages and kickstart. cfg Configure a DNS server in same network (Optional)dnsmasq Sample dhcpd. conf: ddns-update-style none;option domain-name  lab. local ;ignore client-updates;authoritative;allow booting;allow bootp;allow unknown-clients;# internal subnet for my DHCP Serversubnet 10. 1. 10. 0 netmask 255. 255. 255. 0 {range 10. 1. 10. 161 10. 1. 10. 170;option domain-name-servers 10. 1. 10. 120 1. 1. 1. 1;option routers 10. 1. 10. 120;option broadcast-address 10. 1. 10. 255;default-lease-time 600;max-lease-time 7200;# IP of PXE Servernext-server 10. 1. 10. 120;filename  pxelinux. 0 ;}Using DNSMASQ: # cat /etc/dnsmasq. conf |grep -v ^#|grep -v ^$domain-neededbogus-privserver=1. 1. 1. 1server=8. 8. 8. 8address=/lab. local/127. 0. 0. 1address=/lab. local/10. 1. 10. 120interface=enp0s8,lolisten-address=::1,127. 0. 0. 1,10. 1. 10. 120bind-interfacesexpand-hostsdhcp-range=10. 1. 10. 150,10. 1. 10. 160,255. 255. 255. 0,12hdhcp-option=3,10. 1. 10. 254dhcp-boot=pxelinux. 0enable-tftptftp-root=/tftpbootpxe-prompt= Press F8 for menu.  , 10dhcp-leasefile=/var/lib/dnsmasq/dnsmasq. leasesdhcp-authoritativedomain=lab. localconf-dir=/etc/dnsmasq. d,. rpmnew,. rpmsave,. rpmorigSetup pxeboot on dnsmasq Note: ## check dnsmasq logs## find below line in dnsmasq. confdhcp-leasefile=/var/lib/dnsmasq/dnsmasq. leases3. 2. Configure a Web Server:  To keep the ISO for booting and also to store YUM repository.  If using PXE, then need to setp tftp server on same.  It can be on same machine as DHCP server. 3. 3. Download and Preapre RHEL ISO: Refer : Product Downloads  Download the minimal boot image ISO (eg: Red Hat Enterprise Linux 8. 2 Boot ISO) Download the binary ISO and create a local repository server. 3. 4. Configure Repo Server and ISO:  Deploy a server with http (web server) Copy RHEL7/8 content to a directory on server and enable repo (Refer how to setup local repo server) Copy RHEL7/8 Boot ISO (minimal one) to another directory (if planning remote ISO loading) Copy kickstart to Web server path (eg: /var/www/html/ks/rhel/8/ks. cfg)Setup a tftp server: TFTP serverdnsmasq has built-in TFTP server. To use it, create a root directory for TFTP (e. g. /srv/tftp) to put transferable files in. enable-tftptftp-root=/srv/tftpFor increased security it is advised to use dnsmasq’s TFTP secure mode. In secure mode only files owned by the dnsmasq user will be served over TFTP. You will need to chown TFTP root and all files in it to dnsmasq user to use this feature. tftp-secure 4. Setup a PXE Server# yum install dhcp tftp tftp-server syslinux vsftpd xinetd# cp /usr/share/doc/dhcp-4. 2. 5/dhcpd. conf. example /etc/dhcp/dhcpd. confContent of /tftpboot/pxelinux. cfg/default default menu. c32prompt 0timeout 300ONTIMEOUT locallabel 1menu label ^1) Install RHEL 8 with Local Repokernel rhel8/vmlinuzappend initrd=rhel8/initrd. img method=http://10. 1. 10. 120/rhel/8 inst. ks=http://10. 1. 10. 120/ks/rhel/8/ks. cfg devfs=nomount# append initrd=rhel8/initrd. img inst. ks=http://10. 1. 10. 120/ks/rhel/8/ks. cfg devfs=nomountlabel 2menu label ^2) Boot from local drive RefernceConfigure Firewall: # firewall-cmd --add-service=ftp --permanentsuccess# firewall-cmd --add-service=dhcp --permanentsuccess# firewall-cmd --add-port=69/tcp --permanent success# firewall-cmd --add-port=69/udp --permanent success# firewall-cmd --add-port=4011/udp --permanentsuccess# firewall-cmd --reload5. Boot from ISO and Use kickstart configuration If we are loading ISO manually to VM/Physical servers, the boot from “RHEL7/8 Boot ISO” (either attach to VM or fetch it from http path which you have stored in repo server setup previously) When the ISO installer asks for “Install”, press Tab and add inst. ks=http://10. 1. 10. 120/ks/rhel/8/ks. cfg (your http path) and press Enter Installation will kickstart without asking for any answers (unless you missed anything in ks. cfg)Note : To load your Kickstart file automatically without having to specify the inst. ks= boot option, name the file ks. cfg and place it on a storage volume labeled OEMDRV. 5. 1. Automated Booting and Installation:  If you are planning fully automated installation, you can use PXE booting; it will load basic boot ISO (need to create custom boot images).  You can mention the custom ks. cfg in the boot command line itself and installation will not even ask for kickstart file or any answers. Warning: Only use this method with restricted IP/DHCP/MAC binded network, otherwise it may wipe-out other systems when it reboot !!! 6. Appendix6. 1. References:  PREPARING INSTALLATION SOURCES HOW DO YOU PERFORM A KICKSTART INSTALLATION? PREPARING FOR A NETWORK INSTALLATION RHEL/CentOS 8 Kickstart example SAMPLE KICKSTART CONFIGURATIONS6. 2. Validating Kickstart File - ksvalidator: # yum install pykickstart$ ksvalidator /path/to/kickstart. ks## Changes in Kickstart Syntax$ ksverdiff -f RHEL6 -t RHEL76. 3. Sample ks. cfg files:  kickstart basic sample kickstart with Software RAID1 + LVM Install PXE Boot server for automated install RHEL 8 and CentOS 8"
    }, {
    "id": 82,
    "url": "http://localhost:4000/s2i",
    "title": "s2i Cheat Sheet",
    "body": "Also See,  OpenShift-Kubernetes-Docker Cheatsheet Skopeo Cheat Sheet for details.  Podman Cheat Sheet for details. Note : This is s a living document and I will update whenever needed $ s2i version#-- Create $ s2i create IMAGE_NAME DESTINATION_DIR $ s2i create s2i-test s2i-test$ tree -a s2i-tests2i-test├── Dockerfile├── Makefile├── README. md├── s2i│  └── bin│    ├── assemble│    ├── run│    ├── save-artifacts│    └── usage└── test  ├── run  └── test-app    └── index. html4 directories, 9 files#-- configure all templated files as needed before creating image#-- Create the builder image$ cd s2i-test$ sudo podman build -t s2i-do288-httpd . #-- see images$ sudo podman images#-- Build application image by combining builder image + application#-- and output as Dockerfile in destination$ s2i build test/test-app/ \ s2i-do288-httpd s2i-sample-app \ --as-dockerfile ~/s2i-sample-app/Dockerfile$ cd ~/s2i-sample-app$ tree . . ├── Dockerfile├── downloads│  ├── defaultScripts│  └── scripts└── upload  ├── scripts  └── src    └── index. html6 directories, 2 files"
    }, {
    "id": 83,
    "url": "http://localhost:4000/skopeo",
    "title": "Skopeo Cheat Sheet",
    "body": "Also see,  OpenShift-Kubernetes-Docker Cheatsheet Skopeo Cheat Sheet for details.  Podman Cheat Sheet for details. Note : This is s a living document and I will update whenever needed  Skopeo Handy Commands     Copy local oci image to destination   copy using credential to destination   inspect the image   delete an image    ReferencesSkopeo Handy CommandsCopy local oci image to destination: $ skopeo copy \ oci:/home/USER/SOURCE_PATH \ docker://quay. io/USER/IMAGE_NAME:VERcopy using credential to destination: $ skopeo copy \ --dest-creds=USER_NAME:TOKEN \ oci:/home/USER/SOURCE_PATH \ docker://REGISTRY_URL/USER/IMAGE_NAME:VER$ sudo skopeo copy \ containers-storage:localhost/IMAGE_NAME \ docker://REGISTRY_URL/USER/IMAGE_NAME:VERinspect the image: $ skopeo inspect REGISTRY_URL/IMAGE_NAMEdelete an image: $ sudo skopeo delete \ docker://REGISTRY_URL/USER/IMAGE_NAME:VERReferences Skopeo Repo Say “Hello” to Buildah, Podman, and Skopeo - Red Hat Blog Promoting container images between registries with skopeo Skopeo Help Skopeo 1. 0 released"
    }, {
    "id": 84,
    "url": "http://localhost:4000/ssh-hacks",
    "title": "SSH Hacks and Tips",
    "body": " SSH Login to Remote Server over Jumphost NFS over SSH Tunnel Install python3 offline - CentosSSH Login to Remote Server over Jumphost: ssh -J user@JUMP_HOST user@TARGET_HOST Copy:scp -o 'ProxyJump user@JUMP_HOST' FILE_OR_DIR user@TARGET_HOST:/PATH_TO_DIR NFS over SSH Tunnel: Ref: Gist mkdir /mnt/nfs-share#-- Start SSH tunnel (local-to-remote port)ssh -fNv -L 3049:localhost:2049 user@JUMP_HOST#-- Mount the foldermount -t nfs -o port=3049 localhost:/PATH_TO_DIR /mnt/MOUNT_PATHInstall python3 offline - Centos:  Python3 on CentOS Offline Another one"
    }, {
    "id": 85,
    "url": "http://localhost:4000/ssl",
    "title": "SSL Certificate Management - Quick Reference",
    "body": "Create a Root CA: ## create CA key## remove the -des3 option for non-password protected key openssl genrsa -des3 -out myserver-CA. key 4096## self-sign CA Certificateopenssl req -x509 -new -nodes -key rootCA. key -sha256 -days 1024 -out myserver-CA. pemCreate Server Key, CSR and Certificate: ## Create a new SSL Key for server/appopenssl genrsa -out myserver. key 2048## Generate Certificate Signing Request$ openssl req -new \ -subj  /C=US/ST=North Carolina/L=Raleigh/O=Red Hat/CN=todo-https. apps. ocp4. example. com  \ -key myserver. key \ -out myserver. csr## Verify CSR content$ openssl req -in myserver. csr -noout -text## Generate Certificate using CSR and CA## openssl x509 -req -in &lt;CSR FILE&gt; \##  -CA &lt;CA FILE&gt; -CAkey myserver-CA. key -CAcreateserial \##  -passin file:passphrase. txt \##  -out &lt;EXPORT CRT&gt; -days 3650 -sha256 -extfile myserver. ext$ openssl x509 -req \ -passin file:passphrase. txt \ -CA myserver-CA. pem -CAkey myserver-CA. key -CAcreateserial \ -in myserver. csr \ -out myserver. crt \ -days 1825 -sha256 -extfile myserver. ext## verify certificte content$ openssl x509 -in myserver. crt -text -noout"
    }, {
    "id": 86,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 87,
    "url": "http://localhost:4000/terraform-associate",
    "title": "Terraform Associate Certification",
    "body": " Introduction     References to Start    Creating first Instance using Terraform     Configure AWS Credential   Create your first terraform fie   Destroying Resource   Terraform DigitalOcean Droplet   Terraform State File   Desired State and Current State   Provider Architecture   Types of Terraform Provides         Configure 3rd Party provider           Managing Configurations     Attributes &amp; Outputs in Terraform   Referencing Cross-Account Resource attributes   Terraform Variables   Variable Assignment   Count and Count Index in Terraform   Conditional Expression in Terraform   Local Values   Terraform Functions   Data Sources in Terraform   Debugging in Terraform   Formatting in Terraform   Validate Terraform Files   Load Order &amp; Semantics in Terraform   Dynamic Blocks   Terraform Taint   Splat Expression   Terraform Graph   Saving Terraform Plan to a file   Terraform Output   Terraform Settings   Handliing Larger Infrastructure    Terraform Provisioners     Types of Provisioners         remote-exec Provisioners     local-exec Provisioner           Modules and Workspaces     Understanding DRY Principle   Implementing Module   Varialbls and Modules   Terrform Registry   Terraform Workspace    Remote State Management     Terraform and Git Integration for Team Management   Module Sources in Terraform   Terraform and . gitignore   Remote State Management with Terraform   Implementing S3 Backend   Understanding State File Locking   Using DynamoDB with S3 for State Locking   Terraform State Management   Importing Existing Resource    Security     Multi-region and Multi-profile deployment   Terraform with STS   Handling Sensitive Data in Output    Terraform Cloud     Sentinel   Overview of Remote Backends    Appendix A - Useful References Appendix B - Notes     Terraform Enterprise and Terraform Cloud    Appendix C - Frequently Asked Questions Appendix D - Other Commands to Refer Appendix E - QuestionsIntroductionReferences to Start:  Get Started Study Guide - Terraform Associate Certification Exam Review - Terraform Associate Certification Sample Questions - Terraform Associate Certification Terraform Commands (CLI) Install Terraform HashiCorp Infrastructure Automation Certification Study Guide - Terraform Associate Certification What is Mutable vs. Immutable Infrastructure?(hashicorp) What Is Immutable Infrastructure?(digitalocean) 250 Practice Questions For Terraform Associate Certification Terraform Beginners Track - (collabnix - GitHub) Using Ansible to automate app deployment on Terraform-provided infrastructure Writing Ansible Playbooks for New Terraform Servers - (victorops. com) Terraform Course - Automate your AWS cloud infrastructure (freeCodeCamp. org) HashiCorp Certified Terraform Associate - Overview (Video) terraform-beginner-to-advanced-resource - GitHub Guidance on HashiCorp Certified — Terraform Associate -(Ravichandran Somasundaram on Medium) 100DaysOfIaC (Ryan Irujo on GitHub)Creating first Instance using TerraformProviders eg: AWS credential can give as,  Static credentials Environment variables Shared credentials/configuration file CodeBuild, ECS, and EKS Roles EC2 Instance Metadata Service (IMDS and IMDSv2)Configure AWS Credential:  IAM -&gt; Users -&gt; Create New user, Programatic Access Attach Exisiting Policies -&gt; Add Administrator Access Take Access Key and Secret KeyCreate your first terraform fie: provider  aws  { region   =  ap-southeast-1  access_key =  my-access-key  secret_key =  my-secret-key  version =  &gt;=2. 0 }resource  aws_instance   web  { ami      =  ami-0cd31be676780afa7  instance_type =  t2. micro  tags = {  Name =  FirstEC2  }}Then,  terraform init which will download and configure plugins which we have mentioned in the terraform file.  terraform plan will show you the items going to create terraform apply will create the resources as per terraform template.  -auto-approve will not ask for confirmationDestroying Resource:  terraform destoy will delete the resources terraform destroy -target aws_instance. web - destroy specific resource only.  also you can comment out the resource, then terraform will detect it as not part of config and will remove when you do plan or applyTerraform DigitalOcean Droplet: To generate API tokens from Digital Ocean Generate DigitalOcean Token provider  digitalocean  { token =  YOUR-TOKEN }resource  digitalocean_droplet   doinstance  { image =  ubuntu-18-04-x64  name  =  prod  region =  nyc1  size  =  s-1vcpu-1gb }Terraform State File:  will keep every info about the resourceDesired State and Current State: Terraform always make current state to desired state.  terraform refresh - will refresh the state by checking current state terraform plan will do refresh automatically in background.  terraform show will give you the details of state (intstead of opening . tfstate file) Desired state always follow your . tf content. Provider Architecture:                            +-------------------++-------------+   +---------------+ +-----------+  |          ||       |   |        | |      |  |  Resources    || . tf file  +----&gt;+  terraform  +-&gt;+ Provider +--&gt;+  in       ||       |   |        | |      |  |  Provider    |+-------------+   +---------------+ +-----------+  |          |                            +-------------------+ provider will have different versions, if nothing mentioned it will take the latest.  for production, mention specific version of provider as needed. &gt;=1. 0       Greater than equal to the version&lt;=1. 0       Less than equal to the version~&gt;2. 0       Any version in the 2. x range&gt;=2. 10, &lt;=2. 30  Any version between 2. 10 and 2. 30Example terraform{required_version =  ~&gt;0. 12. 0 required_providers { aws =  ~&gt;2. 6  # is equivalent to &gt;= 2. 6, &lt; 2. 7}Types of Terraform Provides:  Hashicorp Distributed     Download automatically during terraform init    Third Party or Community providers     For cases where official providers not supporting some features.    for some proprietary platform to use with Terraform   Configure 3rd Party provider:  Place the plugin in specific directory     Windows: %APPDATA%\terraform. d\plugins   All other systems: ~/. terraform. d/plugins   Managing Configurations keep configurations in directoriesAttributes &amp; Outputs in Terraform: Get details of created resources and use it for further steps. resource  aws_s3_bucket   mys3  { bucket =  demo-onboarding-20200903 }output  mys3bucket  { value = aws_s3_bucket. mys3. bucket_domain_name}then output will be, . . Apply complete! Resources: 2 added, 0 changed, 0 destroyed. Outputs:eip = 13. 251. 177. 36mys3bucket = demo-onboarding-20200903. s3. amazonaws. com There are more attributes we can use; check documentation and under Attribute Refernce in each resource type.  If we dont mention attribute, then terraform will display all attribues associated with the resource. Referencing Cross-Account Resource attributes: You can associate resources by referring attributes of resources. Eg: Assigning EIP to an instance resource  aws_eip_association   eip_assoc  { instance_id = aws_instance. web. id allocation_id = aws_eip. mylb. id}Terraform Variables:  Create variables and store values for repeated usagevariable  my_ip  { default =  10. 1. 10. 10/32 }and use the variable, resource  aws_security_group   allow_tls  { name    =  test-allow_tls  ingress {  description =  TLS from VPC   from_port  = 443  to_port   = 443  protocol  =  tcp   cidr_blocks = [var. my_ip] }}Variable Assignment:    Environment variables - can use environment variable with a prefix TF_VAR_. eg:   export TF_VAR_instance_type=t2. micro     Command Line Flags   terraform plan -var= instancetype=t2. small      From a File - use terraform. tfvars - terraform will load all variables from this file. If different var files to be used, then   terraform plan -var-file= custom. tfvars     Variable Defaults - can keep variable default in another . tf file.  $ cat variables. tf variable  my_ip  { default =  10. 1. 10. 10/32 } if no value mentioned, then default value will be used.  if default value not defined, then terraform will ask for variable when do apply or plan##Variables Datatypes Restict to use specific variable type  number string list mapvariables. tf: variable  image_id  { type = string  }variable  az  { type = list}variable  list  { type = list defaults = [ m5. large , m5. xlarge , t2. micro ]}variable  types  { type = map defaults = {  us-east-1 =  t2. micro   us-west-2 =  t2. nano   ap-south-1 =  t2. small  }}and terraform. tfvars elb_name= myelb timeout= 400 az=[ us-west-1a , us-west-1b ]Fetching Data from maps and list You can call variable as  instance_type = var. types[ us-west-1a ] - for a map instance_type = var. list[0] - for listCount and Count Index in Terraform:  Create multipe resources of same type use count parameter use count. index for counts and use it for indentifying resource. resource  aws_instance   multi-instance  { ami      =  ami-0cd31be676780afa7  instance_type =  t2. micro  count = 3 tags = {  Name =  hello-${count. index}  }}Also use the count. index to fetch details from a list and use it from names. variable  instance_names  { type = list default = [ web-front , web-back , db ]}and,  Name = var. instance_names[count. index]Conditional Expression in Terraform:  Terraform create and act based on conditional expressionsvariable  istest  {}resource  aws_instance   prod  { ami      =  ami-0cd31be676780afa7  instance_type =  t2. micro  # if var. istest is false, then create 1 instance, else 0 instance count = var. istest == false ? 1 : 0}resource  aws_instance   dev  { ami      =  ami-0cd31be676780afa7  instance_type =  t2. large  # if var. istest is true, then create 1 instance, else 0 instance count = var. istest == true ? 1 : 0}then, mention your default value in terraform. tfvars istest = trueLocal Values: Doc - Local Values  Define and use inside resourceslocals { common_tags = {  Owner =  Dev Team   Service =  Backend  }}# in resourceresource  aws_instance   dev  { ami      =  ami-0cd31be676780afa7  instance_type =  t2. large  tags = local. common_tags}resource  aws_ebs_volume   db_ebs  { availability_zone =  us-west-2a  size       = 8 tags = local. common_tags}Terraform Functions: Doc - Built-in Functions function (argument1, argument2)  You can test functions by terraform consolelocals { time = formatdate( DD MMM YYYY hh:mm ZZZ , timestamp())}variable  region  { default =  ap-south-1 }variable  tags  { type = list default = [ firstec2 , secondec2 ]}variable  ami  { type = map default = {   us-east-1  =  ami-0323c3dd2da7fb37d    us-west-2  =  ami-0d6621c01e8c2de2c    ap-south-1  =  ami-0470e33cd681b2476  }}resource  aws_key_pair   loginkey  { key_name  =  login-key  public_key = file( ${path. module}/id_rsa. pub )}resource  aws_instance   app-dev  {  ami = lookup(var. ami,var. region)  instance_type =  t2. micro   key_name = aws_key_pair. loginkey. key_name  count = 2  tags = {   Name = element(var. tags,count. index)  }}Data Sources in Terraform: Doc - Data Source  Allow data to be fetched from external sources and use inside config dynamically. data  aws_ami   app_ami  { most_recent = true owners = [ amazon ] filter {  name =  name   values = [ amzn2-ami-hvm* ] }}# then use the data variable in resourceresource  aws_instance   app-dev  {  ami = data. aws_ami. app_ami. id  instance_type =  t2. micro }Debugging in Terraform: Doc  enable TF_LOG variable with appropriate values - TRACE, DEBUG, INFO, WARN or ERROR export TF_LOG=TRACE to see details logs (TRACE is the most verbose one) export TF_LOG_PATH=YOUR_PATH_FOR_LOG will save logs in file. (You should set TF_LOG)Formatting in Terraform:  Use terraform fmt to cleanup the codeValidate Terraform Files:  terraform validate - Check whether a configuration is syntactically valid or not check if any unsupported arguments, any undeclared variables etcLoad Order &amp; Semantics in Terraform:  Terraform generally loads all the config files - . tf &amp; . tf. json - within the directory, specified in alphabetical order.  Split the code into multiple files, eg:     provider. tf   variables. tf   ec2. tf   iam. tf   etc.    Dynamic Blocks: Doc  for repeated steps or like loopsNormal blocks: resource  aws_security_group   demo_sg  { name    =  sample-sg  ingress {  from_port  = 8200  to_port   = 8200  protocol  =  tcp   cidr_blocks = [ 0. 0. 0. 0/0 ] } ingress {  from_port  = 8201  to_port   = 8201  protocol  =  tcp   cidr_blocks = [ 0. 0. 0. 0/0 ] }}With dynamic blocks: variable  sg_ports  { type    = list(number) description =  list of ingress ports  default   = [8200, 8201,8300, 9200, 9500]}resource  aws_security_group   dynamicsg  { name    =  dynamic-sg  description =  Ingress for Vault  dynamic  ingress  {  for_each = var. sg_ports  iterator = port  content {   from_port  = port. value   to_port   = port. value   protocol  =  tcp    cidr_blocks = [ 0. 0. 0. 0/0 ]  } }} Use iterator for better reading (eg: iterator = port above)Terraform Taint:  manually marks a terraform managed resource as trainted and forcing it to be destroyed and recreated on the next apply.  terraform taint command will make modification in the tfstate file and recreate action will happen in next apply.  terraform taint command will not modify the . tf file or infrastructure. terraform taint aws_instance. myec2Splat Expression:  use wildcards to get multiple resource information. # single resourceoutput  arn-single  { value = aws_iam_user. lb[0]. arn}# multiple resourceoutput  arns  { value = aws_iam_user. lb[*]. arn}Terraform Graph:  Generate visual representation of either a configuration or execution plan. terraform graph &gt; graph. dot DOT format and can convert to an image (Use 3rd party tool)sudo apt-get install graphviz# or sudo yum install graphviz# thencat graph. dot | dot -Tsvg &gt; graph. svgSaving Terraform Plan to a file:  Save terraform plan content for later use - terraform plan -out=backpup-01 (will be a binary file) Later use this file to create resource - terraform apply backpup-01Terraform Output:  To display or extract the value of an output variable from the state file. terraform output iam_namesTerraform Settings:  To configure the behavior of terraform itself we can use terraform block required_providers - specifies all of the providers needed for the modules, mapping each local provider names to a source address and a version constraints. terraform { required_version =  &gt; 0. 12. 0  required_providers {  aws =  ~&gt; 2. 0   mycloud = {   source =  mycloud/mycloud    version =  ~&gt; 1. 0   } }}Handliing Larger Infrastructure:  always split into smaller files and explicitly call use -refresh=false to prevent terraform from querying the current state during operations like terraform plan use -target=resource flag to do terraform action only on that resourceTerraform Provisioners Can be used to model specific actions on the local machine or on a remote machine in order to prepare servers or other infra objects for service. Ref:  Provisioners Provision Infrastructure with PackerTypes of Provisioners: Types  Creation-Time Provisiones     if creation time provisioner fails, the resource will be marked as tainted.     Destroy-Time Provisioners     run only when there is when = destroy option mentioned.    Failure Behaviours Options  on_failure = continue - Ignore and continue with creation or destroy action on_failure = fail - Stop applying and show error; also taint the resource if this is a creation taskremote-exec Provisioners: Ref: Provisioner Connection Settings  invoke actions on remote machine which terraform createdresource  aws_instance   myec2  {  ami =  ami-082b5a644766e0e6f   instance_type =  t2. micro   key_name =  kplabs-terraform   provisioner  remote-exec  {   inline = [     sudo amazon-linux-extras install -y nginx1. 12 ,     sudo systemctl start nginx    ]  connection {   type =  ssh    user =  ec2-user    private_key = file( . /kplabs-terraform. pem )   host = self. public_ip  }  }} A connection block nested directly within a resource affects all of that resource’s provisioners.  A connection block nested in a provisioner block only affects that provisioner, and overrides any resource-level connection settings. local-exec Provisioner: Doc  invoke local executable after resource is created can execute ansible playbooks on the created serverresource  aws_instance   myec2  {  ami =  ami-082b5a644766e0e6f   instance_type =  t2. micro   provisioner  local-exec  {  command =  echo ${aws_instance. myec2. private_ip} &gt;&gt; private_ips. txt  }}Another example: provisioner  local-exec  { command =  sleep 120; ansible-playbook -i '${digitalocean_droplet. www-example. ipv4_address}' playbook. yml }Modules and WorkspacesUnderstanding DRY Principle:  *Dont Repeat Yourself redution repetition of software patters define in source and refer it in resourcesImplementing Module: Doc Define the structure in modile dir. project_dir/modules/ec2 : resource  aws_instance   myec2  {  ami =  ami-082b5a644766e0e6f   instance_type = var. instance_type}and in project just reference the same project_dir/a-project/ : module  ec2module  { source =  . . /. . /modules/ec2 }Dir structure : $ tree . . /module-demo/. . /module-demo/├── a-project│   ├── myec2. tf│   └── provider. tf├── b-project│   ├── myec2. tf│   └── provider. tf└── modules  └── ec2    └── module-ec2. tf4 directories, 5 filesVarialbls and Modules:  you cannot change argument values while calling the module as the values are hardcoded in module; instead use variable in moduleresource  aws_instance   myec2  { ami      =  ami-0cd31be676780afa7  instance_type = var. instance_type} and the variable need to define in module directory - variables. tf, with default values. variable  instance_type  { default =  t2. micro } and from project, you can overwrite the variable as needed while calling the modulemodule  ec2module  { source    =  . . /modules/ec2  instance_type =  t2. large }Terrform Registry: Terraform Registry Doc  The public registry uses a three-part // format, Private modules use a four-part /// format. Notes :  repository of modules writtern by the Terraform community you can use available modules from registry instead of writing your own.  verified modules which are maintained by 3rd party vendors are also available in Terraform Registry a blue verification badge appears next to the module Search in registry. terraform. io and filter with verified modules.  Basic usage samples, variables etc will be displayed in the module page You can reference to the module directly from Terraform registry mention the module version as neededRef: ec2-instance module  ec2_cluster  { source         =  terraform-aws-modules/ec2-instance/aws  version        =  ~&gt; 2. 0  name          =  my-cluster  instance_count     = 5 ami          =  ami-ebd02392  instance_type     =  t2. micro  key_name        =  user1  monitoring       = true vpc_security_group_ids = [ sg-12345678 ] subnet_id       =  subnet-eddcdzz4  tags = {  Terraform  =  true   Environment =  dev  }} When you do terraform init, terraform will download the module to local path . /. terraform/modules/Terraform Workspace: Workspaces  different workspace to manage environment, eg; different variables. $ terraform workspace showdefault$ terraform workspace new devCreated and switched to workspace  dev !You're now on a new, empty workspace. Workspaces isolate their state,so if you run  terraform plan  Terraform will not see any existing statefor this configuration. $ terraform workspace list default* dev$ terraform workspace select devSwitched to workspace  dev . Sample usage: provider  aws  { region   =  ap-southeast-1  shared_credentials_file =  $HOME/. aws/credentials  profile         =  default  version =  &gt;=2. 0 }resource  aws_instance   myec2  { ami      =  ami-0cd31be676780afa7  instance_type = lookup(var. instance_type,terraform. workspace)}variable  instance_type  { type = map default = {  default =  t2. nano   stage =  t2. nano   dev =  t2. micro   prod =  t2. large  }} terraform will create separate terraform. tfstate files in terraform. tfstate. d/WORKSPACE_NAME/ directories in the project directoy$ tree terraform. tfstate. d/terraform. tfstate. d/├── dev│   └── terraform. tfstate├── prod└── stage  └── terraform. tfstate3 directories, 2 filesRemote State ManagementRemote State Terraform and Git Integration for Team Management:  never keep credentials or secrets in repo; use safer methodseg: password =  ${file(. . /db_password. txt)}  DO NOT store terraform. tfstate file in repo as it will store credentials or secrets in plain textModule Sources in Terraform: Supported Module Sources  Local Path - Must be . / or . . / to indicate the local path is intendedmodule  ec2module  { source =  . . /. . /modules/ec2 } Terraform Registry GitHub, BitBucket, Generic Git, Mercurial Reposmodule  vpc  { source =  git::https://example. com/vpc. git }module  storage  { source =  git::ssh://username@example. com/storage. git }module  vpc  { source =  git::https://example. com/vpc. git?ref=v1. 2. 0 }module  mymodule  { source =  github. com/techbeatly/module-repo } HTTP URLs S3 buckets GCS BucketsTerraform and . gitignore: Files to ignore  . terraform terraform. tfvars - sensitive data like username or password terraform. tfstate - should be stored in the remote side crash. logSample . gitignore # Local . terraform directories**/. terraform/*# . tfstate files*. tfstate*. tfstate. *# Crash log filescrash. log# Exclude all . tfvars files, which are likely to contain sentitive data, such as# password, private keys, and other secrets. These should not be part of version # control as they are data points which are potentially sensitive and subject # to change depending on the environment. #*. tfvars# Ignore override files as they are usually used to override resources locally and so# are not checked inoverride. tfoverride. tf. json*_override. tf*_override. tf. json# Include override files you do wish to add to version control using negated pattern## !example_override. tf# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan# example: *tfplan*# Ignore CLI configuration files. terraformrcterraform. rcRemote State Management with Terraform:  Store tfstate in remote location securly Multiple options supported by terraform     Standard Backend - State Storage and Locking   Enhanced Backend - All features of Standard + Remote Management   Implementing S3 Backend: Doc  create an s2 bucket manually and create backend to store the tfstateterraform { backend  s3  {  bucket =  terraform-remote-demo   key  =  remote-terraform-state-demo. tfstate   region =  ap-southeast-1  }}Understanding State File Locking:  consider issue when multiple people working on the tarraform config parallel writing on tfstate file.  terraform will lock the tfstate file.  need to mention the locking in backend (S3 by default not support any locking)Using DynamoDB with S3 for State Locking:    create a DynamoDB table for storing lock detailsTable Name : Primay Key :     and update the backend with dynamodb_table entry  terraform { backend  s3  {  bucket =  terraform-remote-demo   key  =  remote-terraform-state-demo. tfstate   region =  ap-southeast-1   dynamodb_table =  tf-state-demo  }} now, when you run terraform plan or apply, you will see an entry in dynamodb table but only until the command finish. This will allow terraform to avoid multiple actions to be run on same tfstate file. Terraform State Management:  terraform state - for advanced state managementSubcommands:  list        List resources in the state  mv         Move an item in the state  pull        Pull current state and output to stdout  push        Update remote state from a local state file  replace-provider  Replace provider in the state  rm         Remove instances from the state             but it will not remove the resouce from cloud/provider  show        Show a resource in the stateEg:  if you rename or change something on a resource, terraform will destroy and recreate the resource with new name. To avoud this, you can use terraform state move$ terraform state listaws_iam_user. lbaws_instance. myec2$ terraform state mv aws_instance. myec2 aws_instance. myec2newMove  aws_instance. myec2  to  aws_instance. myec2new Successfully moved 1 object(s). $ terraform state listaws_iam_user. lbaws_instance. myec2new terraform state rm will remove the resource from state but it will not remove the resouce from cloud/provider.   $ terraform state rm aws_instance. myec2Removed aws_instance. myec2Successfully removed 1 resource instance(s).     but next time when you run terraform plan or apply, terraform will recreate the instance as again as the resource definition is still there.  terraform state show RESOURCE_NAME will show details of a specific resource$ terraform state show aws_instance. myec2Importing Existing Resource:  importe resources which are created manually create the . tf file based on existing resourceresource  aws_instance   myec2  { ami          =  ami-0b1e534a4ff9019e0  instance_type     =  t2. micro  vpc_security_group_id = [ vpc-4a59ba2c ] key_name       =  tf-20200805  subnet_id       =  subnet-3f9f5877  tags {  Name =  test  }} now import the resource to state$ terraform import aws_instance. myec2 i-034503eb8b60d3a51aws_instance. myec2: Importing from ID  i-034503eb8b60d3a51 . . . aws_instance. myec2: Import prepared! Prepared aws_instance for importaws_instance. myec2: Refreshing state. . . [id=i-034503eb8b60d3a51]Import successful!The resources that were imported are shown above. These resources are now inyour Terraform state and will henceforth be managed by Terraform. Security keep provider credential out of project place (like use aws cli etc)Multi-region and Multi-profile deployment:  define multiple provider blocks with alias and refer provider in resource block also add separate profile in provider blockprovider  aws  { region   =  us-west-1 }provider  aws  { alias   =  aws02  region   =  ap-south-1  profile  =  account02 }and in resources, call proper providers. resource  aws_eip   myeip  { vpc =  true }resource  aws_eip   myeip01  { vpc =  true  provider =  aws. aws02 }Terraform with STS: provider  aws  { region         =  ap-southeast-1  assume_role {  role_arn =  YOUR_ROLE_ARN   session_name =  sts-arn-demo  }}Handling Sensitive Data in Output: locals { db_password = {  admin =  password  }}output  db_password  { value = local. db_password sensitive  = true}Terraform CloudGUI Sentinel: Doc  embeded policy-as-code framework paid featureimport  tfplan  main = rule { all tfplan. resources. aws_instance as _, instances {  all instances as _, r {   (length(r. applied. tags) else 0) &gt; 0  } }}Overview of Remote Backends: Doc  Terraform operations can execute in Terraform cloud and can see the output in local terminal```terraform {#required_version = “~&gt; 0. 12. 0”backend “remote” {}}resource “aws_iam_user” “lb” { name = “remoteuser” path = “/system/”} and, we have `backend. hcl`workspaces { name = “terraform-iac-usecases” }hostname   = “app. terraform. io”organization = “techbeatly” Then, `terraform login` which will ask for token from [API](https://app. terraform. io/app/settings/tokens) and will show the success message. Then, `terraform init` with `backend`$ terraform init -backend-config=backend. hcl $ terraform apply The same settings can alternatively be specified on the command line as follows:$ terraform init   -backend-config=”address=demo. consul. io”   -backend-config=”path=example_app/terraform_state”   -backend-config=”scheme=https” # Appendix A - Useful References- [Ansible, Terraform Excel Among Site Reliability Engineers, DevOps](https://thenewstack. io/ansible-terraform-excel-among-site-reliability-engineers-devops/)# Appendix B - NotesDeclarative Infrastructure Management- trackable via vcs- automation CI/CD- Reproducible env- safe and predictable- workflow- opensource providersPrimary Benefits of Infrastructure as Code- Automation- Versioning- Reusability## Terraform Enterprise and Terraform Cloud- SSO- Auditing- Private Data Center Networking- Clustering# Appendix C - Frequently Asked Questions**Q. Is it mandatory to keep the variables in `variables. tf` ?**# Appendix D - Other Commands to ReferUnlock terraform stateterraform force-unlock LOCK_ID [DIR] Upgrade the provider version to the latest acceptable one. terraform init -upgrade``` Appendix E - Questions"
    }, {
    "id": 88,
    "url": "http://localhost:4000/terraform",
    "title": "Terraform",
    "body": "ReferencesReferences to Start:  Get Started Study Guide - Terraform Associate Certification Exam Review - Terraform Associate Certification Sample Questions - Terraform Associate Certification Terraform Commands (CLI) Install Terraform HashiCorp Infrastructure Automation Certification Study Guide - Terraform Associate Certification What is Mutable vs. Immutable Infrastructure?(hashicorp) What Is Immutable Infrastructure?(digitalocean) 250 Practice Questions For Terraform Associate Certification Terraform Beginners Track - (collabnix - GitHub) Using Ansible to automate app deployment on Terraform-provided infrastructure Writing Ansible Playbooks for New Terraform Servers - (victorops. com) Terraform Course - Automate your AWS cloud infrastructure (freeCodeCamp. org) HashiCorp Certified Terraform Associate - Overview (Video) terraform-beginner-to-advanced-resource - GitHub Guidance on HashiCorp Certified — Terraform Associate -(Ravichandran Somasundaram on Medium) 100DaysOfIaC (Ryan Irujo on GitHub) Manage Your Entire VMware Infrastructure as Code with HashiCorp Terraform (YouTube) HashiCorp Workshops"
    }, {
    "id": 89,
    "url": "http://localhost:4000/vagrant",
    "title": "Vagrant Sandbox",
    "body": " Installing vagrant Project Setup Boxes     Manually downloading Boxes   List down available boxes    Configure Vagrantfile Vagrant Up and SSH to your machine Connect to your instance Synced Folders Provisioning     Create bootstrap. sh    Networking     Port Forwarding   Other Networking    Vagrant Share Cleanup your vagrant environment     Suspend   Halt   Destroy    Rebuilding the Environment ProvidersInstalling vagrant: Vagrant installation is so easy, just download your package from vagrant portal and install as per instruction. Refer Instalation Doc. Verify vagrant installation $ vagrant -vVagrant 2. 1. 1Project Setup: The purpose of the Vagrantfile:  Mark the root directory of your project. Many of the configuration options in Vagrant are relative to this root directory.  Describe the kind of machine and resources you need to run your project, as well as what software to install and how you want to access it. Create your project directory → goto project directory $ vagrant initIt will create a Vagrantfile in that directory. Boxes: Instead of building a virtual machine from scratch, which would be a slow and tedious process, Vagrant uses a base image to quickly clone a virtual machine. These base images are known as “boxes” in Vagrant, and specifying the box to use for your Vagrant environment is always the first step after creating a new Vagrantfile. Manually downloading Boxes: If you are connected to internet, box images will be downloaded automatically. If you want to manually download box, try below. Eg: you want to download Ubuntu image https://app. vagrantup. com/ubuntu/boxes/trusty64/versions/20180611. 0. 0/then append the provider URL and . box name - https://app. vagrantup. com/&lt;organization-name&gt;/boxes/&lt;box-name&gt;/versions/&lt;version&gt;/providers/&lt;provider&gt;. box https://app. vagrantup. com/ubuntu/boxes/trusty64/versions/20180611. 0. 0/providers/virtualbox. box Then you can add it to Vagrant $ vagrant box add boxes/trusty-server-cloudimg-amd64-vagrant-disk1. box --name ubuntu-trusty64==&gt; box: Box file was not detected as metadata. Adding it directly. . . ==&gt; box: Adding box 'ubuntu-trusty64' (v0) for provider:	box: Unpacking necessary files from: file://C:/Gini/VM/vagrant/boxes/trusty-server-cloudimg-amd64-vagrant-disk1. box	box: Progress: 100% (Rate: 267M/s, Estimated time remaining: --:--:--)==&gt; box: Successfully added box 'ubuntu-trusty64' (v0) for 'virtualbox'!List down available boxes: $ vagrant box listubuntu-trusty64 (virtualbox, 0)Configure Vagrantfile: Now we will configure our Vagrantfile with box name to be used. $ cat VagrantfileVagrant. configure( 2 ) do |config| config. vm. box =  ubuntu-trusty64 endVagrant Up and SSH to your machine: You can start your vagrant instance by vagrant up command. $ vagrant upBringing machine 'default' up with 'virtualbox' provider. . . ==&gt; default: Importing base box 'ubuntu-trusty64'. . . ==&gt; default: Matching MAC address for NAT networking. . . ==&gt; default: Setting the name of the VM: ubuntu-trusty64_default_1529377473485_7675==&gt; default: Clearing any previously set forwarded ports. . . Vagrant is currently configured to create VirtualBox synced folders withthe `SharedFoldersEnableSymlinksCreate` option enabled. If the Vagrantguest is not trusted, you may want to disable this option. For moreinformation on this option, please refer to the VirtualBox manual: https://www. virtualbox. org/manual/ch04. html#sharedfoldersThis option can be disabled globally with an environment variable: VAGRANT_DISABLE_VBOXSYMLINKCREATE=1or on a per folder basis within the Vagrantfile: config. vm. synced_folder '/host/path', '/guest/path', SharedFoldersEnableSymlinksCreate: false==&gt; default: Clearing any previously set network interfaces. . . ==&gt; default: Preparing network interfaces based on configuration. . . 	default: Adapter 1: nat==&gt; default: Forwarding ports. . . 	default: 22 (guest) =&gt; 2222 (host) (adapter 1)==&gt; default: Booting VM. . . ==&gt; default: Waiting for machine to boot. This may take a few minutes. . . 	default: SSH address: 127. 0. 0. 1:2222	default: SSH username: vagrant	default: SSH auth method: private key	default: Warning: Connection aborted. Retrying. . . 	default: Warning: Remote connection disconnect. Retrying. . . 	default: Warning: Connection aborted. Retrying. . . 	default: Warning: Connection reset. Retrying. . . 	default: Warning: Connection aborted. Retrying. . . 	default: Warning: Connection reset. Retrying. . . 	default: Warning: Connection aborted. Retrying. . . 	default:	default: Vagrant insecure key detected. Vagrant will automatically replace	default: this with a newly generated keypair for better security. 	default:	default: Inserting generated public key within guest. . . 	default: Removing insecure key from the guest if it's present. . . 	default: Key inserted! Disconnecting and reconnecting using new SSH key. . . ==&gt; default: Machine booted and ready!==&gt; default: Checking for guest additions in VM. . . 	default: The guest additions on this VM do not match the installed version of	default: VirtualBox! In most cases this is fine, but in rare cases it can	default: prevent things such as shared folders from working properly. If you see	default: shared folder errors, please make sure the guest additions within the	default: virtual machine match the version of VirtualBox you have installed on	default: your host and reload your VM. 	default:	default: Guest Additions Version: 4. 3. 36	default: VirtualBox Version: 5. 2==&gt; default: Mounting shared folders. . . 	default: /vagrant =&gt; C:/Gini/VM/vagrant/ubuntu-trusty64Our instanance is up and running as you can see above. You can also see the instance details from Oracle VM VirtualBox Manager console. Connect to your instance: $ vagrant sshWelcome to Ubuntu 14. 04. 5 LTS (GNU/Linux 3. 13. 0-151-generic x86_64) * Documentation: https://help. ubuntu. com/ System information disabled due to load higher than 1. 0 Get cloud support with Ubuntu Advantage Cloud Guest:	http://www. ubuntu. com/business/services/cloud0 packages can be updated. 0 updates are security updates. vagrant@vagrant-ubuntu-trusty-64:~$ hostname; uptime; uname -avagrant-ubuntu-trusty-64 03:16:59 up 12 min, 1 user, load average: 0. 00, 0. 03, 0. 05Linux vagrant-ubuntu-trusty-64 3. 13. 0-151-generic #201-Ubuntu SMP Wed May 30 14:22:13 UTC 2018 x86_64 x86_64 x86_64 GNU/Linuxvagrant@vagrant-ubuntu-trusty-64:~$This is a fully functions ssh session and you can do anything as in a normal ubuntu machine. (except rm -rf / which will remove the shared folder content as well) You can logout form the ssh session as normal. vagrant@vagrant-ubuntu-trusty-64:~$ logoutConnection to 127. 0. 0. 1 closed. Synced Folders: You can access your local files from guest machines (vagrant instances) using synced folders. By default, Vagrant shares your project directory to the /vagrant directory in your guest machine. If you are facing any issues for this, check your VM Guest addition packages and versions. (Can refer our vagrant up console output, you can see the guest addition checks. ) Let’s try some tasks We will check the /vagrant directory and create a new file there. vagrant@vagrant-ubuntu-trusty-64:~$ lsvagrant@vagrant-ubuntu-trusty-64:~$ cd /vagrant/vagrant@vagrant-ubuntu-trusty-64:/vagrant$ ls -ltotal 1-rwxrwxrwx 1 vagrant vagrant 75 Jun 19 03:03 Vagrantfilevagrant@vagrant-ubuntu-trusty-64:/vagrant$ cat &gt;newfile. txtThis is a new file created from vagrant instance. vagrant@vagrant-ubuntu-trusty-64:/vagrant$ ls -ltotal 1-rwxrwxrwx 1 vagrant vagrant 36 Jun 19 03:44 newfile. txt-rwxrwxrwx 1 vagrant vagrant 75 Jun 19 03:03 Vagrantfilevagrant@vagrant-ubuntu-trusty-64:/vagrant$ logoutConnection to 127. 0. 0. 1 closed. Let’s check this from our host machine now $ pwd/c/Gini/VM/vagrant/ubuntu-trusty64$ ls -ltotal 2-rw-r--r-- 1 TS-AP+gmadappa 4266656257 50 Jun 19 11:45 newfile. txt-rw-r--r-- 1 TS-AP+gmadappa 4266656257 75 Jun 19 11:03 Vagrantfile$ cat newfile. txtThis is a new file created from vagrant instance. Provisioning: Let’s take a scenario where we need this virtual machine serve as a web server. Normally, every time anyone using this machine need to start this vm, login to the guest machine and install these packages and setup this. Since vagrant has automated provisioning features, you can easily configure the bootstrap to install all required packages and setup automatically. Create bootstrap. sh: $ cat bootstrap. sh# !/usr/bin/env bashapt-get updateapt-get install -y apache2if ! [ -L /var/www ]; then rm -rf /var/www ln -fs /vagrant /var/wwwfiConfigure bootstrap. sh in VagrantfileVagrant. configure( 2 ) do |config| config. vm. box =  ubuntu-trusty64  config. vm. provision :shell, path:  bootstrap. sh endWhen you run vagrant again, it will create the vm and bootstrap. sh and you will get a vm with apache installed and webroot configured as you mentioned in the bootstrap. sh file. If your vm is already running, try vagrant reload --provision. Note : in below example I have setup nginx server setup. Since I want to test this without internet connection (offline) I have already downloaded the nginx-1. 15. 0. tar. gz file in project directory and this will be available in /vagrant` shared directory. Usually I test those commands manually inside a vm and make sure all are running properly to avoid issues during vagrant up. # !/usr/bin/env bashcd /vagranttar zxvf nginx-1. 15. 0. tar. gzcd nginx-1. 15. 0/. /configure --without-http_rewrite_module --without-http_gzip_modulemakesudo make installsudo /usr/local/nginx/sbin/nginx(To stop nginx /usr/bin/nginx -s stop; where -s signal means to send a signal to a master process: stop, quit, reopen, reload) Let’s test the web server from machine itself (as we did not configure networking for this vm) vagrant@vagrant-ubuntu-trusty-64:~$ wget 127. 0. 0. 1--2018-06-21 05:30:55-- http://127. 0. 0. 1/Connecting to 127. 0. 0. 1:80. . . connected. HTTP request sent, awaiting response. . . 200 OKLength: 612 [text/html]Saving to: 'index. html'100%[=================================================&gt;] 612   	--. -K/s  in 0s2018-06-21 05:30:55 (10. 2 MB/s) - 'index. html' saved [612/612]=====================================================================================Networking: Now we have a web server running inside vm but we need to configure this server to be accessed from outside world (or host machine) Port Forwarding: Port forwarding allows you to specify ports on the guest machine to share via a port on the host machine. Add below line to your Vagrantfile config. vm. network :forwarded_port, guest: 80, host: 4567 Then vagrant reload and try http://127. 0. 0. 1:4567 from your host machine and see the webpage is loading. Other Networking: You can also assign a static IP address to your guest vm or bridge vm onto an existing network. Vagrant Share: By using Vagrant Share you can share your Vagrant environment to anyone via Internet by using a URL.  HTTP sharing SSH sharing - allow instant SSH access by running vagrant connect --ssh General sharingRun vagrant share $ vagrant share. . . ==&gt; default: Creating Vagrant Share session. . . ==&gt; default: HTTP URL: http://test928923. ngrok. io. . . You can use that url (should be something like above) for accessing this. To end the sharing session, hit Ctrl+C in your terminal. Note : This plugin was removed in 2. 0. 2. If you find the share option is not available, you can install this plugin as show below. $ vagrant plugin install vagrant-share Cleanup your vagrant environment: Once you finished, you can resuspend, halt or destroy your vm’s. Suspend: vagrant suspend will save the current running state of the machine and stop it. It will still take your disk space but easy to get back online. $ vagrant suspend==&gt; default: Saving VM state and suspending execution. . . Halt: vagrant halt will gracefully shutdown and power off the guest vm. This is clean method but still preserve the content of your disks. Since its a full power cycle, vm will take extra time to get online back. Destroy: vagrant destroy will stop your vm, power it off and remove all the guest related data and disk. your guest vm. When you want machine back, just do a vagrant up. The disk space and RAM consumed by the guest machine is reclaimed and your host machine is left clean. Please note, since you remove all disks and images, it will take extra time to re-import image and make vm online again. Rebuilding the Environment: As we learned, even you halt, suspend or destroy you can rebuild the VM’s at anytime by running vagrant up as usual. (Make sure you don’t remove the project directory and Vagrantfile). Providers: By default, vagrant is using VirtualBox as provider but you can use VMWare or AWS as well. $ vagrant up --provider=vmware_fusionor$ vagrant up --provider=aws"
    }, {
    "id": 90,
    "url": "http://localhost:4000/Virtualbox",
    "title": "VirtualBox Notes",
    "body": " 1. VirtualBox Guest Addtion on CLI 2. Troubleshooing VirtualBox VM     2. 1. Unable to mount Shared folders in CLI   1. VirtualBox Guest Addtion on CLI Mount Guest Addtion ISO (from Console Menu) Mount it from OS (/vmboxguestiso is destination mount point)  mount /dev/cdrom /vmboxguestiso/    Resolve DependanciesInstall kernel-header `sudo yum update &amp;&amp; sudo yum -y install kernel-headers kernel-devel`## install other packagesyum install gcc make perl2. Troubleshooing VirtualBox VM2. 1. Unable to mount Shared folders in CLI: Check vboxsf module sudo modprobe vboxsf "
    }, {
    "id": 91,
    "url": "http://localhost:4000/vmware-tanzu",
    "title": "VMWare Tanzu",
    "body": " Cluster API The Cluster API Book TAM Lab 062 - Tanzu Kubernetes Grid (TKG) for your Home Lab (Video) Getting Started with vSphere with Tanzu (Cormac Hogan) Deploy HA-Proxy for vSphere with Tanzu (Cormac Hogan)"
    }, {
    "id": 92,
    "url": "http://localhost:4000/wordpress-apache",
    "title": "Wordpress with Apache",
    "body": "Notes on Wordpress Site Migration Configure Your WebServer: Setup a lamp deployment: You can do it manually or just create a template from deploymwent manager. apt install apache2apt install ufwapt install mariadb-servermysql_secure_installationapt install php-curl php-gd php-mbstring php-xml php-xmlrpc php-soap php-intl php-zipEnable the Rewrite Module: If you are setting up a fullstack lamp, this will be done automatically. But please make sure this is enabled. (By this we can utilize the WordPress permalink feature) sudo a2enmod rewriteAllowoverride: Enable . htaccess Overrides by editing the primary Apache configuration file. To allow . htaccess files, we need to set the AllowOverride directive within a Directory block pointing to our document root. Towards the bottom of the file /etc/apache2/apache2. conf, add the following block: &lt;Directory /var/www/html/&gt;  AllowOverride All&lt;/Directory&gt;(In our case I have used /web/mysite) Enable Virtualhost: This is to host multiple sites on same server and this is so easy. Create a config file for your site (mysite. conf) under /etc/apache2/ &lt;VirtualHost *:80&gt;ServerAdmin net. gini@gmail. comServerName mywebsite. comServerAlias www. mywebsite. comDocumentRoot /web/mysiteErrorLog ${APACHE_LOG_DIR}/mysite_error. logCustomLog ${APACHE_LOG_DIR}/mysite_access. log combined&lt;/VirtualHost&gt;Verify your configuration: sudo apache2ctl configtestEnable your website: This can be done at end as well, but you can do this now and check if site is working. And reload apache. a2ensite mysite. confsystemctl reload apache2Configure Let’s Encrypt for SSL: ## add below lines to /etc/apt/sources. listdeb http://ftp. debian. org/debian stretch-backports main## update systemsudo apt update## Install certbotsudo apt install python-certbot-apache -t stretch-backports## or sudo apt install python-certbot-apache## Obtaining an SSL Certificatesudo certbot --apache -d example. com -d www. example. comIf Certbot is not working: wget https://dl. eff. org/certbot-auto &amp;&amp; chmod a+x certbot-autocd /etc/letsencrypt/cp ~/certbot-auto . . /certbot-auto Install and configure firewall: Install ufw or firewalld (firewall) depends on the OS. ## install firewallsudo apt install ufw## check statussudo ufw status## List the ufw application profiles sudo ufw app list## allow web traffic and sshsudo ufw allow 'WWW'sudo ufw allow 'WWW Secure'sudo ufw allow 'SSH'sudo ufw allow 'OpenSSH'## To allow all HTTP traffic you can remove WWW and add WWW Fullsudo ufw allow 'WWW Full'sudo ufw delete allow 'WWW'## enable ufwsudo ufw enable## check ufw statussudo ufw statusCreate the directory for website: mkdir /web/mysitechown www-data:www-data /web/mysiteConfigure Database: You need to create an admin user (for login purpose, instead of root), then a database for wordpress, an admin user for your wordpress database etc. Login to mysql and configure user: Login with root password $ mysql -u root -pPASSWORDmysql&gt; Create an Admin user: This is to manange all databases udner mysql. (Instead of using root as login) mysql&gt; GRANT ALL ON *. * TO 'dbadmin'@'localhost' IDENTIFIED BY 'dbpassword' WITH GRANT OPTION;Reset root password: Update root user password and avoid default root password or blank root password. mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass';Create a Databse for wordpress: You may use the same old database name from old server if you want to keep same documents. mysql&gt; CREATE DATABASE wordpress101 DEFAULT CHARACTER SET utf8 COLLATE utf8_unicode_ci;Create an admin user for wordpress database: Create a new user for wordpress. (If you want to keep the same old wordpress db username and password, use it here) mysql&gt; GRANT ALL ON wordpress101. * TO 'wpdbadmin'@'localhost' IDENTIFIED BY 'wpdbpassword';Update database system: You now have a database and user account, made specifically for WordPress. We need to flush the privileges so that the current instance of MySQL knows about the recent changes we’ve made. mysql&gt; FLUSH PRIVILEGES;Export your database and website files: Now we login to old server and export our database and website files (media, scripts, plugins etc) Export Database: ### from old server$ mysqldump -u root -p old_databse |gzip &gt; old_databse_20190104. gzBackup website files in zip file: Let’s gzip all web folder into a single file for easy transfer. $ cd /var/www/$ tar -cv html | gzip &gt; mywpsite. tar. gzCopy databse and website backup to new server: scp user@old_server:/home/user/old_databse_20190104. gz . scp user@old_server:/home/user/mywpsite. tar. gz . Restore database on new server: ### on new servergunzip &lt; old_databse_20190104. gz | mysql -u root -pMyPasswd wordpress101Restore website files: ### on new servergunzip mywpsite. tar. gztar -xvf mywpsite. tar -C /web/pe/Where -C is to point the destination directory. Update and Verify your wp-config. php: Check your database name, username and password are properly configured. "
    }, {
    "id": 93,
    "url": "http://localhost:4000/wordpress-dev",
    "title": "Wordpress Development",
    "body": " Setting up a Wordpress Development EnvironmentsSetting up Vagrant-VirtualBox-WordPress Dev: ## Install the vagrant-hostsupdater plugin. (Optional)$ vagrant plugin install vagrant-hostsupdaterReferences:  5 of the Best Local WordPress Development Tools Compared for 2020 How to Setup a Local WordPress Development Environment How to Set Up a Vagrant WordPress Development Environment"
    }, {
    "id": 94,
    "url": "http://localhost:4000/wordpress-nginx",
    "title": "Wordpress with nginx",
    "body": " Install LEMP stack on Debian 10     Set Up a Firewall with UFW on Debian         Install firewall - ufw     Configure Firewall policy     Enable ufw and check status          Install nginx   Installing MariaDB   Installing PHP   Configuring Nginx to Use the PHP Processor         Create your project directory     Create a configuration for your domain     Activate new configuration     Test config and reload nginx     Create Test php file          Secure Nginx with Let’s Encrypt on Debian 9         Installing Certbot     Confirming Nginx’s Configuration     Verify and reload nginx config     Obtaining an SSL Certificate     Verifying Certbot Auto-Renewal           extract copy sample wp-config. php copy content to our domain location adjust permission     from old server   on new server   on new server   Update and Verify your wp-config. php    Troubleshooting     How to fix 404 not found nginx problem?    Appendix:Install LEMP stack on Debian 10(Linux, Nginx, MariaDB, PHP) Set Up a Firewall with UFW on Debian: Reference Install firewall - ufw: sudo apt install ufwConfigure Firewall policy: sudo ufw default deny incomingsudo ufw default allow outgoing# allow ssh, http, httpssudo ufw allow sshsudo ufw allow httpsudo ufw allow httpsEnable ufw and check status: sudo ufw enablesudo ufw statusResetting or Disabe ufw sudo ufw disable# Any rules that you created with UFW will no longer be active. You can always run sudo ufw enable if you need to activate it later. sudo ufw reset# This will disable UFW and delete any rules that were previously defined. Install nginx: Reference sudo apt updatesudo apt install nginxInstalling MariaDB: sudo apt install mariadb-serverWhen the installation is finished, it’s recommended that you run a security script that comes pre-installed with MariaDB. This script will remove some insecure default settings and lock down access to your database system. Start the interactive script by running: sudo mysql_secure_installationAnd set root password. ALTER USER 'root'@'localhost' IDENTIFIED BY 'MY_NEW_PASSWORD';FLUSH PRIVILEGES;Login to mariadb $ sudo mariadbWelcome to the MariaDB monitor.  Commands end with ; or \g. Your MariaDB connection id is 61Server version: 10. 3. 18-MariaDB-0+deb10u1 Debian 10Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\h' for help. Type '\c' to clear the current input statement. MariaDB [(none)]&gt; Sample tests: # create databaseMariaDB [(none)]&gt; CREATE DATABASE example_database;# create new user and grant accessMariaDB [(none)]&gt; GRANT ALL ON example_database. * TO 'example_user'@'localhost' IDENTIFIED BY 'password' WITH GRANT OPTION;# Flush the privileges to ensure that they are saved and available in the current session:MariaDB [(none)]&gt; FLUSH PRIVILEGES;MariaDB [(none)]&gt; exit;Installing PHP: sudo apt install php-fpm php-mysqlConfiguring Nginx to Use the PHP Processor: Create your project directory: sudo mkdir /var/www/your_domainAssign ownership of the directory with the $USER environment variable, which should reference your current system user: sudo chown -R $USER:$USER /var/www/wp-testCreate a configuration for your domain: sudo nano /etc/nginx/sites-available/wp-test# Sample Contentserver {  listen 80;  listen [::]:80;  root /var/www/wp-test;  index index. php index. html index. htm;  server_name dev. yourdomain. com;  location / {    try_files $uri $uri/ =404;  }  location ~ \. php$ {    include snippets/fastcgi-php. conf;    fastcgi_pass unix:/var/run/php/php7. 3-fpm. sock;  }}Activate new configuration: sudo ln -s /etc/nginx/sites-available/wp-test /etc/nginx/sites-enabled/Test config and reload nginx: # test configurationsudo nginx -t# reload nginxsudo systemctl reload nginxCreate Test php file: $ cat /var/www/wp-test/info. php&lt;?phpphpinfo();Secure Nginx with Let’s Encrypt on Debian 9: Reference Installing Certbot: To add the backports repository, first open /etc/apt/sources. list and add below lines. deb http://deb. debian. org/debian stretch-backports main contrib non-freedeb-src http://deb. debian. org/debian stretch-backports main contrib non-freeThen, sudo apt updateInstall Certbot’s Nginx package with apt: sudo apt install python-certbot-nginx -t stretch-backportsConfirming Nginx’s Configuration: Make sure your server block configuration contains correct server_name value for your domain. $ sudo cat /etc/nginx/sites-available/wp-test |grep server_name   server_name dev. yourdomain. com;Verify and reload nginx config: sudo nginx -tsudo systemctl reload nginxCertbot can now find the correct server block and update it. Obtaining an SSL Certificate: sudo certbot --nginx -d dev. yourdomain. com -d mail. dev. yourdomain. comIt will ask for email address for notifications; also auto-redirect to https Verifying Certbot Auto-Renewal: Test the renewal process, you can do a dry run with certbot: sudo certbot renew --dry-runEnable certbot automatic renewal for cloudflare CDN sudo certbot certonly \ --dns-cloudflare \ --dns-cloudflare-credentials /etc/cloudflare/credential. ini -d www. mydomain. com# list certificates$ sudo certbot certificatesConfigure Wordpress on LEMP StackReference Create Database and User: sudo mariadb -u root -pCREATE DATABASE wordpress DEFAULT CHARACTER SET utf8 COLLATE utf8_unicode_ci;GRANT ALL ON wordpress. * TO 'wordpress_user'@'localhost' IDENTIFIED BY 'password';FLUSH PRIVILEGES;exit;Installing Additional PHP Extensions: sudo apt updatesudo apt install php-curl php-gd php-intl php-mbstring php-soap php-xml php-xmlrpc php-zipRestart php-fpm process sudo systemctl restart php7. 3-fpm. serviceConfiguring Nginx: Please ntoes, if you have enabled SSL (using Certbot) there will be two server blocks in your nginx configuration. Find the one with root /var/www/your_domain and add some entries as below. server {  . . .   location = /favicon. ico { log_not_found off; access_log off; }  location = /robots. txt { log_not_found off; access_log off; allow all; }  location ~* \. (css|gif|ico|jpeg|jpg|js|png)$ {    expires max;    log_not_found off;  }  . . . }Also adjust try_files   . . .   location / {    #try_files $uri $uri/ =404;    try_files $uri $uri/ /index. php$is_args$args;  }  . . . Downloading WordPress: cd /tmpcurl -LO https://wordpress. org/latest. tar. gz# extracttar xzvf latest. tar. gz# copy sample wp-config. phpcp /tmp/wordpress/wp-config-sample. php /tmp/wordpress/wp-config. php# copy content to our domain locationsudo cp -a /tmp/wordpress/. /var/www/wp-test# adjust permissionsudo chown -R www-data:www-data /var/www/wp-testSetting up the WordPress Configuration File: Generate secret keys and use it inside the config. curl -s https://api. wordpress. org/secret-key/1. 1/salt/Copy the content inside wp-config. php where you can find same entries with dummy values. Adjust database values Configure db entries as we generated in firt steps. . . . define('DB_NAME', 'wordpress');/** MySQL database username */define('DB_USER', 'wordpress_user');/** MySQL database password */define('DB_PASSWORD', 'password');. . . define('FS_METHOD', 'direct');Access the site and finish wordpress setup: Access the url over browser and complete wordpress configuration. Migrating or Moving a Wordpress siteNow we login to old server and export our database and website files (media, scripts, plugins etc) Export Database: ### from old server$ mysqldump -u root -p old_databse |gzip &gt; old_databse_20190104. gzBackup website files in zip file: Let’s gzip all web folder into a single file for easy transfer. $ cd /var/www/$ tar -cv html | gzip &gt; mywpsite. tar. gzCopy databse and website backup to new server: scp user@old_server:/home/user/old_databse_20190104. gz . scp user@old_server:/home/user/mywpsite. tar. gz . Restore database on new server: ### on new servergunzip &lt; old_databse_20190104. gz | mysql -u root -pMyPasswd wordpress101Restore website files: ### on new servergunzip mywpsite. tar. gztar -xvf mywpsite. tar -C /web/pe/Where -C is to point the destination directory. Update and Verify your wp-config. php: Check your database name, username and password are properly configured. TroubleshootingHow to fix 404 not found nginx problem?: Change the line in the location block to: try_files $uri $uri/ /index. php?q=$uri&amp;$args; and reload nginx sudo systemctl reload nginx Appendix:bitnami deploymenthttps://docs. bitnami. com/google/apps/wordpress-pro/configuration/create-vhost-nginx/ "
    }, {
    "id": 95,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 96,
    "url": "http://localhost:4000/page2/",
    "title": "automate...containerize...",
    "body": "  {% if page. url ==  /  %}                            Gineesh Madapparambath                                    Helping customers on Automation and Containerization journey using Ansible, Kubernetes, OpenShift and Terraform.                            YouTube. com/techbeatly                           Talks about #devops, #ansible, #openshift, #terraform, and #kubernetes                 Read More                  	                  {% for post in site. pages %}             {% if post. titleshort %}              {{ post. titleshort | downcase }}            {% endif %}          {% endfor %}                           {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; background-position: center; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}{% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 97,
    "url": "http://localhost:4000/page3/",
    "title": "automate...containerize...",
    "body": "  {% if page. url ==  /  %}                            Gineesh Madapparambath                                    Helping customers on Automation and Containerization journey using Ansible, Kubernetes, OpenShift and Terraform.                            YouTube. com/techbeatly                           Talks about #devops, #ansible, #openshift, #terraform, and #kubernetes                 Read More                  	                  {% for post in site. pages %}             {% if post. titleshort %}              {{ post. titleshort | downcase }}            {% endif %}          {% endfor %}                           {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; background-position: center; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}{% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 98,
    "url": "http://localhost:4000/page4/",
    "title": "automate...containerize...",
    "body": "  {% if page. url ==  /  %}                            Gineesh Madapparambath                                    Helping customers on Automation and Containerization journey using Ansible, Kubernetes, OpenShift and Terraform.                            YouTube. com/techbeatly                           Talks about #devops, #ansible, #openshift, #terraform, and #kubernetes                 Read More                  	                  {% for post in site. pages %}             {% if post. titleshort %}              {{ post. titleshort | downcase }}            {% endif %}          {% endfor %}                           {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; background-position: center; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}{% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 99,
    "url": "http://localhost:4000/Ansible-Navigator-Cheat-Sheet/",
    "title": "Ansible Navigator Cheat Sheet",
    "body": "2021/12/10 - Hello hi "
    }, {
    "id": 100,
    "url": "http://localhost:4000/Ansible-BestPractices/",
    "title": "Ansible Best Practices – Ansible Real Life Series",
    "body": "2021/07/29 - Learn about Ansible Best Practices to follow where to keep your moduleswhere to keep your roleshow to name your variableshow to save your directory structureand more… "
    }, {
    "id": 101,
    "url": "http://localhost:4000/how-to-get-hands-on-experience-in-aws/",
    "title": "How To Get Hands-On Experience in AWS",
    "body": "2021/01/04 - Every Time you have the same situation, you know the technology and maybe you are already a certified professional in that technology, but you are not getting enough hands-on on that technology because your organization or your team is not working with that technology. And what, you will slowly start to forget about this topic. Photo by Bill Oxford on Unsplash I heard the same concern from a few friends recently, as they are already Certified as AWS Architects or DevOps engineers but never got a chance to work on AWS! But trust me, there are so many ways to get hands-on, you can do your own PoC’s and get more exposure on each and every technology. So the next question is, what should I do for the PoC ? what type of experiments I can try with my own arrangements ? Well, if you ask me this for AWS, there is a great collection of Hands-on Tutorials by AWS itself. In August 2020, Red Hat announced the availability of Remote exams for students and the entire Learning community were so happy, yes we can attend Red Hat exams from our home or office; not all exams yet. Read Full Article here - How To Get Hands-On Experience in AWS  Subscribe to YouTube Channel"
    }, {
    "id": 102,
    "url": "http://localhost:4000/getting-started-with-ansible-collections/",
    "title": "Getting Started with Ansible Collections",
    "body": "2020/12/22 - Ansible Collection is a great way of getting content contributions from various Ansible Developers. Earlier there was a tagline for Ansible – “Batteries included”, but now the battery is a bit small I will say as default Ansible installation will still include the necessary libraries and modules needed for your automation kickstart but not the entire Ansible module and libraries. This is good in a way that Ansible developers don’t need to wait for a specific release cycle of Ansible to include their latest version of module or library; instead they can distribute their content and make available latest versions via Ansible Collections separately. Read Full Article here - Getting Started with Ansible Collections  Subscribe to YouTube Channel"
    }, {
    "id": 103,
    "url": "http://localhost:4000/configure-your-windows-host-to-manage-by-ansible/",
    "title": "Configure Your Windows Host to be Managed by Ansible",
    "body": "2020/12/01 - I was talking to my friend about Ansible automation and how we are implementing automated solutions for cloud and on-premise infrastructure. Then he told me that, his team is looking for such tools to automate their Windows servers and Desktops. When I suggested Ansible, he didn’t believe me as he thought Ansible cannot do anything with Windows machines ! Oh. . then I realized that there are some misunderstanding about Ansible and it’s supported platforms as most of them thought Ansible is only available for Linux (or Unix); yes that is true (Ansible is not natively available for Windows yet) but you know, you can use Ansible to manage your Windows machines as well (and network devices, firewall devices, cloud, containers and more) Also Read : Automation with Ansible Guides (Cover: Photo by Charles Parker from Pexels) Do you know, you have more than 100 Windows modules already available to use from Ansible Community. Read Full Article here - Configure Your Windows Host to be Managed by Ansible  Subscribe to YouTube Channel"
    }, {
    "id": 104,
    "url": "http://localhost:4000/remove-nodes-from-kubespray-managed-kubernetes-cluster/",
    "title": "Remove nodes from Kubespray Managed Kubernetes Cluster",
    "body": "2020/11/30 - Kubespray is a combination of Ansible and Kubernetes and you can use Kubespray for deploying production ready Kubernetes clusters. You can manage full-lifecycle of Kubernetes clusters using Kubespray and in this demo we will see how to remove nodes from existing Kubernetes clusters. Read how to deploy a Kubernetes cluster using Kubespray. (Photo by Aneta Foubíková from Pexels) "
    }, {
    "id": 105,
    "url": "http://localhost:4000/adding-new-nodes-to-kubespray-managed-kubernetes-cluster",
    "title": "Adding new nodes to Kubespray Managed Kubernetes Cluster",
    "body": "2020/11/27 - Kubespray is a combination of Ansible and Kubernetes and you can use Kubespray for deploying production ready Kubernetes clusters. Learn how to add new nodes in a Kubernetes cluster using Kubespray. You can manage full-lifecycle of Kubernetes clusters using Kubespray and in this demo we will see how to remove nodes from existing Kubernetes clusters. Read how to deploy a Kubernetes cluster using Kubespray. (Photo by Albin Berlin from Pexels) "
    }, {
    "id": 106,
    "url": "http://localhost:4000/deploying-kubernetes-with-kubespray/",
    "title": "Deploying Kubernetes with Kubespray",
    "body": "2020/11/23 - You have multiple ways to deploy a production ready Kubernetes cluster and Kuberspray is another method in which we will use Ansible for configuring the nodes and deploying a kubernetes cluster in a pre-provisioned infrastructure. You can automate infrastructure provisioning using Terraform or even using Ansible but we are not covering that in this article. (Photo by Tom Fisk from Pexels) "
    }, {
    "id": 107,
    "url": "http://localhost:4000/redhat-remote-exam-guide/",
    "title": "Red Hat Remote Exams – Everything you need to know",
    "body": "2020/11/20 - We all know Red Hat certifications are very valuable and it will help you a lot for getting shortlisted in the hiring process. But we also know, Red Hat Exams are not simple multiple choice question exams but 100% practical exams and performance measurement is involved. And Red Hat exams always happen at class rooms, onsite or dedicated kiosks at authorized Exam centers. But we all know, most of the exam centers were closed during COVID-19 pandemic and we were in a situation where we cannot schedule or attend any Red Hat exams. To be honest, I have scheduled my OpenShift exam 6 times and cancelled or rescheduled because of this situation, until I attended that here in Singapore in last July. But this is not the situation anymore. Disclaimer : I am not affiliated or associated with Red Hat, or any of its affiliates right now. This is purely based on my personal experience with Red Hat Remote exams. In August 2020, Red Hat announced the availability of Remote exams for students and the entire Learning community were so happy, yes we can attend Red Hat exams from our home or office; not all exams yet. It was only 4 exams in the first place but we can see more exams are available as remote exams now. But we all got confused when we read the details of the exam environment; yes it is not easy and not straightforward like other remote exams. "
    }, {
    "id": 108,
    "url": "http://localhost:4000/how-to-increase-attachment-file-size-limit-in-gitlab/",
    "title": "How to Increase Attachment File Size Limit in GitLab",
    "body": "2020/11/19 - We know GitLab is one of the best and easy to use git server solutions available in the market now. And GitLab CE is available as free and we can simply spin up a VM and set up our own Git server with all features. We can fine tune the GitLab server as needed and based on organization policies and this must be done if you are using this in production. Let me explain a common scenario. While using our internal GitLab server, we received complaints that users are not able to upload large files from GUI and unable to proceed with deployment using the repositories. "
    }, {
    "id": 109,
    "url": "http://localhost:4000/how-to-attend-red-hat-remote-exam/",
    "title": "How to attend Red Hat Remote Exam ? Every details you need to know",
    "body": "2020/11/09 - In August 2020, Red Hat announced the availability of Remote exams for students and the entire Learning community were so happy, yes we can attend Red Hat exams from our home or office; not all exams yet. In this video I will explain about Red Hat Remote Exams and how to prepare for a remote exam. Please note, I am not affiliated or associated with Red Hat, or any of its affiliates right now. This is purely based on my personal experience with Red Hat Remote exams.  — Chapters —0:00 Introduction1:30 Available Remote Exams2:30 What is Remote Exam4:00 How Red Hat Remote Exam will work ?5:58 How to Enrol for Remote Exam7:16 Price for Remote Exam7:35 How to Prepare your workstation for Remote Exam ?8:30 System Requirement9:16 Multiple Options to Setup your workstatin14:30 Mic &amp; Speaker15:01 Internet16:01 Am I ready for Remote Exam ?17:35 Can I have Drinks and Snacks During Exam ? Read Full Video here - How to attend Red Hat Remote Exam ? Every details you need to know  Subscribe to YouTube Channel"
    }, {
    "id": 110,
    "url": "http://localhost:4000/how-to-import-existing-vmware-vm-in-to-terraform/",
    "title": "How to Import Existing VMWare VM’s into Terraform",
    "body": "2020/11/04 - Terraform is an amazing tool for your infrastructure automation. Everything about your infrastructure can be write as code and maintain by team; means your infrastructure is transparent and immutable. Also Read : Learning path and Exam Tips for HashiCorp Certified Terraform Associate (Cover Image: unsplash. com/@qwitka) Contents hide 1 Step 1: Collect Details of Existing VM from VMWare vCenter2 Step 2: Create Your Terraform Code for Existing VM3 Step 3: Initialize your Terraform Code4 Step 4: Import the VM to Terraform State5 Final Notes Read Full Blog here - How to Import Existing VMWare VM’s into Terraform  Subscribe to YouTube Channel"
    }, {
    "id": 111,
    "url": "http://localhost:4000/connecting-ansible-tower-to-git-server-with-self-signed-certificates/",
    "title": "Connecting Ansible Tower to Git Server with Self Signed Certificates",
    "body": "2020/10/12 - So many questioned me when I mention git server in an Ansible Tower environment; and later I realized that, most of them are keeping their projects inside Ansible Tower !!! Okay, that is just an option in Ansible Tower and Highly NOT Recommended for production setup or for environment with multiple teams accessing the Ansible Tower. Why Should Not ?  Creating a new Project in Ansible Tower You need to give permission for each and every user to this Ansible Tower node and to this directory – /var/lib/awx/projects It won’t be effective when you deploy Ansible Tower as multi-node cluster as you need to update the playbooks and project files in every node, under /var/lib/awx/projects Less control on editing playbooks or files as you have to do everything from CLI (mostly Ansible Tower nodes installs on nodes without GUI) Other users can easily access other projects and files if you didn’t configure your directory properly.  and many other reasons…Read Full Blog here - Connecting Ansible Tower to Git Server with Self Signed Certificates  Subscribe to YouTube Channel"
    }, {
    "id": 112,
    "url": "http://localhost:4000/hashicorp-certified-terraform-associate-learning-exam-tips/",
    "title": "HashiCorp Certified Terraform Associate – Learning & Exam Tips",
    "body": "2020/09/18 - I started using Terraform somewhere in 2018, but very limited usage as I thought it is just another tool for provisioning infrastructure and other services – or a variant of Vagrant; and I never expected that Terraform will grow in such way that, most of the organizations prefer to use it, especially when they have multi-cloud architecture. So, I have decided to explore more by converting our Ansible and Vagrant based projects to Terraform configurations; yes still using Ansible inside as provisioner. And later I decided to sit for HashiCorp Certified Terraform Associate exam, which is the official Certification program conducting by HashiCorp. So, this is another review or preparation kind of stuff for Terraform Certification Exam as I received few queries on the same and thought to add a note here. Read Full Blog here - HashiCorp Certified Terraform Associate – Learning &amp; Exam Tips  Subscribe to YouTube Channel"
    }, {
    "id": 113,
    "url": "http://localhost:4000/10-tips-for-kuberenetes-exam-cka-ckad/",
    "title": "How to Pass CKA & CKAD Exams ? 10 Tips for Kubernetes Exams",
    "body": "2020/08/15 - Here see the best tips for Kubernetes Exams - Certified Kubernetes Administrator (CKA) &amp; Certified Kubernetes Application Developer (CKAD). Watch Full Video "
    }, {
    "id": 114,
    "url": "http://localhost:4000/deploy-minikube-using-vagrant-and-ansible-on-virtualbox-infrastructure-as-code/",
    "title": "Deploy Minikube Using Vagrant and Ansible on VirtualBox – Infrastructure as Code",
    "body": "2020/08/11 - Minikube is a tool to run a kubernetes cluster locally, that means on your laptop or inside a virtual machine. Minikube will run a single node kubernetes cluster on your machine with almost all features you want to try on an actual kubernetes cluster; like DNS, NodePorts,ConfigMaps and Secrets, then kubernetes Dashboard, Container Runtime like Docker, CRI-O, and containerd, and also enabling CNI or Container Network Interface, ingress, yes almost all items you need your hands get dirty.  Read Deploy Minikube Using Vagrant and Ansible on VirtualBox Watch Full Video"
    }, {
    "id": 115,
    "url": "http://localhost:4000/kubernetes-certification-cka-ckad-exam-tips-learning-path/",
    "title": "CKA & CKAD - Kubernetes Exam Tips, Learning Path and Certification",
    "body": "2020/05/03 - Since kubernetes is getting wide acceptance, Kubernetes Certification is the most trending one in IT circle now. I have completed both recently (March – April 2020) and I started getting a lot queries and doubts regarding the certification and other preparation details. I am more than happy to answer those but I thought to prepare one article here; better or what ? So, below details are from my experience and hope it will be useful for your exam preparation. See Full Article "
    }, {
    "id": 116,
    "url": "http://localhost:4000/start-you-openshift-journey-with-openshift-interactive-learning-portal/",
    "title": "Start Your OpenShift journey with OpenShift Interactive Learning Portal – FREE",
    "body": "2020/04/14 - See Full Article As we know, OpenShift is the #1 Enterprise Kubernetes platform available in market now. And OpenShift knowledge is one of the demanding skills for a an engineer in DevOps track. But, like any other complex platform, OpenShift is build on several components and setting up a cluster is bit difficult, especially those without infrastructure knowledge. But thanks to Red Hat and katakoda platform, Red Hat Interactive Learning Portal is available for you with NO COST ! Yes, its FREE ! (Image : https://unsplash. com/@nickmorrison) See Full Article image: unsplash/@heftiba "
    }, {
    "id": 117,
    "url": "http://localhost:4000/how-to-install-vmware-vcenter-server-appliance/",
    "title": "How to Install VMWare vCenter Server Appliance",
    "body": "2020/03/20 - When we talk about virtualization, VMWare is one of the best choice and most popular in town. You can download VMware vSphere Hypervisor 6. 7 for FREE and install on bare metal servers or workstations; but this will be single node cluster. You need VMWare vCenter for a full vSphere cluster experience and this is a licensed software. Fortunately, VMWare is offering free trial for this product and you can download the trial software from their portal. Disclaimer : This demo is for explaining the steps to cover VMWare vCenter installation in a lab or test environment. You must consider recommended prerequisites and configurations for production level clusters. Contents1 Prepare Physical Host2 Download vCenter Appliance3 Install – Stage 1: Deploy vCenter Server Appliance3. 1 Choose Installation Method3. 2 Choose your ESXi host Target3. 3 Configure vCenter Appliance Spcifications4 Install – Stage 2: Setup vCenter Server4. 1 Access VMWare vCenter5 Next Step – Add ESXi Hosts Read Full Blog here - How to Install VMWare vCenter Server Appliance  Subscribe to YouTube Channel"
    }, {
    "id": 118,
    "url": "http://localhost:4000/start-your-openshift-journey-with-these-free-red-hat-training-programs/",
    "title": "Start Your OpenShift Journey with these Free Red Hat Training Programs",
    "body": "2020/03/09 - Red Hat OpenShift Container Platform is one of the best kubernetes platform for enterprises. As we know kubernetes and OpenShift are multi-component stack and not easy to learn without a proper curriculum. Yes, I agree that the documentation is superb but that is for reference and not useful if you are a beginner. Fortunately Red Hat is offering some of the basic courses free of charge and you can enroll with a valid email ID. Honestly, theses are basic courses but more than enough for you to understand the platform. (Image : https://unsplash. com/@annaelizaearl) See Full Article image : @thefallofmath "
    }, {
    "id": 119,
    "url": "http://localhost:4000/installing-ovirt-4-with-self-hosted-engine-on-enterprise-linux/",
    "title": "Installing oVirt 4 with Self-Hosted Engine on Enterprise Linux",
    "body": "2020/03/05 - oVirt is one of the best free and open-source virtualization solution with enterprise level features. Installing oVirt cluster is simple and straight forward and you can refer the installation guide to achieve the same. Basically you will setup oVirt Engine (Controller) and add oVirt Nodes to the cluster. But we don’t need a dedicated physical server for oVirt Engine but you can host it as a VM in oVirt itself via Self-Hosted Engine method. See the demo below for details. (Image by @tvick)/unsplash) See Full Article image: @belart84 "
    }, {
    "id": 120,
    "url": "http://localhost:4000/ansible-tower-start-or-stop-tower-service/",
    "title": "ansible-tower-service, Start or Stop Ansible Tower",
    "body": "2020/02/19 - As we know, Ansible Tower is a multi-component system including Ansible, Tower API, RabbitMQ, Database (managed or external) etc. And will be more complicated when you install Ansible Tower Cluster (multi-node or multi-data center clusters). So, what if you need to stop Ansible Tower gracefully, for some reason (patching or some reason) and start components again ?       *Image: rockwellcollins. com   01 Jan 2000 — Hand on throttle — Image by © Firefly Productions/CORBIS*   See Full Article Also read : How to backup and restore Ansible Tower configuration ? "
    }, {
    "id": 121,
    "url": "http://localhost:4000/how-to-add-custom-modules-in-ansible/",
    "title": "How To Add Custom Modules In Ansible",
    "body": "2020/02/03 - See Full Article Okay, we all know Ansible installation is coming with default modules and you don’t need to download or configure any additional modules for normal cases, Yes batteries already included. But, what if you need to achieve something with Ansible but a suitable module is not available ? No worries, you can write your own modules for local use; that’s the beauty of Ansible. Before you start, just make sure you have searched the module library and confirmed nothing suitable for your special use. See Full Article Also read : How to add Container Images to Google Container Registry (GCR) "
    }, {
    "id": 122,
    "url": "http://localhost:4000/how-to-create-scheduled-snapshots-in-google-cloud-platform/",
    "title": "How to Create Scheduled Snapshots in Google Cloud Platform",
    "body": "2020/01/16 - Yes, your server and data are on cloud, but to avoid the risk of unexpected data loss still you need to follow standard backup mechanisms provided by cloud providers like snapshots or replication. The best way is to take the disk snapshots – Operating System disk as well as data disks. And the best part is Scheduled Snapshots by which you don’t need to worry about regular snapshots. Here see a simple demo for scheduled snapshots in Google Cloud Platform. See Full Article in techbeatly. com Also read : How to add Container Images to Google Container Registry (GCR) "
    }, {
    "id": 123,
    "url": "http://localhost:4000/gns3-vm-how-to-enable-kvm-support/",
    "title": "GNS3 VM – How to Enable KVM Support",
    "body": "2019/12/26 - GNS3 (Graphical Network Simulator) is the most popular – open source, free software – tool for designing and simulating network infrastructure. GNS3 has 2 parts  The GNS3-all-in-one software (GUI) The GNS3 virtual machine (VM)This post is not meant for explaining GNS3; read GNS3 Documentation for more details. I just want to share the common mistake, which newbies do when setup GNS3. And the simple configuration to resolve the same. While setting up separate GNS3 VM, you need to enable KVM support for the GNS3 VM (even if you are using all in one). Because, most of the appliance need KVM support on GNS3 VM otherwise you won’t be able to import network appliance to GNS3. See Full Article in techbeatly. com "
    }, {
    "id": 124,
    "url": "http://localhost:4000/how-to-request-resource-quota-increase-in-microsoft-azure/",
    "title": "How to Request Resource Quota Increase in Microsoft Azure",
    "body": "2019/11/08 - As we know public cloud is offering unlimited* amount of resources and as an end user of public cloud we don’t need to worry about the backend resources. But, there are few facts to consider when we talk about unlimited. What if someone run a script to spin up 1000 VMs !What if someone give wrong variable and your automation program created 10000 database instances or disks !What is some users purposefully created VMs with high in size ? See Full Article in techbeatly. com "
    }, {
    "id": 125,
    "url": "http://localhost:4000/build-your-own-git-server-using-gogs/",
    "title": "Build Your Own Git Server using Gogs",
    "body": "2019/10/22 - When it comes to VCS (Version Control System), we will have confusion as we have many products in market with almost same capabilities. When we think about enterprise level usage, GitLab, GitHub, Subversion, Bitbucket etc. will come on top of the list and some are with enterprise level support too. Installing our own git server with GUI is not a big deal as we have gitlab-ce (Learn more) which is free and another option is self-hosted bitbucket (but not free), or you can go for other premium VCS solutions. Fortunately, we have a simple opensource product called Gogs, which itself described as “A painless self-hosted Git service“. Yes, you can setup your own gitlab/github like git server with Gogs. You have different choices for installation from source, from packages, from binaries or just run it inside a docker container. See Full Article in techbeatly. com "
    }, {
    "id": 126,
    "url": "http://localhost:4000/Adding-Container-Images-to-Google-Container-Registry/",
    "title": "Adding Container Images to Google Container Registry (GCR)",
    "body": "2019/10/21 - Google Container Registry (GCR) is a service in Google Cloud Platform (GCP) to manage your own docker container repository. This is fully managed service and you can store your custom container images as well as common images from other image repositories. Also, you can implement fine-grained access control on your private container registry using GCR. Let’s see how we can add an image to our own GCR and see details in Web GUI. See Full Article "
    }, {
    "id": 127,
    "url": "http://localhost:4000/Ansible-Tower-How-to-Backup-and-Restore/",
    "title": "Ansible Tower - How to Backup and Restore",
    "body": "2019/09/24 - Ansible Tower is a wonderful product which help to implement a central hub for IT automation. Ansible Tower is basically a web console and REST API for underlying Ansible Engine but with more features and act as centralized system with logging and RBAC (Role Based Access Control). Installing and configuring Ansible Tower (with or without HA) is a straight forward task but what if we need to take backup of the setup ? We have jobs, templates, projects, credentials everything in Tower; how can we take backup of the Ansible Tower and restore it in case of emergency ? See Full Article "
    }, {
    "id": 128,
    "url": "http://localhost:4000/Ansible-Part-8-Managing-Facts/",
    "title": "Ansible – Part 8 – Managing Facts",
    "body": "2019/09/03 - Ansible facts are nothing but some variables which are automatically discovered by the Ansible on managed hosts while running ansible adhoc commands or playbooks. See Automation with Ansible full chapters. See Full Article "
    }, {
    "id": 129,
    "url": "http://localhost:4000/Ansible-Part-7-Managing-Variables/",
    "title": "Ansible – Part 7 – Managing Variables",
    "body": "2019/08/30 - You can use variables in ansible plays to store values like users to create, packages to install etc. By using variables, you can manage your plays with dynamic input values. See Automation with Ansible full chapters. Example Variable usages: Users to createPackages to installServices to restartFiles to more or createRefer : Ansible Documentation See Full Article "
    }, {
    "id": 130,
    "url": "http://localhost:4000/Configure-TLS-Encrypted-Tunnel-For-Remote-Logs-Using-Syslog-ng/",
    "title": "Configure TLS Encrypted Tunnel For Remote Logs Using Syslog-ng",
    "body": "2019/08/08 - You might be a Sysadmin, developer, DBA or whatever, logs are like treasure boxes for anyone working in IT. And the best practice to keep logs in a central location together with local copy. Most of the logging programs have the ability to send logs to a remote logging server (as well as receive logs from remote machines); eg rsyslog, syslog-ng etc. But, still there is a concern for sending server/application/database logs sending over tcp as plain text; yes indeed. But no need to worry as most of the logging programs will have simple mechanisms to implement TLS Tunnels for sending and receiving logs. In below demo, we will implement TLS tunnel to send logs from one machine (using syslog-ng) and receive the logs on another logging server (syslog-ng). See Full Article "
    }, {
    "id": 131,
    "url": "http://localhost:4000/malaysia-long-term-social-visit-pass-ltsvp-frequently-asked-questions/",
    "title": "Malaysia Long Term Social Visit Pass (LTSVP) – Frequently Asked Questions",
    "body": "2019/07/02 - LTSVP stands for Long Term Social Visit Pass and this is using inorder bring our parents for long stay in Malaysia for a maximum of 1 year. The duration may be less if the employment pass for the applicant is less than 1 year at the time of application. We have clearly explained about the LTSVP application process in our old post How To Process Long Term Social Visit Pass(LTSVP) in Malaysia and we are happy to see that it helped a lot of people to understand the process and procedures. We are receiving so many comments and queries on the same via comment system as well as emails. Hence we thought to have a FAQ Page (Frequently Asked Questions) for this purpose, so readers will get more understanding and clarity on the processes. See Full Article in rovervibes. com "
    }, {
    "id": 132,
    "url": "http://localhost:4000/HTTP-Status-Codes-Quick-Reference/",
    "title": "HTTP Status Codes",
    "body": "2019/07/02 - You are browsing some sites or testing your own web applications, and suddenly you receive a white screen with some error number ! No worries, if you are getting some error message means, your web server and web browser is helping you to resolve the issue with those error codes. There are more than 30 HTTP error codes in total but you don’t need all of them. HTTP response status codes indicate whether a specific HTTP request has been successfully completed or not. See Full Article "
    }, {
    "id": 133,
    "url": "http://localhost:4000/Sending-Email-Using-Python-and-smtplib-Quick-HowTo/",
    "title": "Sending Email Using Python and smtplib",
    "body": "2019/06/13 - We can easily send email from python script using smtplib module, which defines an SMTP client session object that can be used to send mail to any Internet machine with an SMTP or ESMTP listener daemon. See Full Article "
    }, {
    "id": 134,
    "url": "http://localhost:4000/Learn-the-basics-of-Linux-from-Red-Hat-FREE/",
    "title": "Learn the basics of Linux from Red Hat",
    "body": "2019/02/25 - Red Hat is offering its beginner level Linux course at no-cost; yes you heard it right – Its free ! Red Hat Enterprise Linux Technical Overview (RH024) is offering as Video Classroom and you can attend at your own convenience. See Full Article "
    }, {
    "id": 135,
    "url": "http://localhost:4000/How-to-delete-a-pod-with-Terminating-state-in-OpenShift-or-Kubernetes-Cluster/",
    "title": "How to delete a pod with Terminating state in OpenShift or Kubernetes Cluster",
    "body": "2019/01/24 - There might be situations where you have already deleted pods (or already removed dc aka deployment configuration) but pods are stuck in Terminating state. There are few suggestions if you google around (Red Hat Thread : How to delete pods hanging in Terminating state) and just listing down the best method or steps which worked for me. See Full Article "
    }, {
    "id": 136,
    "url": "http://localhost:4000/How-to-Fix-a-Failed-PV-in-OpenShift-Cluster/",
    "title": "How to Fix a Failed PV in OpenShift Cluster",
    "body": "2018/11/16 - There are several cases a PV (PhysicalVolume) appear as Failed in OpenShift or Kubernetes cluster. Once of the reason is wrong ClaimRef by which PV will mark as claimed but actually not in use. Let’s see one example. See Full Article "
    }, {
    "id": 137,
    "url": "http://localhost:4000/OpenShift-Cluster-How-to-Drain-or-Evacuate-a-Node-for-Maintenance/",
    "title": "OpenShift Cluster – How to Drain or Evacuate a Node for Maintenance",
    "body": "2018/11/07 - As we know OpenShift clusters are bundled with multiple compute nodes, master nodes, infra nodes etc, it’s not a big deal to manage node maintenance for OS patching kind of activities. But we need to ensure we have enough capacity on other nodes to balance the workload. When there is a maintenance work – eg: Kernel patching – we need to exercise this without impacting those pods and application running on cluster. See Full Article "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        
<div class="container">
<div class="jumbotron jumbotron-fluid mb-3 pl-0 pt-0 pb-0 bg-white position-relative">
		<div class="h-100 tofront">
			<div class="row  justify-content-between ">
				<div class=" col-md-6  pr-0 pr-md-4 pt-4 pb-4 align-self-center">
					<p class="text-uppercase font-weight-bold">
                        <span class="catlist">
						
                          <a class="sscroll text-danger" href="/categories.html#containers">containers</a><span class="sep">, </span>
                        
                          <a class="sscroll text-danger" href="/categories.html#kubernetes">kubernetes</a><span class="sep">, </span>
                        
                        </span>
					</p>
					<h1 class="display-4 mb-4 article-headline">Adding new nodes to Kubespray Managed Kubernetes Cluster</h1>
					<div class="d-flex align-items-center">
                        
                        <img class="rounded-circle" src="/assets/images/avatar.png" alt="Gineesh" width="70"/>
                        
						<small class="ml-3"> Gineesh <span><a target="_blank" href="https://twitter.com/ginigangadharan" class="btn btn-outline-success btn-sm btn-round ml-1">Follow</a></span>
                            <span class="text-muted d-block mt-1">Nov 27, 2020 · <span class="reading-time">
  
  
    1 min read
  
</span>
    </span>
						</small>
					</div>
				</div>
                
				<div class="col-md-6 pr-0 align-self-center">
					<img class="rounded" src="/assets/images/2020/pexels-albin-berlin-kubespray-add-node-kubernetes-cluster-new-ship-1536x1024.jpg" alt="Adding new nodes to Kubespray Managed Kubernetes Cluster">
				</div>
                
			</div>
		</div>
	</div>
</div>





<div class="container-lg pt-4 pb-4">
	<div class="row justify-content-center">
        
        
        <!-- Share -->
		<div class="col-lg-2 pr-4 mb-4 col-md-12">
			<div class="sticky-top sticky-top-offset text-center">
				<div class="text-muted">
					Share this
				</div>
				<div class="share d-inline-block">
					<!-- AddToAny BEGIN -->
					<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
						<a class="a2a_dd" href="https://www.addtoany.com/share"></a>
						<a class="a2a_button_facebook"></a>
						<a class="a2a_button_twitter"></a>
					</div>
					<script async src="https://static.addtoany.com/menu/page.js"></script>
					<!-- AddToAny END -->
				</div>
			</div>
		</div>
        
        
		<div class="col-md-12 col-lg-8">

			<!-- Liunux Foundation Promo - 
			<a target="_blank" href="https://shareasale.com/r.cfm?b=1612350&amp;u=2435164&amp;m=59485&amp;urllink=&amp;afftrack="><img src="https://static.shareasale.com/image/59485/September_PromoAds_901x501.png" border="0" /></a>

			.....Remove by 22nd Sept 2020-->

      <!-- Article -->
			<article class="article-post">                
			<p>Kubespray is a combination of Ansible and Kubernetes and you can use Kubespray for deploying production ready Kubernetes clusters. Learn how to add new nodes in a Kubernetes cluster using Kubespray.</p>

<p>You can manage full-lifecycle of Kubernetes clusters using Kubespray and in this demo we will see how to remove nodes from existing Kubernetes clusters.</p>

<p>Read <a href="https://www.techbeatly.com/2020/11/deploying-kubernetes-with-kubespray.html">how to deploy a Kubernetes cluster using Kubespray</a>.</p>

<p><em>(Photo by <a href="https://www.pexels.com/@albinberlin">Albin Berlin</a> from Pexels)</em></p>
                
			</article>
			

			<!-- Tags -->
			<div class="mb-4">
				<span class="taglist">
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#kubernetes">kubernetes</a>
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#kubespray">kubespray</a>
				
				</span>
			</div>
 
            <!-- Mailchimp Subscribe Form -->
            
            
            
             <!-- Author Box -->
                				
				<div class="row mt-5">
					<div class="col-md-2 align-self-center">
                         
                        <img class="rounded-circle" src="/assets/images/avatar.png" alt="Gineesh" width="90"/>
                         
					</div>
					<div class="col-md-10">		
                        <h5 class="font-weight-bold">Written by Gineesh <span><a target="_blank" href="https://twitter.com/ginigangadharan" class="btn btn-outline-success btn-sm btn-round ml-2">Follow</a></span></h5>
						Backpacker, Foodie, Techie					
					</div>
				</div>				
                
            
						<!-- google ads -->
						<!-- iamgini-hor-102-article -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1676785078101005"
     data-ad-slot="4500052445"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

            <!-- Comments -->
            
                <!--  Don't edit anything here. Set your disqus id in _config.yml -->

<div id="comments" class="mt-5">
    <div id="disqus_thread">
    </div>
    <script type="text/javascript">
        var disqus_shortname = 'iamgini-com'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>
    Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
    </noscript>
</div>
            
			
			<!--- latest -->
			<div style="padding-top: 20px;">
    <h4 class="font-weight-bold spanborder"><span>Latest Stories</span></h4>  

            
        
        <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h6 class="mb-1 h6 font-weight-bold">
	<a class="text-dark" href="/Ansible-Navigator-Cheat-Sheet/">Ansible Navigator Cheat Sheet</a>
	</h6>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		</span>
	</small>
</div>

</div>

            
        
        <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h6 class="mb-1 h6 font-weight-bold">
	<a class="text-dark" href="/Ansible-BestPractices/">Ansible Best Practices – Ansible Real Life Series</a>
	</h6>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#ansible">Ansible</a><span class="sep">, </span>
		
		</span>
	</small>
</div>

</div>

            
        
        <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h6 class="mb-1 h6 font-weight-bold">
	<a class="text-dark" href="/how-to-get-hands-on-experience-in-aws/">How To Get Hands-On Experience in AWS</a>
	</h6>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#aws">AWS</a><span class="sep">, </span>
		
		</span>
	</small>
</div>

</div>

            
        
        <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h6 class="mb-1 h6 font-weight-bold">
	<a class="text-dark" href="/getting-started-with-ansible-collections/">Getting Started with Ansible Collections</a>
	</h6>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#redhat">redhat</a><span class="sep">, </span>
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#ansible">ansible</a><span class="sep">, </span>
		
		</span>
	</small>
</div>

</div>

            
        
        <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h6 class="mb-1 h6 font-weight-bold">
	<a class="text-dark" href="/configure-your-windows-host-to-manage-by-ansible/">Configure Your Windows Host to be Managed by Ansible</a>
	</h6>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#redhat">redhat</a><span class="sep">, </span>
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#ansible">ansible</a><span class="sep">, </span>
		
		</span>
	</small>
</div>

</div>

    
   
</div> 




			
			<!-- google ads -->
			<!-- iamgini-hor-103-article -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1676785078101005"
     data-ad-slot="5440519661"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			<!--- featured-->
			<div >
    <h4 class="font-weight-bold spanborder"><span>Featured</span></h4>  
    <ol class="list-featured">				
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/start-you-openshift-journey-with-openshift-interactive-learning-portal/" class="text-dark">Start Your OpenShift journey with OpenShift Interactive Learning Portal – FREE</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#openshift">openshift</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/installing-ovirt-4-with-self-hosted-engine-on-enterprise-linux/" class="text-dark">Installing oVirt 4 with Self-Hosted Engine on Enterprise Linux</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#ovirt">oVirt</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/how-to-create-scheduled-snapshots-in-google-cloud-platform/" class="text-dark">How to Create Scheduled Snapshots in Google Cloud Platform</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#cloud">cloud</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/Ansible-Tower-How-to-Backup-and-Restore/" class="text-dark">Ansible Tower - How to Backup and Restore</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#ansible">ansible</a><span class="sep">, </span>
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#automation">automation</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/Sending-Email-Using-Python-and-smtplib-Quick-HowTo/" class="text-dark">Sending Email Using Python and smtplib</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    </span>
                </span>
            </span>
            </li>                
           
    </ol>
</div>     
		
		</div>
        
        
	</div>
</div>


<!-- Aletbar Prev/Next -->
<div class="alertbar">
    <div class="container">
        <div class="row prevnextlinks small font-weight-bold">
          
            <div class="col-md-6 rightborder pl-0">
                <a class="text-dark" href="/deploying-kubernetes-with-kubespray/"> <img height="30px" class="mr-1" src="/assets/images/2020/pexels-tom-fisk-deploying-kubernetes-with-kubespray.jpg">  Deploying Kubernetes with Kubespray</a>
            </div>
          
          
            <div class="col-md-6 text-right pr-0">
                <a class="text-dark" href="/remove-nodes-from-kubespray-managed-kubernetes-cluster/"> Remove nodes from Kubespray Managed Kubernetes Cluster  <img height="30px" class="ml-1" src="/assets/images/2020/pexels-aneta-foubikova-2336927-kubespray-delete-kubernetes-node-1536x1024.jpg"> </a>
            </div>
          
        </div>
    </div>
</div>

    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>

    <div style="text-align: center;"></div>
    <!-- iamgini-hor-101 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1676785078101005"
     data-ad-slot="3017232774"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>

    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><img src="/assets/images/logo-g-v1.png" alt="Gineesh Madapparambath">&nbsp;&nbsp;<strong>Gineesh Madapparambath</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>
            </div>
            <div>
                <a target="_blank" class="text-dark font-weight-bold" href="https://www.techbeatly.com">techbeatly.com</a> | 
                <a target="_blank" class="text-dark font-weight-bold" href="https://www.rovervibes.com">rovervibes.com</a>
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
